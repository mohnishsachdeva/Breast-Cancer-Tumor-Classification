{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:deeppink;font-family:Georgia;text-align:center'> Import Libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:Blue;font-family:Georgia;text-align:center'> Read CSV File(Loading Dataset)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/tumor_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:red;font-family:Georgia;text-align:center'> Copying original dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:red;font-family:Georgia;text-align:center'> Information about dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "wf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = wf.isnull().sum().sort_values(ascending = False)\n",
    "# percent = (wf.isnull().sum() / wf.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis = 1, keys = ['total_null', 'percent_null'])\n",
    "# missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Missing Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 32                569\n",
       "compactness_se               0\n",
       "fractal_dimension_worst      0\n",
       "symmetry_worst               0\n",
       "concave points_worst         0\n",
       "concavity_worst              0\n",
       "compactness_worst            0\n",
       "smoothness_worst             0\n",
       "area_worst                   0\n",
       "perimeter_worst              0\n",
       "texture_worst                0\n",
       "radius_worst                 0\n",
       "fractal_dimension_se         0\n",
       "symmetry_se                  0\n",
       "concave points_se            0\n",
       "concavity_se                 0\n",
       "id                           0\n",
       "diagnosis                    0\n",
       "area_se                      0\n",
       "perimeter_se                 0\n",
       "texture_se                   0\n",
       "radius_se                    0\n",
       "fractal_dimension_mean       0\n",
       "symmetry_mean                0\n",
       "concave points_mean          0\n",
       "concavity_mean               0\n",
       "compactness_mean             0\n",
       "smoothness_mean              0\n",
       "area_mean                    0\n",
       "perimeter_mean               0\n",
       "texture_mean                 0\n",
       "radius_mean                  0\n",
       "smoothness_se                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:Center'> Check Duplicate Records </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, diagnosis, radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean, concave points_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, perimeter_se, area_se, smoothness_se, compactness_se, concavity_se, concave points_se, symmetry_se, fractal_dimension_se, radius_worst, texture_worst, perimeter_worst, area_worst, smoothness_worst, compactness_worst, concavity_worst, concave points_worst, symmetry_worst, fractal_dimension_worst, Unnamed: 32]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 33 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wf.duplicated().sum()) # to check the duplicate records\n",
    "wf[wf.duplicated()]\n",
    "\n",
    "# wf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Unique Values of each Feature </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_value_of_all_columns(dataset):\n",
    "    for i in dataset.columns:\n",
    "        print(dataset[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842302     1\n",
      "90250      1\n",
      "901315     1\n",
      "9013579    1\n",
      "9013594    1\n",
      "          ..\n",
      "873885     1\n",
      "873843     1\n",
      "873701     1\n",
      "873593     1\n",
      "92751      1\n",
      "Name: id, Length: 569, dtype: int64\n",
      "B    357\n",
      "M    212\n",
      "Name: diagnosis, dtype: int64\n",
      "12.34    4\n",
      "11.71    3\n",
      "12.46    3\n",
      "13.05    3\n",
      "10.26    3\n",
      "        ..\n",
      "12.23    1\n",
      "14.45    1\n",
      "19.18    1\n",
      "18.08    1\n",
      "7.76     1\n",
      "Name: radius_mean, Length: 456, dtype: int64\n",
      "20.52    3\n",
      "16.85    3\n",
      "16.84    3\n",
      "19.83    3\n",
      "14.93    3\n",
      "        ..\n",
      "18.58    1\n",
      "15.11    1\n",
      "22.41    1\n",
      "14.92    1\n",
      "24.54    1\n",
      "Name: texture_mean, Length: 479, dtype: int64\n",
      "82.61     3\n",
      "87.76     3\n",
      "134.70    3\n",
      "93.97     2\n",
      "82.69     2\n",
      "         ..\n",
      "127.50    1\n",
      "90.63     1\n",
      "82.53     1\n",
      "100.40    1\n",
      "47.92     1\n",
      "Name: perimeter_mean, Length: 522, dtype: int64\n",
      "512.2     3\n",
      "1075.0    2\n",
      "582.7     2\n",
      "399.8     2\n",
      "641.2     2\n",
      "         ..\n",
      "507.4     1\n",
      "609.9     1\n",
      "463.7     1\n",
      "428.9     1\n",
      "181.0     1\n",
      "Name: area_mean, Length: 539, dtype: int64\n",
      "0.10070    5\n",
      "0.11500    4\n",
      "0.10540    4\n",
      "0.10750    4\n",
      "0.10630    3\n",
      "          ..\n",
      "0.08876    1\n",
      "0.09965    1\n",
      "0.13230    1\n",
      "0.08968    1\n",
      "0.05263    1\n",
      "Name: smoothness_mean, Length: 474, dtype: int64\n",
      "0.11470    3\n",
      "0.12060    3\n",
      "0.07698    2\n",
      "0.05743    2\n",
      "0.03834    2\n",
      "          ..\n",
      "0.05562    1\n",
      "0.06141    1\n",
      "0.04695    1\n",
      "0.10520    1\n",
      "0.04362    1\n",
      "Name: compactness_mean, Length: 537, dtype: int64\n",
      "0.000000    13\n",
      "0.120400     3\n",
      "0.111500     2\n",
      "0.033440     2\n",
      "0.110300     2\n",
      "            ..\n",
      "0.065930     1\n",
      "0.248700     1\n",
      "0.287100     1\n",
      "0.009193     1\n",
      "0.351400     1\n",
      "Name: concavity_mean, Length: 537, dtype: int64\n",
      "0.00000    13\n",
      "0.02864     3\n",
      "0.14710     2\n",
      "0.05778     2\n",
      "0.02272     2\n",
      "           ..\n",
      "0.02361     1\n",
      "0.05189     1\n",
      "0.14960     1\n",
      "0.18780     1\n",
      "0.15200     1\n",
      "Name: concave points_mean, Length: 542, dtype: int64\n",
      "0.1714    4\n",
      "0.1769    4\n",
      "0.1893    4\n",
      "0.1601    4\n",
      "0.1717    4\n",
      "         ..\n",
      "0.2251    1\n",
      "0.2200    1\n",
      "0.1979    1\n",
      "0.1950    1\n",
      "0.1587    1\n",
      "Name: symmetry_mean, Length: 432, dtype: int64\n",
      "0.06113    3\n",
      "0.05913    3\n",
      "0.05907    3\n",
      "0.05667    3\n",
      "0.06782    3\n",
      "          ..\n",
      "0.05234    1\n",
      "0.06323    1\n",
      "0.08116    1\n",
      "0.05975    1\n",
      "0.07016    1\n",
      "Name: fractal_dimension_mean, Length: 499, dtype: int64\n",
      "0.2860    3\n",
      "0.2204    3\n",
      "0.2684    2\n",
      "0.2239    2\n",
      "0.1601    2\n",
      "         ..\n",
      "0.6298    1\n",
      "0.8361    1\n",
      "0.2889    1\n",
      "0.1731    1\n",
      "0.3857    1\n",
      "Name: radius_se, Length: 540, dtype: int64\n",
      "1.1500    3\n",
      "1.3500    3\n",
      "1.2680    3\n",
      "0.8561    3\n",
      "1.0160    2\n",
      "         ..\n",
      "0.7655    1\n",
      "0.4757    1\n",
      "0.4956    1\n",
      "0.7629    1\n",
      "1.5950    1\n",
      "Name: texture_se, Length: 519, dtype: int64\n",
      "1.778    4\n",
      "1.243    2\n",
      "2.569    2\n",
      "2.183    2\n",
      "3.008    2\n",
      "        ..\n",
      "2.112    1\n",
      "1.687    1\n",
      "1.011    1\n",
      "1.742    1\n",
      "2.548    1\n",
      "Name: perimeter_se, Length: 533, dtype: int64\n",
      "16.97    3\n",
      "17.67    3\n",
      "16.64    3\n",
      "18.54    3\n",
      "20.98    2\n",
      "        ..\n",
      "26.43    1\n",
      "22.77    1\n",
      "40.98    1\n",
      "81.46    1\n",
      "19.15    1\n",
      "Name: area_se, Length: 528, dtype: int64\n",
      "0.006399    2\n",
      "0.010520    2\n",
      "0.012910    2\n",
      "0.007189    2\n",
      "0.005298    2\n",
      "           ..\n",
      "0.006905    1\n",
      "0.002866    1\n",
      "0.014390    1\n",
      "0.003290    1\n",
      "0.006522    1\n",
      "Name: smoothness_se, Length: 547, dtype: int64\n",
      "0.01812    3\n",
      "0.02310    3\n",
      "0.01104    3\n",
      "0.02431    2\n",
      "0.01203    2\n",
      "          ..\n",
      "0.01200    1\n",
      "0.03728    1\n",
      "0.02263    1\n",
      "0.04759    1\n",
      "0.00466    1\n",
      "Name: compactness_se, Length: 541, dtype: int64\n",
      "0.000000    13\n",
      "0.016520     2\n",
      "0.016980     2\n",
      "0.026810     2\n",
      "0.035760     2\n",
      "            ..\n",
      "0.015560     1\n",
      "0.019780     1\n",
      "0.001597     1\n",
      "0.017740     1\n",
      "0.071170     1\n",
      "Name: concavity_se, Length: 533, dtype: int64\n",
      "0.00000    13\n",
      "0.01167     3\n",
      "0.01110     3\n",
      "0.01499     3\n",
      "0.01004     2\n",
      "           ..\n",
      "0.00842     1\n",
      "0.01883     1\n",
      "0.01321     1\n",
      "0.01757     1\n",
      "0.01664     1\n",
      "Name: concave points_se, Length: 507, dtype: int64\n",
      "0.01344    4\n",
      "0.02045    3\n",
      "0.01884    3\n",
      "0.01647    3\n",
      "0.01870    3\n",
      "          ..\n",
      "0.02538    1\n",
      "0.01172    1\n",
      "0.01613    1\n",
      "0.01682    1\n",
      "0.02676    1\n",
      "Name: symmetry_se, Length: 498, dtype: int64\n",
      "0.004005    2\n",
      "0.003224    2\n",
      "0.004560    2\n",
      "0.005667    2\n",
      "0.001892    2\n",
      "           ..\n",
      "0.001988    1\n",
      "0.001671    1\n",
      "0.001087    1\n",
      "0.003470    1\n",
      "0.006185    1\n",
      "Name: fractal_dimension_se, Length: 545, dtype: int64\n",
      "12.360    5\n",
      "13.340    4\n",
      "13.500    4\n",
      "12.840    3\n",
      "15.140    3\n",
      "         ..\n",
      "15.740    1\n",
      "11.870    1\n",
      "17.800    1\n",
      "12.370    1\n",
      "9.456     1\n",
      "Name: radius_worst, Length: 457, dtype: int64\n",
      "27.26    3\n",
      "17.70    3\n",
      "16.93    2\n",
      "30.50    2\n",
      "23.17    2\n",
      "        ..\n",
      "32.06    1\n",
      "24.70    1\n",
      "22.00    1\n",
      "39.34    1\n",
      "30.37    1\n",
      "Name: texture_worst, Length: 511, dtype: int64\n",
      "117.70    3\n",
      "105.90    3\n",
      "101.70    3\n",
      "184.60    2\n",
      "106.40    2\n",
      "         ..\n",
      "79.57     1\n",
      "84.42     1\n",
      "139.20    1\n",
      "75.39     1\n",
      "59.16     1\n",
      "Name: perimeter_worst, Length: 514, dtype: int64\n",
      "472.4     2\n",
      "1210.0    2\n",
      "826.4     2\n",
      "402.8     2\n",
      "1750.0    2\n",
      "         ..\n",
      "579.5     1\n",
      "762.4     1\n",
      "521.5     1\n",
      "1410.0    1\n",
      "268.6     1\n",
      "Name: area_worst, Length: 544, dtype: int64\n",
      "0.13470    4\n",
      "0.12750    4\n",
      "0.12230    4\n",
      "0.14010    4\n",
      "0.12340    4\n",
      "          ..\n",
      "0.22260    1\n",
      "0.13810    1\n",
      "0.14290    1\n",
      "0.13220    1\n",
      "0.08996    1\n",
      "Name: smoothness_worst, Length: 411, dtype: int64\n",
      "0.34160    3\n",
      "0.14860    3\n",
      "0.21700    2\n",
      "0.09866    2\n",
      "0.22640    2\n",
      "          ..\n",
      "0.24450    1\n",
      "0.10190    1\n",
      "0.32990    1\n",
      "0.16100    1\n",
      "0.06444    1\n",
      "Name: compactness_worst, Length: 529, dtype: int64\n",
      "0.00000    13\n",
      "0.45040     3\n",
      "0.13770     3\n",
      "0.18040     2\n",
      "0.18110     2\n",
      "           ..\n",
      "0.00692     1\n",
      "0.16480     1\n",
      "0.29020     1\n",
      "0.67800     1\n",
      "0.93870     1\n",
      "Name: concavity_worst, Length: 539, dtype: int64\n",
      "0.00000    13\n",
      "0.05556     3\n",
      "0.06296     3\n",
      "0.12180     3\n",
      "0.07431     3\n",
      "           ..\n",
      "0.15410     1\n",
      "0.29030     1\n",
      "0.26880     1\n",
      "0.03990     1\n",
      "0.26500     1\n",
      "Name: concave points_worst, Length: 492, dtype: int64\n",
      "0.2226    3\n",
      "0.2369    3\n",
      "0.2972    3\n",
      "0.3196    3\n",
      "0.3109    3\n",
      "         ..\n",
      "0.3024    1\n",
      "0.3000    1\n",
      "0.3215    1\n",
      "0.1909    1\n",
      "0.4087    1\n",
      "Name: symmetry_worst, Length: 500, dtype: int64\n",
      "0.07427    3\n",
      "0.09026    2\n",
      "0.12970    2\n",
      "0.08174    2\n",
      "0.10550    2\n",
      "          ..\n",
      "0.06938    1\n",
      "0.07697    1\n",
      "0.09772    1\n",
      "0.08631    1\n",
      "0.07039    1\n",
      "Name: fractal_dimension_worst, Length: 535, dtype: int64\n",
      "Series([], Name: Unnamed: 32, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print_unique_value_of_all_columns(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Dataset Description  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>569.0</td>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>8670.000000</td>\n",
       "      <td>869218.000000</td>\n",
       "      <td>906024.000000</td>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>9.113205e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.412729e+01</td>\n",
       "      <td>3.524049e+00</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>1.578000e+01</td>\n",
       "      <td>2.811000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.928965e+01</td>\n",
       "      <td>4.301036e+00</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>2.180000e+01</td>\n",
       "      <td>3.928000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>9.196903e+01</td>\n",
       "      <td>2.429898e+01</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>1.041000e+02</td>\n",
       "      <td>1.885000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>6.548891e+02</td>\n",
       "      <td>3.519141e+02</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>7.827000e+02</td>\n",
       "      <td>2.501000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>9.636028e-02</td>\n",
       "      <td>1.406413e-02</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>1.053000e-01</td>\n",
       "      <td>1.634000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.043410e-01</td>\n",
       "      <td>5.281276e-02</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>1.304000e-01</td>\n",
       "      <td>3.454000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>8.879932e-02</td>\n",
       "      <td>7.971981e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>1.307000e-01</td>\n",
       "      <td>4.268000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>4.891915e-02</td>\n",
       "      <td>3.880284e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>7.400000e-02</td>\n",
       "      <td>2.012000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.811619e-01</td>\n",
       "      <td>2.741428e-02</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>1.957000e-01</td>\n",
       "      <td>3.040000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>6.279761e-02</td>\n",
       "      <td>7.060363e-03</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>6.612000e-02</td>\n",
       "      <td>9.744000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>4.051721e-01</td>\n",
       "      <td>2.773127e-01</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>4.789000e-01</td>\n",
       "      <td>2.873000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853e+00</td>\n",
       "      <td>5.516484e-01</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000e+00</td>\n",
       "      <td>4.885000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059e+00</td>\n",
       "      <td>2.021855e+00</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000e+00</td>\n",
       "      <td>2.198000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>4.033708e+01</td>\n",
       "      <td>4.549101e+01</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>4.519000e+01</td>\n",
       "      <td>5.422000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>7.040979e-03</td>\n",
       "      <td>3.002518e-03</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>8.146000e-03</td>\n",
       "      <td>3.113000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.547814e-02</td>\n",
       "      <td>1.790818e-02</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>3.245000e-02</td>\n",
       "      <td>1.354000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>3.189372e-02</td>\n",
       "      <td>3.018606e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>4.205000e-02</td>\n",
       "      <td>3.960000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.179614e-02</td>\n",
       "      <td>6.170285e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>1.471000e-02</td>\n",
       "      <td>5.279000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.054230e-02</td>\n",
       "      <td>8.266372e-03</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>2.348000e-02</td>\n",
       "      <td>7.895000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>3.794904e-03</td>\n",
       "      <td>2.646071e-03</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>4.558000e-03</td>\n",
       "      <td>2.984000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.626919e+01</td>\n",
       "      <td>4.833242e+00</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>1.879000e+01</td>\n",
       "      <td>3.604000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.567722e+01</td>\n",
       "      <td>6.146258e+00</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>2.972000e+01</td>\n",
       "      <td>4.954000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.072612e+02</td>\n",
       "      <td>3.360254e+01</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>1.254000e+02</td>\n",
       "      <td>2.512000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>8.805831e+02</td>\n",
       "      <td>5.693570e+02</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1.084000e+03</td>\n",
       "      <td>4.254000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.323686e-01</td>\n",
       "      <td>2.283243e-02</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>1.460000e-01</td>\n",
       "      <td>2.226000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.542650e-01</td>\n",
       "      <td>1.573365e-01</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>3.391000e-01</td>\n",
       "      <td>1.058000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.721885e-01</td>\n",
       "      <td>2.086243e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>3.829000e-01</td>\n",
       "      <td>1.252000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.146062e-01</td>\n",
       "      <td>6.573234e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>1.614000e-01</td>\n",
       "      <td>2.910000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.900756e-01</td>\n",
       "      <td>6.186747e-02</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>3.179000e-01</td>\n",
       "      <td>6.638000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>8.394582e-02</td>\n",
       "      <td>1.806127e-02</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>9.208000e-02</td>\n",
       "      <td>2.075000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count          mean           std          min  \\\n",
       "id                       569.0  3.037183e+07  1.250206e+08  8670.000000   \n",
       "radius_mean              569.0  1.412729e+01  3.524049e+00     6.981000   \n",
       "texture_mean             569.0  1.928965e+01  4.301036e+00     9.710000   \n",
       "perimeter_mean           569.0  9.196903e+01  2.429898e+01    43.790000   \n",
       "area_mean                569.0  6.548891e+02  3.519141e+02   143.500000   \n",
       "smoothness_mean          569.0  9.636028e-02  1.406413e-02     0.052630   \n",
       "compactness_mean         569.0  1.043410e-01  5.281276e-02     0.019380   \n",
       "concavity_mean           569.0  8.879932e-02  7.971981e-02     0.000000   \n",
       "concave points_mean      569.0  4.891915e-02  3.880284e-02     0.000000   \n",
       "symmetry_mean            569.0  1.811619e-01  2.741428e-02     0.106000   \n",
       "fractal_dimension_mean   569.0  6.279761e-02  7.060363e-03     0.049960   \n",
       "radius_se                569.0  4.051721e-01  2.773127e-01     0.111500   \n",
       "texture_se               569.0  1.216853e+00  5.516484e-01     0.360200   \n",
       "perimeter_se             569.0  2.866059e+00  2.021855e+00     0.757000   \n",
       "area_se                  569.0  4.033708e+01  4.549101e+01     6.802000   \n",
       "smoothness_se            569.0  7.040979e-03  3.002518e-03     0.001713   \n",
       "compactness_se           569.0  2.547814e-02  1.790818e-02     0.002252   \n",
       "concavity_se             569.0  3.189372e-02  3.018606e-02     0.000000   \n",
       "concave points_se        569.0  1.179614e-02  6.170285e-03     0.000000   \n",
       "symmetry_se              569.0  2.054230e-02  8.266372e-03     0.007882   \n",
       "fractal_dimension_se     569.0  3.794904e-03  2.646071e-03     0.000895   \n",
       "radius_worst             569.0  1.626919e+01  4.833242e+00     7.930000   \n",
       "texture_worst            569.0  2.567722e+01  6.146258e+00    12.020000   \n",
       "perimeter_worst          569.0  1.072612e+02  3.360254e+01    50.410000   \n",
       "area_worst               569.0  8.805831e+02  5.693570e+02   185.200000   \n",
       "smoothness_worst         569.0  1.323686e-01  2.283243e-02     0.071170   \n",
       "compactness_worst        569.0  2.542650e-01  1.573365e-01     0.027290   \n",
       "concavity_worst          569.0  2.721885e-01  2.086243e-01     0.000000   \n",
       "concave points_worst     569.0  1.146062e-01  6.573234e-02     0.000000   \n",
       "symmetry_worst           569.0  2.900756e-01  6.186747e-02     0.156500   \n",
       "fractal_dimension_worst  569.0  8.394582e-02  1.806127e-02     0.055040   \n",
       "Unnamed: 32                0.0           NaN           NaN          NaN   \n",
       "\n",
       "                                   25%            50%           75%  \\\n",
       "id                       869218.000000  906024.000000  8.813129e+06   \n",
       "radius_mean                  11.700000      13.370000  1.578000e+01   \n",
       "texture_mean                 16.170000      18.840000  2.180000e+01   \n",
       "perimeter_mean               75.170000      86.240000  1.041000e+02   \n",
       "area_mean                   420.300000     551.100000  7.827000e+02   \n",
       "smoothness_mean               0.086370       0.095870  1.053000e-01   \n",
       "compactness_mean              0.064920       0.092630  1.304000e-01   \n",
       "concavity_mean                0.029560       0.061540  1.307000e-01   \n",
       "concave points_mean           0.020310       0.033500  7.400000e-02   \n",
       "symmetry_mean                 0.161900       0.179200  1.957000e-01   \n",
       "fractal_dimension_mean        0.057700       0.061540  6.612000e-02   \n",
       "radius_se                     0.232400       0.324200  4.789000e-01   \n",
       "texture_se                    0.833900       1.108000  1.474000e+00   \n",
       "perimeter_se                  1.606000       2.287000  3.357000e+00   \n",
       "area_se                      17.850000      24.530000  4.519000e+01   \n",
       "smoothness_se                 0.005169       0.006380  8.146000e-03   \n",
       "compactness_se                0.013080       0.020450  3.245000e-02   \n",
       "concavity_se                  0.015090       0.025890  4.205000e-02   \n",
       "concave points_se             0.007638       0.010930  1.471000e-02   \n",
       "symmetry_se                   0.015160       0.018730  2.348000e-02   \n",
       "fractal_dimension_se          0.002248       0.003187  4.558000e-03   \n",
       "radius_worst                 13.010000      14.970000  1.879000e+01   \n",
       "texture_worst                21.080000      25.410000  2.972000e+01   \n",
       "perimeter_worst              84.110000      97.660000  1.254000e+02   \n",
       "area_worst                  515.300000     686.500000  1.084000e+03   \n",
       "smoothness_worst              0.116600       0.131300  1.460000e-01   \n",
       "compactness_worst             0.147200       0.211900  3.391000e-01   \n",
       "concavity_worst               0.114500       0.226700  3.829000e-01   \n",
       "concave points_worst          0.064930       0.099930  1.614000e-01   \n",
       "symmetry_worst                0.250400       0.282200  3.179000e-01   \n",
       "fractal_dimension_worst       0.071460       0.080040  9.208000e-02   \n",
       "Unnamed: 32                        NaN            NaN           NaN   \n",
       "\n",
       "                                  max  \n",
       "id                       9.113205e+08  \n",
       "radius_mean              2.811000e+01  \n",
       "texture_mean             3.928000e+01  \n",
       "perimeter_mean           1.885000e+02  \n",
       "area_mean                2.501000e+03  \n",
       "smoothness_mean          1.634000e-01  \n",
       "compactness_mean         3.454000e-01  \n",
       "concavity_mean           4.268000e-01  \n",
       "concave points_mean      2.012000e-01  \n",
       "symmetry_mean            3.040000e-01  \n",
       "fractal_dimension_mean   9.744000e-02  \n",
       "radius_se                2.873000e+00  \n",
       "texture_se               4.885000e+00  \n",
       "perimeter_se             2.198000e+01  \n",
       "area_se                  5.422000e+02  \n",
       "smoothness_se            3.113000e-02  \n",
       "compactness_se           1.354000e-01  \n",
       "concavity_se             3.960000e-01  \n",
       "concave points_se        5.279000e-02  \n",
       "symmetry_se              7.895000e-02  \n",
       "fractal_dimension_se     2.984000e-02  \n",
       "radius_worst             3.604000e+01  \n",
       "texture_worst            4.954000e+01  \n",
       "perimeter_worst          2.512000e+02  \n",
       "area_worst               4.254000e+03  \n",
       "smoothness_worst         2.226000e-01  \n",
       "compactness_worst        1.058000e+00  \n",
       "concavity_worst          1.252000e+00  \n",
       "concave points_worst     2.910000e-01  \n",
       "symmetry_worst           6.638000e-01  \n",
       "fractal_dimension_worst  2.075000e-01  \n",
       "Unnamed: 32                       NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for converting the column_values to unique number \n",
    "# def converstion(dataset,column_name):\n",
    "#     unique_values = dataset[column_name].unique().tolist()\n",
    "#     unique_values.sort()\n",
    "#     convert = {column_name : {k: v for k,v in zip(unique_values,list(range(0,len(unique_values))))}}\n",
    "#     dataset.replace(convert,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Drop Useless Columns </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for removing the single value column from a dataset\n",
    "def remove_value_column(dataset_name,column_name):\n",
    "    if len(dataset_name[column_name].unique()) == 1:\n",
    "        print(column_name,\"is removed\")\n",
    "        dataset_name.drop([column_name], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 32 is removed\n"
     ]
    }
   ],
   "source": [
    "for col in wf.columns:\n",
    "    remove_value_column(wf,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column(dataset, column_name):\n",
    "    dataset.drop(column_name, axis = 1, inplace = True)\n",
    "    print(column_name, \"deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id'] deleted\n"
     ]
    }
   ],
   "source": [
    "drop_column(wf,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> ENCODING  </h2>\n",
    "<h3 style='color:red;font-family:Georgia;text-align:left'> Label Encoding </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converstion(wf,\"diagnosis\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf['diagnosis'] = le.fit_transform(wf['diagnosis']) #numeric value between 0 and the number of classes minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Splitting Feature vectors and target vector  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_testing_data_split(dataset,training_data_size):\n",
    "#     # for getting the 75% training data\n",
    "#     training_data = dataset.iloc[0:int(training_data_size*len(dataset))]\n",
    "    \n",
    "#     testing_data = dataset.iloc[int(training_data_size*len(dataset)):]\n",
    "    \n",
    "#     return training_data,testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_training_testing_data_split(dataset,training_data_size):\n",
    "    \n",
    "    row_indices = np.arange(0,len(dataset)) # for shuffling the rows \n",
    "    np.random.shuffle(row_indices)\n",
    "    shuffled_dataset = dataset.iloc[row_indices]\n",
    "    \n",
    "    \n",
    "    bening_data = shuffled_dataset[shuffled_dataset['diagnosis']==0].iloc[0:training_data_size//2]\n",
    "    malignant_data = shuffled_dataset[shuffled_dataset['diagnosis']==1].iloc[0:training_data_size//2]\n",
    "    \n",
    "    training_data_balanced = pd.concat([bening_data,malignant_data])\n",
    "    \n",
    "    bening_data_remaining = shuffled_dataset[shuffled_dataset['diagnosis']==0].iloc[training_data_size//2:]\n",
    "    malignant_data_remaining = shuffled_dataset[shuffled_dataset['diagnosis']==1].iloc[training_data_size//2:]\n",
    "    \n",
    "    testing_data = pd.concat([bening_data_remaining,malignant_data_remaining])\n",
    "    \n",
    "    row_indices = np.arange(0,len(training_data_balanced)) # for shuffling the rows \n",
    "    np.random.shuffle(row_indices)\n",
    "    training_data_balanced_shuffled = training_data_balanced.iloc[row_indices]\n",
    "    \n",
    "    X_train = training_data_balanced_shuffled.iloc[:,1:]\n",
    "    y_train = training_data_balanced_shuffled.iloc[:,:1]\n",
    "    X_test = testing_data.iloc[:,1:]\n",
    "    y_test = testing_data.iloc[:,:1]\n",
    "    \n",
    "    return training_data_balanced_shuffled,testing_data,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_balanced_shuffled,testing_data,X_train,y_train,X_test,y_test =  balance_training_testing_data_split(wf,int(0.7*wf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>17.19</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>0.06527</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>...</td>\n",
       "      <td>21.58</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.38890</td>\n",
       "      <td>0.19840</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.07570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>17.14</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>...</td>\n",
       "      <td>22.25</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>18.63</td>\n",
       "      <td>25.11</td>\n",
       "      <td>124.80</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.18870</td>\n",
       "      <td>0.23190</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>...</td>\n",
       "      <td>23.15</td>\n",
       "      <td>34.01</td>\n",
       "      <td>160.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>0.61330</td>\n",
       "      <td>0.18480</td>\n",
       "      <td>0.3444</td>\n",
       "      <td>0.09782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0</td>\n",
       "      <td>14.11</td>\n",
       "      <td>12.88</td>\n",
       "      <td>90.03</td>\n",
       "      <td>616.5</td>\n",
       "      <td>0.09309</td>\n",
       "      <td>0.05306</td>\n",
       "      <td>0.01765</td>\n",
       "      <td>0.02733</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>...</td>\n",
       "      <td>15.53</td>\n",
       "      <td>18.00</td>\n",
       "      <td>98.40</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.12810</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.05307</td>\n",
       "      <td>0.05890</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.07083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0</td>\n",
       "      <td>13.69</td>\n",
       "      <td>16.07</td>\n",
       "      <td>87.84</td>\n",
       "      <td>579.1</td>\n",
       "      <td>0.08302</td>\n",
       "      <td>0.06374</td>\n",
       "      <td>0.02556</td>\n",
       "      <td>0.02031</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>...</td>\n",
       "      <td>14.84</td>\n",
       "      <td>20.21</td>\n",
       "      <td>99.16</td>\n",
       "      <td>670.6</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.13460</td>\n",
       "      <td>0.06987</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.07701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>11.71</td>\n",
       "      <td>16.67</td>\n",
       "      <td>74.72</td>\n",
       "      <td>423.6</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>...</td>\n",
       "      <td>13.33</td>\n",
       "      <td>25.48</td>\n",
       "      <td>86.16</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.12710</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.06968</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.07343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>13.64</td>\n",
       "      <td>16.34</td>\n",
       "      <td>87.21</td>\n",
       "      <td>571.8</td>\n",
       "      <td>0.07685</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>0.01857</td>\n",
       "      <td>0.01723</td>\n",
       "      <td>0.1353</td>\n",
       "      <td>...</td>\n",
       "      <td>14.67</td>\n",
       "      <td>23.19</td>\n",
       "      <td>96.08</td>\n",
       "      <td>656.7</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.10500</td>\n",
       "      <td>0.08586</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.08025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.97</td>\n",
       "      <td>102.40</td>\n",
       "      <td>744.7</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.15620</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.09113</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>...</td>\n",
       "      <td>21.20</td>\n",
       "      <td>29.41</td>\n",
       "      <td>142.10</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.55530</td>\n",
       "      <td>0.21210</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0</td>\n",
       "      <td>10.80</td>\n",
       "      <td>21.98</td>\n",
       "      <td>68.79</td>\n",
       "      <td>359.9</td>\n",
       "      <td>0.08801</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.03614</td>\n",
       "      <td>0.01404</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>...</td>\n",
       "      <td>12.76</td>\n",
       "      <td>32.04</td>\n",
       "      <td>83.69</td>\n",
       "      <td>489.5</td>\n",
       "      <td>0.13030</td>\n",
       "      <td>0.1696</td>\n",
       "      <td>0.19270</td>\n",
       "      <td>0.07485</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>0.07662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0</td>\n",
       "      <td>12.85</td>\n",
       "      <td>21.37</td>\n",
       "      <td>82.63</td>\n",
       "      <td>514.5</td>\n",
       "      <td>0.07551</td>\n",
       "      <td>0.08316</td>\n",
       "      <td>0.06126</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>...</td>\n",
       "      <td>14.40</td>\n",
       "      <td>27.01</td>\n",
       "      <td>91.63</td>\n",
       "      <td>645.8</td>\n",
       "      <td>0.09402</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.18380</td>\n",
       "      <td>0.05601</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.08151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "264          1        17.19         22.07          111.60      928.3   \n",
       "25           1        17.14         16.40          116.00      912.7   \n",
       "30           1        18.63         25.11          124.80     1088.0   \n",
       "326          0        14.11         12.88           90.03      616.5   \n",
       "374          0        13.69         16.07           87.84      579.1   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "136          0        11.71         16.67           74.72      423.6   \n",
       "51           0        13.64         16.34           87.21      571.8   \n",
       "392          1        15.49         19.97          102.40      744.7   \n",
       "427          0        10.80         21.98           68.79      359.9   \n",
       "407          0        12.85         21.37           82.63      514.5   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "264          0.09726           0.08995         0.09061              0.06527   \n",
       "25           0.11860           0.22760         0.22290              0.14010   \n",
       "30           0.10640           0.18870         0.23190              0.12440   \n",
       "326          0.09309           0.05306         0.01765              0.02733   \n",
       "374          0.08302           0.06374         0.02556              0.02031   \n",
       "..               ...               ...             ...                  ...   \n",
       "136          0.10510           0.06095         0.03592              0.02600   \n",
       "51           0.07685           0.06059         0.01857              0.01723   \n",
       "392          0.11600           0.15620         0.18910              0.09113   \n",
       "427          0.08801           0.05743         0.03614              0.01404   \n",
       "407          0.07551           0.08316         0.06126              0.01867   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "264         0.1867  ...         21.58          29.33           140.50   \n",
       "25          0.3040  ...         22.25          21.40           152.40   \n",
       "30          0.2183  ...         23.15          34.01           160.50   \n",
       "326         0.1373  ...         15.53          18.00            98.40   \n",
       "374         0.1872  ...         14.84          20.21            99.16   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "136         0.1339  ...         13.33          25.48            86.16   \n",
       "51          0.1353  ...         14.67          23.19            96.08   \n",
       "392         0.1929  ...         21.20          29.41           142.10   \n",
       "427         0.2016  ...         12.76          32.04            83.69   \n",
       "407         0.1580  ...         14.40          27.01            91.63   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "264      1436.0           0.15580             0.2567          0.38890   \n",
       "25       1461.0           0.15450             0.3949          0.38530   \n",
       "30       1670.0           0.14910             0.4257          0.61330   \n",
       "326       749.9           0.12810             0.1109          0.05307   \n",
       "374       670.6           0.11050             0.2096          0.13460   \n",
       "..          ...               ...                ...              ...   \n",
       "136       546.7           0.12710             0.1028          0.10460   \n",
       "51        656.7           0.10890             0.1582          0.10500   \n",
       "392      1359.0           0.16810             0.3913          0.55530   \n",
       "427       489.5           0.13030             0.1696          0.19270   \n",
       "407       645.8           0.09402             0.1936          0.18380   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "264               0.19840          0.3216                  0.07570  \n",
       "25                0.25500          0.4066                  0.10590  \n",
       "30                0.18480          0.3444                  0.09782  \n",
       "326               0.05890          0.2100                  0.07083  \n",
       "374               0.06987          0.3323                  0.07701  \n",
       "..                    ...             ...                      ...  \n",
       "136               0.06968          0.1712                  0.07343  \n",
       "51                0.08586          0.2346                  0.08025  \n",
       "392               0.21210          0.3187                  0.10190  \n",
       "427               0.07485          0.2965                  0.07662  \n",
       "407               0.05601          0.2488                  0.08151  \n",
       "\n",
       "[398 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_balanced_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>11.22</td>\n",
       "      <td>33.81</td>\n",
       "      <td>70.79</td>\n",
       "      <td>386.8</td>\n",
       "      <td>0.07780</td>\n",
       "      <td>0.03574</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>...</td>\n",
       "      <td>12.36</td>\n",
       "      <td>41.78</td>\n",
       "      <td>78.44</td>\n",
       "      <td>470.9</td>\n",
       "      <td>0.09994</td>\n",
       "      <td>0.06885</td>\n",
       "      <td>0.02318</td>\n",
       "      <td>0.03002</td>\n",
       "      <td>0.2911</td>\n",
       "      <td>0.07307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0</td>\n",
       "      <td>16.14</td>\n",
       "      <td>14.86</td>\n",
       "      <td>104.30</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.09495</td>\n",
       "      <td>0.08501</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>...</td>\n",
       "      <td>17.71</td>\n",
       "      <td>19.58</td>\n",
       "      <td>115.90</td>\n",
       "      <td>947.9</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>0.17220</td>\n",
       "      <td>0.23100</td>\n",
       "      <td>0.11290</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.07012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.51</td>\n",
       "      <td>97.45</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.08371</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.065050</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>...</td>\n",
       "      <td>16.41</td>\n",
       "      <td>19.31</td>\n",
       "      <td>114.20</td>\n",
       "      <td>808.2</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.36270</td>\n",
       "      <td>0.34020</td>\n",
       "      <td>0.13790</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.08362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0</td>\n",
       "      <td>14.26</td>\n",
       "      <td>18.17</td>\n",
       "      <td>91.22</td>\n",
       "      <td>633.1</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>0.05220</td>\n",
       "      <td>0.024750</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>...</td>\n",
       "      <td>16.22</td>\n",
       "      <td>25.26</td>\n",
       "      <td>105.80</td>\n",
       "      <td>819.7</td>\n",
       "      <td>0.09445</td>\n",
       "      <td>0.21670</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.07530</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>0.07676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>12.72</td>\n",
       "      <td>13.78</td>\n",
       "      <td>81.78</td>\n",
       "      <td>492.1</td>\n",
       "      <td>0.09667</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.019240</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17.48</td>\n",
       "      <td>88.54</td>\n",
       "      <td>553.7</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.14720</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>0.06343</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>17.02</td>\n",
       "      <td>23.98</td>\n",
       "      <td>112.80</td>\n",
       "      <td>899.3</td>\n",
       "      <td>0.11970</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>...</td>\n",
       "      <td>20.88</td>\n",
       "      <td>32.09</td>\n",
       "      <td>136.10</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0.16340</td>\n",
       "      <td>0.35590</td>\n",
       "      <td>0.55880</td>\n",
       "      <td>0.18470</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>0.08482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>16.65</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>...</td>\n",
       "      <td>26.46</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1</td>\n",
       "      <td>17.27</td>\n",
       "      <td>25.42</td>\n",
       "      <td>112.40</td>\n",
       "      <td>928.8</td>\n",
       "      <td>0.08331</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.057360</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>20.38</td>\n",
       "      <td>35.46</td>\n",
       "      <td>132.80</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>0.14360</td>\n",
       "      <td>0.41220</td>\n",
       "      <td>0.50360</td>\n",
       "      <td>0.17390</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.07944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.11920</td>\n",
       "      <td>0.28400</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>16.35</td>\n",
       "      <td>23.29</td>\n",
       "      <td>109.00</td>\n",
       "      <td>840.4</td>\n",
       "      <td>0.09742</td>\n",
       "      <td>0.14970</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.087730</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>...</td>\n",
       "      <td>19.38</td>\n",
       "      <td>31.03</td>\n",
       "      <td>129.30</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>0.14150</td>\n",
       "      <td>0.46650</td>\n",
       "      <td>0.70870</td>\n",
       "      <td>0.22480</td>\n",
       "      <td>0.4824</td>\n",
       "      <td>0.09614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "232          0        11.22         33.81           70.79      386.8   \n",
       "406          0        16.14         14.86          104.30      800.0   \n",
       "227          0        15.00         15.51           97.45      684.5   \n",
       "298          0        14.26         18.17           91.22      633.1   \n",
       "294          0        12.72         13.78           81.78      492.1   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "32           1        17.02         23.98          112.80      899.3   \n",
       "24           1        16.65         21.38          110.00      904.6   \n",
       "441          1        17.27         25.42          112.40      928.8   \n",
       "372          1        21.37         15.10          141.30     1386.0   \n",
       "370          1        16.35         23.29          109.00      840.4   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "232          0.07780           0.03574        0.004967             0.006434   \n",
       "406          0.09495           0.08501        0.055000             0.045280   \n",
       "227          0.08371           0.10960        0.065050             0.037800   \n",
       "298          0.06576           0.05220        0.024750             0.013740   \n",
       "294          0.09667           0.08393        0.012880             0.019240   \n",
       "..               ...               ...             ...                  ...   \n",
       "32           0.11970           0.14960        0.241700             0.120300   \n",
       "24           0.11210           0.14570        0.152500             0.091700   \n",
       "441          0.08331           0.11090        0.120400             0.057360   \n",
       "372          0.10010           0.15150        0.193200             0.125500   \n",
       "370          0.09742           0.14970        0.181100             0.087730   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "232         0.1845  ...         12.36          41.78            78.44   \n",
       "406         0.1735  ...         17.71          19.58           115.90   \n",
       "227         0.1881  ...         16.41          19.31           114.20   \n",
       "298         0.1635  ...         16.22          25.26           105.80   \n",
       "294         0.1638  ...         13.50          17.48            88.54   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "32          0.2248  ...         20.88          32.09           136.10   \n",
       "24          0.1995  ...         26.46          31.56           177.00   \n",
       "441         0.1467  ...         20.38          35.46           132.80   \n",
       "372         0.1973  ...         22.69          21.84           152.10   \n",
       "370         0.2175  ...         19.38          31.03           129.30   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "232       470.9           0.09994            0.06885          0.02318   \n",
       "406       947.9           0.12060            0.17220          0.23100   \n",
       "227       808.2           0.11360            0.36270          0.34020   \n",
       "298       819.7           0.09445            0.21670          0.15650   \n",
       "294       553.7           0.12980            0.14720          0.05233   \n",
       "..          ...               ...                ...              ...   \n",
       "32       1344.0           0.16340            0.35590          0.55880   \n",
       "24       2215.0           0.18050            0.35780          0.46950   \n",
       "441      1284.0           0.14360            0.41220          0.50360   \n",
       "372      1535.0           0.11920            0.28400          0.40240   \n",
       "370      1165.0           0.14150            0.46650          0.70870   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "232               0.03002          0.2911                  0.07307  \n",
       "406               0.11290          0.2778                  0.07012  \n",
       "227               0.13790          0.2954                  0.08362  \n",
       "298               0.07530          0.2636                  0.07676  \n",
       "294               0.06343          0.2369                  0.06922  \n",
       "..                    ...             ...                      ...  \n",
       "32                0.18470          0.3530                  0.08482  \n",
       "24                0.20950          0.3613                  0.09564  \n",
       "441               0.17390          0.2500                  0.07944  \n",
       "372               0.19660          0.2730                  0.08666  \n",
       "370               0.22480          0.4824                  0.09614  \n",
       "\n",
       "[171 rows x 31 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data_balanced_shuffled.iloc[:,1:]\n",
    "y_train = training_data_balanced_shuffled.iloc[:,:1]\n",
    "X_test = testing_data.iloc[:,1:]\n",
    "y_test = testing_data.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>17.19</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>0.06527</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>21.58</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.38890</td>\n",
       "      <td>0.19840</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.07570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.14</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>22.25</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18.63</td>\n",
       "      <td>25.11</td>\n",
       "      <td>124.80</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.18870</td>\n",
       "      <td>0.23190</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>...</td>\n",
       "      <td>23.15</td>\n",
       "      <td>34.01</td>\n",
       "      <td>160.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>0.61330</td>\n",
       "      <td>0.18480</td>\n",
       "      <td>0.3444</td>\n",
       "      <td>0.09782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>14.11</td>\n",
       "      <td>12.88</td>\n",
       "      <td>90.03</td>\n",
       "      <td>616.5</td>\n",
       "      <td>0.09309</td>\n",
       "      <td>0.05306</td>\n",
       "      <td>0.01765</td>\n",
       "      <td>0.02733</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>...</td>\n",
       "      <td>15.53</td>\n",
       "      <td>18.00</td>\n",
       "      <td>98.40</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.12810</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.05307</td>\n",
       "      <td>0.05890</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.07083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>13.69</td>\n",
       "      <td>16.07</td>\n",
       "      <td>87.84</td>\n",
       "      <td>579.1</td>\n",
       "      <td>0.08302</td>\n",
       "      <td>0.06374</td>\n",
       "      <td>0.02556</td>\n",
       "      <td>0.02031</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.05669</td>\n",
       "      <td>...</td>\n",
       "      <td>14.84</td>\n",
       "      <td>20.21</td>\n",
       "      <td>99.16</td>\n",
       "      <td>670.6</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.13460</td>\n",
       "      <td>0.06987</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.07701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>11.71</td>\n",
       "      <td>16.67</td>\n",
       "      <td>74.72</td>\n",
       "      <td>423.6</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.05945</td>\n",
       "      <td>...</td>\n",
       "      <td>13.33</td>\n",
       "      <td>25.48</td>\n",
       "      <td>86.16</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.12710</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.06968</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.07343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>13.64</td>\n",
       "      <td>16.34</td>\n",
       "      <td>87.21</td>\n",
       "      <td>571.8</td>\n",
       "      <td>0.07685</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>0.01857</td>\n",
       "      <td>0.01723</td>\n",
       "      <td>0.1353</td>\n",
       "      <td>0.05953</td>\n",
       "      <td>...</td>\n",
       "      <td>14.67</td>\n",
       "      <td>23.19</td>\n",
       "      <td>96.08</td>\n",
       "      <td>656.7</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.10500</td>\n",
       "      <td>0.08586</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.08025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>15.49</td>\n",
       "      <td>19.97</td>\n",
       "      <td>102.40</td>\n",
       "      <td>744.7</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.15620</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.09113</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.06744</td>\n",
       "      <td>...</td>\n",
       "      <td>21.20</td>\n",
       "      <td>29.41</td>\n",
       "      <td>142.10</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.55530</td>\n",
       "      <td>0.21210</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>10.80</td>\n",
       "      <td>21.98</td>\n",
       "      <td>68.79</td>\n",
       "      <td>359.9</td>\n",
       "      <td>0.08801</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.03614</td>\n",
       "      <td>0.01404</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.05977</td>\n",
       "      <td>...</td>\n",
       "      <td>12.76</td>\n",
       "      <td>32.04</td>\n",
       "      <td>83.69</td>\n",
       "      <td>489.5</td>\n",
       "      <td>0.13030</td>\n",
       "      <td>0.1696</td>\n",
       "      <td>0.19270</td>\n",
       "      <td>0.07485</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>0.07662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>12.85</td>\n",
       "      <td>21.37</td>\n",
       "      <td>82.63</td>\n",
       "      <td>514.5</td>\n",
       "      <td>0.07551</td>\n",
       "      <td>0.08316</td>\n",
       "      <td>0.06126</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06114</td>\n",
       "      <td>...</td>\n",
       "      <td>14.40</td>\n",
       "      <td>27.01</td>\n",
       "      <td>91.63</td>\n",
       "      <td>645.8</td>\n",
       "      <td>0.09402</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.18380</td>\n",
       "      <td>0.05601</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.08151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "264        17.19         22.07          111.60      928.3          0.09726   \n",
       "25         17.14         16.40          116.00      912.7          0.11860   \n",
       "30         18.63         25.11          124.80     1088.0          0.10640   \n",
       "326        14.11         12.88           90.03      616.5          0.09309   \n",
       "374        13.69         16.07           87.84      579.1          0.08302   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "136        11.71         16.67           74.72      423.6          0.10510   \n",
       "51         13.64         16.34           87.21      571.8          0.07685   \n",
       "392        15.49         19.97          102.40      744.7          0.11600   \n",
       "427        10.80         21.98           68.79      359.9          0.08801   \n",
       "407        12.85         21.37           82.63      514.5          0.07551   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "264           0.08995         0.09061              0.06527         0.1867   \n",
       "25            0.22760         0.22290              0.14010         0.3040   \n",
       "30            0.18870         0.23190              0.12440         0.2183   \n",
       "326           0.05306         0.01765              0.02733         0.1373   \n",
       "374           0.06374         0.02556              0.02031         0.1872   \n",
       "..                ...             ...                  ...            ...   \n",
       "136           0.06095         0.03592              0.02600         0.1339   \n",
       "51            0.06059         0.01857              0.01723         0.1353   \n",
       "392           0.15620         0.18910              0.09113         0.1929   \n",
       "427           0.05743         0.03614              0.01404         0.2016   \n",
       "407           0.08316         0.06126              0.01867         0.1580   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "264                 0.05580  ...         21.58          29.33   \n",
       "25                  0.07413  ...         22.25          21.40   \n",
       "30                  0.06197  ...         23.15          34.01   \n",
       "326                 0.05700  ...         15.53          18.00   \n",
       "374                 0.05669  ...         14.84          20.21   \n",
       "..                      ...  ...           ...            ...   \n",
       "136                 0.05945  ...         13.33          25.48   \n",
       "51                  0.05953  ...         14.67          23.19   \n",
       "392                 0.06744  ...         21.20          29.41   \n",
       "427                 0.05977  ...         12.76          32.04   \n",
       "407                 0.06114  ...         14.40          27.01   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "264           140.50      1436.0           0.15580             0.2567   \n",
       "25            152.40      1461.0           0.15450             0.3949   \n",
       "30            160.50      1670.0           0.14910             0.4257   \n",
       "326            98.40       749.9           0.12810             0.1109   \n",
       "374            99.16       670.6           0.11050             0.2096   \n",
       "..               ...         ...               ...                ...   \n",
       "136            86.16       546.7           0.12710             0.1028   \n",
       "51             96.08       656.7           0.10890             0.1582   \n",
       "392           142.10      1359.0           0.16810             0.3913   \n",
       "427            83.69       489.5           0.13030             0.1696   \n",
       "407            91.63       645.8           0.09402             0.1936   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "264          0.38890               0.19840          0.3216   \n",
       "25           0.38530               0.25500          0.4066   \n",
       "30           0.61330               0.18480          0.3444   \n",
       "326          0.05307               0.05890          0.2100   \n",
       "374          0.13460               0.06987          0.3323   \n",
       "..               ...                   ...             ...   \n",
       "136          0.10460               0.06968          0.1712   \n",
       "51           0.10500               0.08586          0.2346   \n",
       "392          0.55530               0.21210          0.3187   \n",
       "427          0.19270               0.07485          0.2965   \n",
       "407          0.18380               0.05601          0.2488   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "264                  0.07570  \n",
       "25                   0.10590  \n",
       "30                   0.09782  \n",
       "326                  0.07083  \n",
       "374                  0.07701  \n",
       "..                       ...  \n",
       "136                  0.07343  \n",
       "51                   0.08025  \n",
       "392                  0.10190  \n",
       "427                  0.07662  \n",
       "407                  0.08151  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = wf.iloc[:,1:] #feature vectors only --> excluding Diagnosis (target Feature)\n",
    "# y = wf.iloc[:,:1] # target feature only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n",
      "(171, 30)\n",
      "(398, 1)\n",
      "(171, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit_Transform\n",
    "\n",
    "### The (fit method) is calculating the mean and variance of each of the features present in our data.\n",
    "### The (transform method) is transforming all the features using the respective mean and variance.\n",
    "\n",
    "# transform()\n",
    "### Using the transform method we can use the same mean and variance as it is calculated from our training data to transform our test data. Thus, the parameters learned by our model using the training data will help us to transform our test data.\n",
    "\n",
    "# fit() \n",
    "### just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal object's state. Afterwards, you can call its transform() method to apply the transformation to any particular set of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Scaling  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this method, we convert variables with different scales of measurements into a single scale. StandardScaler normalizes the data using the formula (x-mean)/standard deviation\n",
    "\n",
    "### Gaussian with 0 mean and unit variance\n",
    "\n",
    "# $$ \\frac{x-\\mu}{\\sigma} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red;font-family:Georgia;text-align:center'> Normalizing  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# mmscaler = MinMaxScaler()\n",
    "# X_train = mmscaler.fit_transform(X_train)\n",
    "# X_test = mmscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "                GaussianNB,\n",
    "                LogisticRegression\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algorithms = pd.DataFrame(columns=[\"Model\", \"Train Accuracy\", \"Test Accuracy\",\"TP\",\"TN\",\"FP\",\"FN\",\"Total\",\"Accuracy\",\"Precision\",\"Recall or TPR\",\"F1-Score\",\"Specificity\",\"FPR\"])\n",
    "\n",
    "confusion_matrices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoML(algorithm): \n",
    "\n",
    "    model = algorithm().fit(X_train, y_train)\n",
    "#     print('model', model)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "#     print('train_acc',train_acc)\n",
    "    model_name = algorithm.__name__\n",
    "#     print('model_name',model_name)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "#     print('y_pred',y_pred)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "#     print('test_acc',y_pred)\n",
    "\n",
    "    actual_class = np.array(y_test)\n",
    "    actual_class = actual_class.reshape(actual_class.shape[0])\n",
    "    \n",
    "    confusion_matrix_df = pd.crosstab(actual_class,y_pred, colnames = ['Predicted'], rownames = ['Actual'], margins = True)\n",
    "    \n",
    "    # negative is 0\n",
    "    # positive is 1\n",
    "    \n",
    "    tp = confusion_matrix_df[1][1] #True Positive\n",
    "    fn = confusion_matrix_df[0][1] #False Negative\n",
    "    fp = confusion_matrix_df[1][0] #False Positive\n",
    "    tn = confusion_matrix_df[0][0] #True Negative\n",
    "    total_size = y_test.shape[0]\n",
    "    \n",
    "    # Accuracy is the number of correct predictions over all kinds of prediction\n",
    "    Accuracy  = ((tp+tn)/(tp+tn+fp+fn)) \n",
    "    \n",
    "    #Precision is a measure that tells out of positive predicted (𝑻𝑷+𝑭𝑷), what % is truly positive (𝑻𝑷).\n",
    "    Precision = (tp/(tp+fp)) \n",
    "    \n",
    "    # Recall is a measure that tells out of positive(𝑻𝑷+𝑭𝑵),what % are predicted positive (𝑻𝑷).\n",
    "    Recall_true_positive_rate    = (tp/(tp+fn))  \n",
    "    \n",
    "    #F1-Score is the Harmonic Mean of precision and recall. It takes both 𝑭𝑷 & 𝑭𝑵 into account.\n",
    "    #Alone, neither precision or recall tells the whole story.\n",
    "    #We can have really good precision with terrible recall or vice versa.\n",
    "    F1_score  = (2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))) \n",
    "    \n",
    "    #Specificity is a measure that tells out of negative(𝑻𝑵+𝑭𝑷),what % are predicted negative (𝑻𝑵).\n",
    "    Specificity = (tn/(tn+fp))\n",
    "    \n",
    "    #proportion of true negatives that are misclassified as positives. \n",
    "    #out of predicted positive(𝑻𝑵+𝑭𝑷),what % are false positive (𝑭𝑷).\n",
    "    False_positive_rate = (fp/(tn+fp)) # 1-Specificity\n",
    "   \n",
    "    return model_name, train_acc, test_acc, confusion_matrix_df, tp, tn, fp, fn, total_size, Accuracy, Precision, Recall_true_positive_rate, F1_score, Specificity, False_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg in algorithms:\n",
    "    \n",
    "    model_name, train_acc, test_acc, confusion_matrix, true_positive, true_negative, false_positive, false_negative, total_size, Accuracy, Precision, Recall_true_positive_rate, F1_score, Specificity,False_positive_rate = autoML(alg)\n",
    "    \n",
    "    df_algorithms = df_algorithms.append(\n",
    "                                        { \"Model\" : model_name,\n",
    "                                          \"Train Accuracy\": train_acc,\n",
    "                                          \"Test Accuracy\": test_acc,\n",
    "                                          \"TP\" : true_positive,\n",
    "                                          \"TN\": true_negative,\n",
    "                                          \"FP\" : false_positive,\n",
    "                                          \"FN\" : false_negative,\n",
    "                                          \"Total\" : total_size,\n",
    "                                          \"Accuracy\": Accuracy,\n",
    "                                          \"Precision\" : Precision,\n",
    "                                          \"Recall or TPR\": Recall_true_positive_rate,\n",
    "                                          \"F1-Score\": F1_score,\n",
    "                                          \"Specificity\": Specificity,\n",
    "                                          \"FPR\":False_positive_rate\n",
    "                                         }, ignore_index=True)\n",
    "    confusion_matrices[model_name] = confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Total</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall or TPR</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.918129</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0.918129</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.075949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.982412</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>13</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.981013</td>\n",
       "      <td>0.018987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Train Accuracy  Test Accuracy  TP   TN  FP FN Total  \\\n",
       "0          GaussianNB        0.932161       0.918129  11  146  12  2   171   \n",
       "1  LogisticRegression        0.982412       0.982456  13  155   3  0   171   \n",
       "\n",
       "   Accuracy  Precision  Recall or TPR  F1-Score  Specificity       FPR  \n",
       "0  0.918129   0.478261       0.846154  0.611111     0.924051  0.075949  \n",
       "1  0.982456   0.812500       1.000000  0.896552     0.981013  0.018987  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>12</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>148</td>\n",
       "      <td>23</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1  All\n",
       "Actual                 \n",
       "0          146  12  158\n",
       "1            2  11   13\n",
       "All        148  23  171"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrices[\"GaussianNB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>155</td>\n",
       "      <td>16</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1  All\n",
       "Actual                 \n",
       "0          155   3  158\n",
       "1            0  13   13\n",
       "All        155  16  171"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrices[\"LogisticRegression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(x = 'Model', y = 'Test Accuracy', data = df_algorithms)\n",
    "plt.title('Testing Accuracy for each Model', fontsize = 15)\n",
    "plt.ylim(0.8,1.005)\n",
    "plt.xlabel('Model', fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to determine whether tumor is <font color='Blue'>*Benign*</font> or <font color='red'>*Malignant*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ~ _Logistic Regression_ is a \"Discriminative Learning Model\" that searches for the Decision Boundar(ies) which can  properly segregate random data point(s) to their respective correct category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is useful when the response variable is binary but the _explanatory variables_ (feature varaibles) are continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Types of Logistic Regression_\n",
    "#### 1. Binary Logistic Regression\n",
    ">The categorical response has only two 2 possible outcomes. __Example:__ Tunor is Benign or Malignant\n",
    "#### 2. Multinomial Logistic Regression\n",
    ">Three or more categories without ordering. __Example:__ Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "#### 3. Ordinal Logistic Regression\n",
    ">Three or more categories with ordering. __Example:__ Novel or Movie rating from 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we know, if divde both the posterior probabilities\n",
    "> # $\n",
    "\\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{Blue}{P(C = 0 | X = x_i)}} = \\color{Magenta}{\\frac{P(X = x_i|C = 1)}{P(X = x_i|C = 0)}}\n",
    "$\n",
    " ### where $ C $ is the class/label _(target variable)_  which can be only $ \\color{Blue}{0\\;(Benign)} $   or  $ \\color{Red}{1 \\; (Malignant)} $  & $ X $ is the _Feature Vector_  in which $ x_i $ is representing single feature of the $ i^{th} $ tumor in the feature vector $ X $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # $\n",
    "\\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{Blue}{P(C = 0 | X = x_i)}} = \\color{Magenta}{\\frac{\\frac{1}{\\sqrt{2\\pi}\\hat{{\\sigma_M}}}e^-\\frac{(x_i-\\hat{\\mu_M})^2}{2\\hat{\\sigma_M}^2}}{\\frac{1}{\\sqrt{2\\pi}\\hat{{\\sigma_B}}}e^-\\frac{(x_i-\\hat{\\mu_B})^2}{2\\hat{\\sigma_B}^2}}}\n",
    "$\n",
    "## if we solve this above equation with the an assumption of homoscedasticity means _same variance_ \n",
    "\n",
    ">## $ {\\hat{\\sigma_B}^2} \\approx {\\hat{\\sigma_M}^2} $  or $ {\\hat{\\sigma_B}^2} \\approx {\\hat{\\sigma_M}^2} \\Rightarrow {\\hat{\\sigma_P}^2} $ where $ {\\hat{\\sigma_P}^2} $ is pooled variance  \n",
    "\n",
    "## we will have \n",
    "\n",
    "># $ \\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{Blue}{P(C = 0 | X = x_i)}} = \\color{magenta}{e^{\\begin{pmatrix} \\begin{pmatrix} \\frac{x_i(\\hat{\\mu}_M + \\hat{\\mu}_B)}{\\hat{\\sigma_P}^2}\\end{pmatrix}+\\begin{pmatrix}-\\frac{(\\hat{\\mu}_M - \\hat{\\mu}_B)}{2\\hat{\\sigma_P}^2}\\end{pmatrix} \\end{pmatrix}}} $\n",
    "\n",
    "## for simplification we can take these number as\n",
    "># $ \\hat{\\theta}_1 = \\begin{pmatrix} \\frac{x_i(\\hat{\\mu}_M + \\hat{\\mu}_B)}{\\hat{\\sigma_P}^2} \\end{pmatrix} , \\hat{\\theta}_0 = -\\begin{pmatrix} -\\frac{(\\hat{\\mu}_M - \\hat{\\mu}_B)}{2\\hat{\\sigma_P}^2} \\end{pmatrix} $\n",
    "\n",
    "## so we can write \n",
    "> # $\n",
    "\\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{Blue}{P(C = 0 | X = x_i)}} = \\color{magenta}{e^{({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}} \\rightarrow  eq.1$ \n",
    "\n",
    "## if we apply log on both the sides of eq.1 we will see\n",
    "> # $ log_e{\\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{blue}{P(C = 0 | X = x_i)}} =log_e{ \\color{magenta}{e^{({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}}}} $\n",
    "> # $ log_e{\\frac{\\color{red}{P(C = 1|X = x_i)}}{\\color{blue}{P(C = 0 | X = x_i)}} =\\color{magenta}{({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And  $ \\color{magenta} {\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}}$ is known as disicriminant score <br><br> which is the input of the sigmoid function $ f(z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:DarkBlue;font-family:Georgia;text-align:center'> Sigmoid Function  </h1>\n",
    "\n",
    "# $${f(z) = \\frac{1}{1+e^{-(z)}}}$$\n",
    "### $ \\text{If ‘z’ goes to infinity, f(z) will become 1 and if ‘z’ goes to negative infinity, f(z) will become 0.}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(z):\n",
    "    \n",
    "    f_of_z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return f_of_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001006770820085637\n",
      "0.5\n",
      "0.9989932291799144\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_func(-6.9))\n",
    "print(sigmoid_func(0))\n",
    "print(sigmoid_func(6.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "z = 1/(1 + np.exp(-x))  \n",
    "plt.plot(x, z, lw = 3)\n",
    "\n",
    "myfont = {'fontsize':15 , 'fontname':'sans-serif'}\n",
    "\n",
    "plt.xlabel(r'$z$ (input)',fontdict=myfont)\n",
    "plt.ylabel(\"Sigmoid Function \" r'$f(z)$', fontdict=myfont)\n",
    "# plt.axvline(x = 0, ymin = 0, ymax = 1, color = \"#ff5700\" , label = r'$\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}$ is the Decision Boundary' '\\nat ' r'$z = 0$', ls = \"--\")\n",
    "# plt.axvline(x = 0, ymin = 0, ymax = 1, color = \"magenta\" , label = r'$\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}$ is the Decision Boundary' '\\nat ' r'$z = 0$', ls = \"--\")\n",
    "# plt.axhline(y = 0.5, color = \"green\" ,label = r'& at $z = 0$ & the value of $ f(z) $ is $0.5$', ls = \"--\")\n",
    "# plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper left',fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the intuition in case of single feature \n",
    "### If we take $ z=\\color{magenta} {\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "z = 1/(1 + np.exp(-x))  \n",
    "plt.plot(x, z, lw = 3)\n",
    "\n",
    "myfont = {'fontsize':15 , 'fontname':'sans-serif'}\n",
    "\n",
    "plt.xlabel(\"Single feature X (any feature)\",fontdict=myfont)\n",
    "plt.ylabel(\"Likelihood Probability r'$ (X|C) $\", fontdict=myfont)\n",
    "# plt.axvline(x = 0, ymin = 0, ymax = 1, color = \"#ff5700\" , label = r'$\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}$ is the Decision Boundary' '\\nat ' r'$z = 0$', ls = \"--\")\n",
    "plt.axvline(x = 0, ymin = 0, ymax = 1, color = \"magenta\" , label = r'$\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}$ is the Decision Boundary' '\\nat ' r'$z = 0$', ls = \"--\")\n",
    "plt.axhline(y = 0.5, color = \"green\" ,label = r'& at $z = 0$ & the value of $ f(z) $ is $0.5$', ls = \"--\")\n",
    "plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper left',fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single feature\n",
    "###  for any $ i^{th }$ tumor the value of \n",
    ">### $ \\color{magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}} > 0 \\rightarrow $ _Malignant_ & $ \\color{magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x_i}} < 0 \\rightarrow $ _Bening_\n",
    "\n",
    "## And in general \n",
    ">## $ \\color{Magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x}} $ _is known as Linear Discriminant Function_\n",
    "\n",
    "###  if you have single feature, your discriminant function will be $ \\color{Magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x_1}} $\n",
    "### if you have two features, your discriminant function will be $ \\color{Magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x_1} +\\hat{\\theta}_2{x_2}} $\n",
    "### if you have _'M'_  features, your discriminant function will be \n",
    ">## $ \\color{Magenta}{\\hat{\\theta}_0 +\\hat{\\theta}_1{x_1} +\\hat{\\theta}_2{x_2} + \\dots   +\\hat{\\theta}_n{x_n}} $ or  $ \\color{Magenta}{\\hat{\\theta}_0} + \\sum \\limits_{j=1}^{M}\\color{Magenta}{\\hat{\\theta}_j{x_j}} $ \n",
    "\n",
    "# In vector form it can be written like\n",
    "\n",
    "> ## $ \\hat{\\theta}_0 +\\hat{\\theta}^T{X} $\n",
    "\n",
    "> ## where $ \\hat{\\theta}_0 , \\hat{\\theta} =\\begin{bmatrix}\n",
    "                                                                {\\hat{\\theta}_1\\\\\n",
    "                                                                 \\hat{\\theta}_2\\\\\n",
    "                                                                     \\vdots\\\\\n",
    "                                                                 \\hat{\\theta}_M}\n",
    "                                                         \\end{bmatrix} $ are the parameters of the function    \n",
    "> ## and $ X $ is the feature vector $ \\begin{bmatrix}\n",
    "                                                                {x_1^i\\\\\n",
    "                                                                 x_2^i\\\\\n",
    "                                                                     \\vdots\\\\\n",
    "                                                                 x_M^i}\n",
    "                                                                \\end{bmatrix}$\n",
    "                                                                \n",
    "\n",
    "### you can write this $ \\hat{\\theta}_0 $ inside the $ \\hat{\\theta} $ vector by taking the one more value in feature vector $ X $ as $ x_0 = 1 $\n",
    "### $$ \\text{and then } \\hat{\\theta} \\text{ & } X \\text{ will look like} \\\\ \n",
    "                                                        \\begin{bmatrix}\n",
    "                                                                {\\hat{\\theta}_0\\\\\n",
    "                                                                 \\hat{\\theta}_1\\\\\n",
    "                                                                     \\vdots\\\\\n",
    "                                                                 \\hat{\\theta}_M}\n",
    "                                                         \\end{bmatrix}\n",
    "                                                        \\begin{bmatrix}\n",
    "                                                                {1\\\\\n",
    "                                                                 x_1^i\\\\\n",
    "                                                                     \\vdots\\\\\n",
    "                                                                 x_M^i}\n",
    "                                                         \\end{bmatrix}\n",
    "                                                                \\\\\n",
    "                                                                \\hat{\\theta}^T{X}\n",
    "                                                                $$\n",
    "\n",
    "## In the perspective of _Linear Disicriminant Analysis (LDA) as Dimenstionality Reduction_ this $ \\hat{\\theta} $ vector is a unit vector on which we project of X feature vector to minimize the within group variance or cluster varaince (individual class variance) and maximize the between group variance (variance between classes) to classify our data with high number of probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$$$\n",
    "\n",
    "# Formulation of cost funtion of Logistic Regression\n",
    "### we know from $eq.1$\n",
    "> # $\n",
    "\\frac{\\color{red}{P(C = 1|X=x_i)}}{\\color{Blue}{P(C = 0 |X=x_i)}} = \\color{magenta}{e^{({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}} \\rightarrow  eq.1$ \n",
    "\n",
    "### and we can write dinominator as\n",
    "> # $ \\color{Red}{\n",
    "\\frac{P(C = 1|X_i)}{1 - P(C = 1 | X_i)}} =  \\color{magenta}{e^{({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}}\n",
    "$\n",
    "\n",
    "### and further more simplification will give the posterior probability of tumor being <font color='Red'>*Malignant*</font>\n",
    "> # $\\color{red}{P (C = 1|X_i)} = \\frac{1}{1+\\color{magenta}{e^{-({\\hat{\\theta}_1{x_i}+\\hat{\\theta}_0})}}} \\rightarrow eq.2 $  \n",
    "\n",
    "### therefore, the Posterior Probabiltiy of same tumor being <font color='Blue'>*Benign*</font> can be written as :\n",
    "> # $\\color{Blue}{P(C = 0 | X_i)}=[{1-P (C = 1|X_i)}] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the value of $ C \\in \\begin{Bmatrix}0,1\\end{Bmatrix} $ <br><br> Interesting thing to observe here that it is very much similar to the Binomial Distribution in which we have only two possible outcomes<br><br>  so if we write the above formula in the form of Binomial PDF \n",
    "\n",
    "> ## $\\color{red}{P (C = 1|X_i)} =\\begin{bmatrix}{P (C = 1|X_i)}\\end{bmatrix}^1\\bullet\\begin{bmatrix}{1-P (C = 1|X_i)}\\end{bmatrix}^0$\n",
    "## We can write in more general form by replacing $'1'$ with $C_i$ where $ C_i \\in \\begin{Bmatrix}0,1\\end{Bmatrix} $\n",
    "> ## $\\color{#ff5700}{P (C = C_i|X = X_i) =\\begin{bmatrix}{P (C = C_i|X=X_i)}\\end{bmatrix}^{\\large{C_i}}\\bullet\\begin{bmatrix}{1-P (C = C_i|X=X_i)}\\end{bmatrix}^{\\large{(1-C_i)}}}$\n",
    "### this was for single example (single row) in the training data <br><br> Suppose, there are $\\large{N_{train}}$ number of tumors in training data (whole training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now if we calculate the Joint Probabilty\n",
    "\n",
    "> ### $$ \\begin{bmatrix}P(C=C_1|X_1)\\,\\cap\\,P(C=C_2|X_2)\\,\\cap\\,P(C=C_3|X_3)\\,\\cap\\,\\dots\\cap\\,P(C=C_{N_{train}}|X_{N_{train}})\\end{bmatrix} $$\n",
    "## Taking the weak assumption of IID _(Independent and Indentical Distributed)_\n",
    "> # $$ {\\prod}_{i=1}^{N_{train}} P(C=C_i|X_i)$$\n",
    "## This above expression is also known as Likelihood Function which can be further written as\n",
    "> # $$ {\\prod}_{i=1}^{N_{train}} {\\scriptsize{\\begin{bmatrix}{P(C=C_i|X_i)}\\end{bmatrix}^{\\normalsize{C_i}}\\bullet\\begin{bmatrix}{1-P (C = C_i|X=X_i)}\\end{bmatrix}^{\\normalsize{(1-C_i)}}}} $$\n",
    "\n",
    "\n",
    "\n",
    "## can be written as\n",
    "> # $$ L(\\hat{\\theta}_0,\\hat{\\theta})={\\prod}_{i=1}^{N_{train}} {\\begin{bmatrix}\\normalsize{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{bmatrix}^{\\normalsize{C_i}}}\\bullet{\\begin{bmatrix}\\normalsize{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{bmatrix}^{\\normalsize{(1-C_i)}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Therefore the _Log Likelihood Function_ will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## $$ \\color{#ff5700}{log_eL(\\hat{\\theta}_0,\\hat{\\theta})={\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}log_e{\\begin{pmatrix}\\large{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}^{\\normalsize{C_i}}}+ log_e{\\begin{pmatrix}\\large{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}^{\\normalsize{(1-C_i)}}} \\end{bmatrix}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function can also be considered as a PDF whose best estimates of population parameters are given by $\\hat{\\theta}_0,\\hat{\\theta}$ <br>Now, Maximum Likelihood Estimation will be used to find the best estimates of population parameters of Sigmoid Function. <br>Therefore, the Log Likelihood Function now needs to be maximized using Gradient Ascent Algorithm. \n",
    "\n",
    "## so now our optimization problem becomes\n",
    "\n",
    "> # $$ \\max_{\\hat{\\theta}_0,\\hat{\\theta}} log_eL(\\hat{\\theta}_0,\\hat{\\theta})$$\n",
    "\n",
    "## But usually, every function is minimized that’s why we put negative sign right before the Log Likelihood Function because Log Likelihood Function is Concave and negative sign is going to make it Convex.\n",
    "\n",
    "> # $$ \\min_{\\hat{\\theta}_0,\\hat{\\theta}} - log_eL(\\hat{\\theta}_0,\\hat{\\theta})$$\n",
    "\n",
    "### $$ \\large{\\color{#ff5700}{-log_eL(\\hat{\\theta}_0,\\hat{\\theta})={-\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}log_e{\\begin{pmatrix}\\large{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}^{\\normalsize{C_i}}} + log_e{\\begin{pmatrix}\\large{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}^{\\normalsize{(1-C_i)}}} \\end{bmatrix}}}$$\n",
    "\n",
    "### $$ \\large{\\color{#ff5700}{-log_eL(\\hat{\\theta}_0,\\hat{\\theta})={-\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}{\\normalsize{C_i}}\\,log_e{\\begin{pmatrix}\\large{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}}+ {\\normalsize{(1-C_i)}}\\,log_e{\\begin{pmatrix}\\large{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}} \n",
    "\\end{bmatrix}}}$$\n",
    "\n",
    "### $$ \\normalsize{\\color{#ff5700}{-log_eL(\\hat{\\theta}_0,\\hat{\\theta})={-\\frac{1}{N_{train}}\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}{\\normalsize{C_i}}\\,log_e{\\begin{pmatrix}\\large{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}}+ {\\normalsize{(1-C_i)}}\\,log_e{\\begin{pmatrix}\\large{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}} \n",
    "\\end{bmatrix}}}$$\n",
    "\n",
    "\n",
    "# Now we can solve this via <font color = 'Red' >Gradient Descent Algorithm </font>\n",
    "## This negative log Likelihood function is known by many names is datasceince and machien learning\n",
    "\n",
    "> ## _Cost Function_\n",
    "> ## _Loss Function_\n",
    "> ## _Binary Cross Entropy Loss_\n",
    "> ## _Categorical Cross Entropy Loss_\n",
    "> ## _Log Loss_\n",
    "> ## _Kullback Liebler Divergence (KL Divergence)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'Red' > For Gradient Descent Algorithm we need the gradient vector</font>\n",
    "### we need to calculate the derivative of the cost function with respect to $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the gradient of the loss function\n",
    "### $$ \\normalsize{\\color{#ff5700}{-log_eL(\\hat{\\theta}_0,\\hat{\\theta})={-\\frac{1}{N_{train}}\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}{\\normalsize{C_i}}\\,log_e{\\begin{pmatrix}\\large{\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}}+ {\\normalsize{(1-C_i)}}\\,log_e{\\begin{pmatrix}\\large{1-\\frac{1}{1+{e^{-\\begin{pmatrix}\\hat{\\theta}_{\\normalsize{0}}+\\hat{\\theta}^{\\normalsize{T}}{X_i}\\end{pmatrix}}}}}\\end{pmatrix}} \n",
    "\\end{bmatrix}}}$$\n",
    "> ### Generalized form\n",
    "### $$ \\normalsize{-log_eL(\\hat{\\theta}_0,\\hat{\\theta})={-\\frac{1}{N_{train}}\\sum}_{i=1}^{N_{train}} \\begin{bmatrix}{\\normalsize{C_i}}\\,log_eP{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}+ {\\normalsize{(1-C_i)}}\\,log_e\\begin{pmatrix}1-P{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}\\end{pmatrix}\n",
    "\\end{bmatrix}}$$\n",
    "> ### Derivative of the loss function with respect to\n",
    ">## $ \n",
    "{\\Large\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}_j}}}{\\normalsize{\\;=\\;}} x^i_j {\\scriptsize{\\:\\bullet\\:}} C_i \\begin{pmatrix}1-P{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}\\end{pmatrix} -  x^i_j {\\scriptsize{\\:\\bullet\\:}} (1-C_i) {\\scriptsize{\\:\\bullet}} \\begin{pmatrix}P{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}\\end{pmatrix}\n",
    "$\n",
    "> ### Further caclulation will lead to\n",
    ">## $ \n",
    "{\\Large\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}_j}}}{\\normalsize{\\;=\\;}} x^i_j {\\scriptsize{\\:\\bullet}} \\begin{bmatrix}C_i -P{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}} \\end{bmatrix}\n",
    "$\n",
    "> ### Put negative sign & submission back\n",
    ">## $ \n",
    "{\\Large\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}_j}}}{\\normalsize{\\;=\\;}}{-\\sum}_{i=1}^{N_{train}}   \\begin{bmatrix}C_i -P{\\begin{pmatrix}X_i;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}  \\end{bmatrix} {\\scriptsize{\\bullet \\:}}x^i_j\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's convert this into vector form _(Linear Algebra)_ to perform fast calculation in python programming\n",
    "\n",
    ">## $ C \\rightarrow \n",
    "        \\begin{bmatrix}\n",
    "                {C_1\\\\\n",
    "                 C_2\\\\\n",
    "                     \\vdots\\\\\n",
    "                 C_{N_{train}}}\n",
    "        \\end{bmatrix} \\Rightarrow N \\times 1\n",
    "        \\\\\n",
    "        P \\rightarrow \n",
    "        \\begin{bmatrix}\n",
    "                {P{\\begin{pmatrix}X_1;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}\\\\\n",
    "                 P{\\begin{pmatrix}X_2;\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}\\\\\n",
    "                     \\vdots\\\\\n",
    "                 P{\\begin{pmatrix}X_{N_{train}};\\hat{\\theta}_0,\\hat{\\theta}\\end{pmatrix}}}\n",
    "        \\end{bmatrix} \\Rightarrow N \\times 1 \n",
    "        \\\\\n",
    "     X \\rightarrow \n",
    "         \\begin{bmatrix} \n",
    "             x_1^1 & x_1^2 & \\dots & x_1^{N_{train}} \\\\\n",
    "             x_2^1 & x_2^2 & \\dots & x_2^{N_{train}} \\\\\n",
    "             \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "             x_M^{N_{train}} & x_M^{N_{train}} & \\dots & x_M^{N_{train}} \\end{bmatrix} \\Rightarrow M \\times N\n",
    "        \\\\\n",
    "        X^T \\rightarrow \n",
    "         \\begin{bmatrix} \n",
    "             x_1^1 & x_2^1 & \\dots & x_M^1 \\\\\n",
    "             x_1^2 & x_2^2 & \\dots & x_M^2 \\\\\n",
    "             \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "             x_1^{N_{train}} & x_2^{N_{train}} & \\dots & x_M^{N_{train}} \\end{bmatrix} \\Rightarrow N \\times M\n",
    "     \\\\\n",
    "         {\\hat{\\theta}} \\rightarrow \n",
    "        \\begin{bmatrix}\n",
    "                {\\hat{\\theta}_1\\\\\n",
    "                 \\hat{\\theta}_2\\\\\n",
    "                     \\vdots\\\\\n",
    "                 \\hat{\\theta}_M}\n",
    "        \\end{bmatrix} \\Rightarrow M \\times 1\n",
    "        $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## $ {\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}} \\Rightarrow N \\times 1 \\\\\n",
    "         {\\large{e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}  \\Rightarrow N \\times 1 \\\\\n",
    "         {\\normalsize{1+e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}  \\Rightarrow N \\times 1\\\\\n",
    "         {\\Large{\\frac{1}{1+e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}}  \\Rightarrow N \\times 1 \\\\\n",
    "            C-\\begin{bmatrix}{\\Large{{\\frac{1}{1+e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}}}\\end{bmatrix}\\Rightarrow N \\times 1 \\\\ \n",
    "            \\begin{bmatrix}C-\\begin{bmatrix}{\\Large{{\\frac{1}{1+e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}}}\\end{bmatrix}\\end{bmatrix}^T\\Rightarrow 1 \\times N \\\\ \n",
    "            \\begin{bmatrix}C-\\begin{bmatrix}{\\Large{{\\frac{1}{1+e^{-\\begin{pmatrix}{\\hat{\\theta}_0} + X^T {\\scriptsize{\\bullet \\:}} {\\hat{\\theta}}\\end{pmatrix}}}}}}\\end{bmatrix}\\end{bmatrix}^T\\bullet X^T \\Rightarrow 1 \\times M\\\\\n",
    "             \\begin{bmatrix}C - P\\end{bmatrix}^T \\bullet X^T \\Rightarrow 1 \\times M\n",
    "        $\n",
    ">## $ \n",
    "{\\large\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}}}}{\\normalsize{\\;=\\;}}  \\begin{bmatrix}C - P\\end{bmatrix}^T \\bullet X^T \\Rightarrow 1 \\times M\\\\\n",
    "$\n",
    "## $$ \n",
    "{\\normalsize\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}}}}{\\normalsize{\\;=\\;}} {\\normalsize{-\\frac{1}{N_{train}}}} \\begin{bmatrix}C - P\\end{bmatrix}^T \\bullet X^T \\Rightarrow 1 \\times M\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_train_decision_boundaries(X_transpose,theta_hat_aka_weights,theta_0_hat_aka_intercept):\n",
    "    \n",
    "    # dot product\n",
    "    dot_product_X_transpose_theta = np.dot(X_transpose,theta_hat_aka_weights) # N x 1 vector\n",
    "    \n",
    "    # adding theta_0 through broad casting\n",
    "    decision_boundaries = theta_0_hat_aka_intercept + dot_product_X_transpose_theta # N x 1 vector\n",
    "    \n",
    "    return decision_boundaries  # N x 1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_P_vector(decision_boundaries,N_train):\n",
    "    P_vector = np.zeros((N_train, 1))\n",
    "#     print(P_vector.shape)\n",
    "    sigmoid_values={}\n",
    "\n",
    "    for i in range(0,len(decision_boundaries)):\n",
    "#         print(i)\n",
    "#         print(decision_boundaries[i])\n",
    "        sigmoid_values[i] = sigmoid_func(decision_boundaries[i])\n",
    "        \n",
    "        P_vector[i]=list(sigmoid_values.values())[i]\n",
    "    return P_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_vector = np.array(y_train) # N x 1\n",
    "C_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the gradients\n",
    "def gradient_vector(X_transpose, C_vector, P_vector):\n",
    "\n",
    "    subtracted_vector = C_vector - P_vector # N x 1\n",
    "    \n",
    "    dot_product_sub_vec_and_X_trans = np.dot(subtracted_vector.T,X_transpose) # 1 x N times N x M -----> 1 x M\n",
    "    \n",
    "    del_by_del_theta_hat = - ((1/N_train)*dot_product_sub_vec_and_X_trans) # 1 x M\n",
    "    \n",
    "    del_by_del_theta_hat_transpose = del_by_del_theta_hat.T # for subrtracting from 30 x 1 theta_hat_aka_weights_initial which was M x 1\n",
    "    \n",
    "    return del_by_del_theta_hat_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_function(C_vector,P_vector):\n",
    "    \n",
    "    return -(np.matmul(C_vector.T,np.log(P_vector)) + np.matmul((1-C_vector).T,np.log(1-P_vector)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transpose = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N or N_train 398\n",
      "M 30\n"
     ]
    }
   ],
   "source": [
    "N_train, M = X_transpose.shape # N is N_train (number of rows) & M is Number of columns\n",
    "print(\"N or N_train\",N_train)\n",
    "print(\"M\",M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "> ### $ {\\hat{\\theta}_0}_{initial} = 0 \\rightarrow \\text{theta_0_hat_aka_intercept_initial} \\\\ \n",
    "{\\hat{\\theta}}_{initial} = \\begin{bmatrix}\n",
    "                {0\\\\\n",
    "                 0\\\\\n",
    "                     \\vdots\\\\\n",
    "                 0}\n",
    "        \\end{bmatrix} \\Rightarrow M \\times 1 \\rightarrow \\text{theta_hat_aka_weights_initial}\\\\\n",
    "        \\epsilon = 10^{-2} \\text{ to } 10^{-5} \\rightarrow \\text{epsilon learning Rate} \\\\\n",
    "        tolerance = 10^{-2} \\text{ to } 10^{-5} \\\\ \n",
    "        $ \n",
    ">> ### $\n",
    "        \\text{while (True)} \n",
    "        \\\\\n",
    "        {\\hat{\\theta}_0}_{final} = {\\hat{\\theta}_0}_{initial} - \\epsilon \\bullet {\\large\\color{red}{\\frac{\\partial}{\\partial\\,\\hat{\\theta}_0}}}\\Bigr|_{\\substack{{\\hat{\\theta}_0}_{initial}\\\\{\\hat{\\theta}}_{initial}}} \n",
    "        \\\\\n",
    "        {\\hat{\\theta}}_{final} = {\\hat{\\theta}}_{initial} - \\epsilon \\bullet {\\large\\color{red}{\\nabla{\\hat{\\theta}}_{initial}}}\\Bigr|_{\\substack{{\\hat{\\theta}_0}_{initial}\\\\{\\hat{\\theta}}_{initial}}} \n",
    "        \\\\\n",
    "        \\text{if }(f({\\hat{\\theta}}_{initial}) - f({\\hat{\\theta}}_{final})) <tolerance\n",
    "        $\n",
    ">>> ### $  converged  \\\\ break$\n",
    ">> ### $\\text{else} $\n",
    ">>> ### $ {\\hat{\\theta}_0}_{initial} = {\\hat{\\theta}_0}_{final} \n",
    "        \\\\\n",
    "        {\\hat{\\theta}}_{initial} = {\\hat{\\theta}}_{final}\n",
    "        $       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0_hat_aka_intercept_initial = 0\n",
    "\n",
    "theta_hat_aka_weights_initial = np.zeros((M,1)) # M x 1 vector of zeros\n",
    "\n",
    "epsilon_learning_rate = 10**(-3) # 1 / f'' \n",
    "\n",
    "tolerance = 10**(-3)\n",
    "\n",
    "iterations  = list()\n",
    "\n",
    "neg_log_loss_history = list()\n",
    "\n",
    "iteration_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  0 Cross Entropy Loss= 275.87257786285824\n",
      "iteration number =  1 Cross Entropy Loss= 275.02855986472036\n",
      "iteration number =  2 Cross Entropy Loss= 274.190064508904\n",
      "iteration number =  3 Cross Entropy Loss= 273.3570546848525\n",
      "iteration number =  4 Cross Entropy Loss= 272.5294933049503\n",
      "iteration number =  5 Cross Entropy Loss= 271.70734331150044\n",
      "iteration number =  6 Cross Entropy Loss= 270.89056768358614\n",
      "iteration number =  7 Cross Entropy Loss= 270.07912944381206\n",
      "iteration number =  8 Cross Entropy Loss= 269.2729916649238\n",
      "iteration number =  9 Cross Entropy Loss= 268.4721174763012\n",
      "iteration number =  10 Cross Entropy Loss= 267.676470070324\n",
      "iteration number =  11 Cross Entropy Loss= 266.8860127086069\n",
      "iteration number =  12 Cross Entropy Loss= 266.10070872810206\n",
      "iteration number =  13 Cross Entropy Loss= 265.3205215470671\n",
      "iteration number =  14 Cross Entropy Loss= 264.545414670896\n",
      "iteration number =  15 Cross Entropy Loss= 263.7753516978132\n",
      "iteration number =  16 Cross Entropy Loss= 263.0102963244277\n",
      "iteration number =  17 Cross Entropy Loss= 262.25021235114644\n",
      "iteration number =  18 Cross Entropy Loss= 261.495063687447\n",
      "iteration number =  19 Cross Entropy Loss= 260.7448143570073\n",
      "iteration number =  20 Cross Entropy Loss= 259.9994285026935\n",
      "iteration number =  21 Cross Entropy Loss= 259.2588703914037\n",
      "iteration number =  22 Cross Entropy Loss= 258.5231044187691\n",
      "iteration number =  23 Cross Entropy Loss= 257.79209511371096\n",
      "iteration number =  24 Cross Entropy Loss= 257.06580714285514\n",
      "iteration number =  25 Cross Entropy Loss= 256.344205314803\n",
      "iteration number =  26 Cross Entropy Loss= 255.62725458425962\n",
      "iteration number =  27 Cross Entropy Loss= 254.914920056021\n",
      "iteration number =  28 Cross Entropy Loss= 254.20716698881864\n",
      "iteration number =  29 Cross Entropy Loss= 253.50396079902492\n",
      "iteration number =  30 Cross Entropy Loss= 252.80526706421878\n",
      "iteration number =  31 Cross Entropy Loss= 252.1110515266128\n",
      "iteration number =  32 Cross Entropy Loss= 251.42128009634393\n",
      "iteration number =  33 Cross Entropy Loss= 250.73591885462824\n",
      "iteration number =  34 Cross Entropy Loss= 250.05493405678172\n",
      "iteration number =  35 Cross Entropy Loss= 249.3782921351086\n",
      "iteration number =  36 Cross Entropy Loss= 248.70595970165851\n",
      "iteration number =  37 Cross Entropy Loss= 248.03790355085494\n",
      "iteration number =  38 Cross Entropy Loss= 247.37409066199587\n",
      "iteration number =  39 Cross Entropy Loss= 246.71448820162982\n",
      "iteration number =  40 Cross Entropy Loss= 246.05906352580766\n",
      "iteration number =  41 Cross Entropy Loss= 245.40778418221356\n",
      "iteration number =  42 Cross Entropy Loss= 244.76061791217674\n",
      "iteration number =  43 Cross Entropy Loss= 244.11753265256567\n",
      "iteration number =  44 Cross Entropy Loss= 243.47849653756805\n",
      "iteration number =  45 Cross Entropy Loss= 242.8434779003573\n",
      "iteration number =  46 Cross Entropy Loss= 242.2124452746495\n",
      "iteration number =  47 Cross Entropy Loss= 241.58536739615164\n",
      "iteration number =  48 Cross Entropy Loss= 240.96221320390447\n",
      "iteration number =  49 Cross Entropy Loss= 240.34295184152163\n",
      "iteration number =  50 Cross Entropy Loss= 239.72755265832825\n",
      "iteration number =  51 Cross Entropy Loss= 239.11598521040034\n",
      "iteration number =  52 Cross Entropy Loss= 238.5082192615084\n",
      "iteration number =  53 Cross Entropy Loss= 237.90422478396687\n",
      "iteration number =  54 Cross Entropy Loss= 237.3039719593924\n",
      "iteration number =  55 Cross Entropy Loss= 236.7074311793729\n",
      "iteration number =  56 Cross Entropy Loss= 236.1145730460497\n",
      "iteration number =  57 Cross Entropy Loss= 235.52536837261624\n",
      "iteration number =  58 Cross Entropy Loss= 234.9397881837338\n",
      "iteration number =  59 Cross Entropy Loss= 234.35780371586856\n",
      "iteration number =  60 Cross Entropy Loss= 233.77938641755094\n",
      "iteration number =  61 Cross Entropy Loss= 233.20450794956028\n",
      "iteration number =  62 Cross Entropy Loss= 232.633140185037\n",
      "iteration number =  63 Cross Entropy Loss= 232.06525520952414\n",
      "iteration number =  64 Cross Entropy Loss= 231.50082532094137\n",
      "iteration number =  65 Cross Entropy Loss= 230.93982302949263\n",
      "iteration number =  66 Cross Entropy Loss= 230.38222105751038\n",
      "iteration number =  67 Cross Entropy Loss= 229.82799233923868\n",
      "iteration number =  68 Cross Entropy Loss= 229.2771100205563\n",
      "iteration number =  69 Cross Entropy Loss= 228.72954745864317\n",
      "iteration number =  70 Cross Entropy Loss= 228.18527822159126\n",
      "iteration number =  71 Cross Entropy Loss= 227.6442760879627\n",
      "iteration number =  72 Cross Entropy Loss= 227.10651504629635\n",
      "iteration number =  73 Cross Entropy Loss= 226.57196929456552\n",
      "iteration number =  74 Cross Entropy Loss= 226.04061323958817\n",
      "iteration number =  75 Cross Entropy Loss= 225.51242149639185\n",
      "iteration number =  76 Cross Entropy Loss= 224.987368887535\n",
      "iteration number =  77 Cross Entropy Loss= 224.46543044238683\n",
      "iteration number =  78 Cross Entropy Loss= 223.94658139636653\n",
      "iteration number =  79 Cross Entropy Loss= 223.43079719014492\n",
      "iteration number =  80 Cross Entropy Loss= 222.9180534688092\n",
      "iteration number =  81 Cross Entropy Loss= 222.40832608099282\n",
      "iteration number =  82 Cross Entropy Loss= 221.90159107797183\n",
      "iteration number =  83 Cross Entropy Loss= 221.3978247127299\n",
      "iteration number =  84 Cross Entropy Loss= 220.89700343899273\n",
      "iteration number =  85 Cross Entropy Loss= 220.39910391023378\n",
      "iteration number =  86 Cross Entropy Loss= 219.90410297865287\n",
      "iteration number =  87 Cross Entropy Loss= 219.41197769412867\n",
      "iteration number =  88 Cross Entropy Loss= 218.92270530314678\n",
      "iteration number =  89 Cross Entropy Loss= 218.43626324770463\n",
      "iteration number =  90 Cross Entropy Loss= 217.95262916419435\n",
      "iteration number =  91 Cross Entropy Loss= 217.47178088226525\n",
      "iteration number =  92 Cross Entropy Loss= 216.99369642366645\n",
      "iteration number =  93 Cross Entropy Loss= 216.51835400107186\n",
      "iteration number =  94 Cross Entropy Loss= 216.04573201688746\n",
      "iteration number =  95 Cross Entropy Loss= 215.57580906204305\n",
      "iteration number =  96 Cross Entropy Loss= 215.10856391476887\n",
      "iteration number =  97 Cross Entropy Loss= 214.64397553935828\n",
      "iteration number =  98 Cross Entropy Loss= 214.18202308491814\n",
      "iteration number =  99 Cross Entropy Loss= 213.7226858841064\n",
      "iteration number =  100 Cross Entropy Loss= 213.26594345185958\n",
      "iteration number =  101 Cross Entropy Loss= 212.81177548410997\n",
      "iteration number =  102 Cross Entropy Loss= 212.3601618564934\n",
      "iteration number =  103 Cross Entropy Loss= 211.91108262304914\n",
      "iteration number =  104 Cross Entropy Loss= 211.46451801491233\n",
      "iteration number =  105 Cross Entropy Loss= 211.02044843899938\n",
      "iteration number =  106 Cross Entropy Loss= 210.5788544766878\n",
      "iteration number =  107 Cross Entropy Loss= 210.13971688249092\n",
      "iteration number =  108 Cross Entropy Loss= 209.70301658272768\n",
      "iteration number =  109 Cross Entropy Loss= 209.26873467418935\n",
      "iteration number =  110 Cross Entropy Loss= 208.83685242280256\n",
      "iteration number =  111 Cross Entropy Loss= 208.40735126229018\n",
      "iteration number =  112 Cross Entropy Loss= 207.98021279283023\n",
      "iteration number =  113 Cross Entropy Loss= 207.55541877971348\n",
      "iteration number =  114 Cross Entropy Loss= 207.13295115200037\n",
      "iteration number =  115 Cross Entropy Loss= 206.71279200117772\n",
      "iteration number =  116 Cross Entropy Loss= 206.29492357981601\n",
      "iteration number =  117 Cross Entropy Loss= 205.87932830022694\n",
      "iteration number =  118 Cross Entropy Loss= 205.4659887331229\n",
      "iteration number =  119 Cross Entropy Loss= 205.054887606278\n",
      "iteration number =  120 Cross Entropy Loss= 204.64600780319088\n",
      "iteration number =  121 Cross Entropy Loss= 204.23933236175097\n",
      "iteration number =  122 Cross Entropy Loss= 203.83484447290698\n",
      "iteration number =  123 Cross Entropy Loss= 203.43252747933926\n",
      "iteration number =  124 Cross Entropy Loss= 203.03236487413562\n",
      "iteration number =  125 Cross Entropy Loss= 202.6343402994715\n",
      "iteration number =  126 Cross Entropy Loss= 202.23843754529446\n",
      "iteration number =  127 Cross Entropy Loss= 201.84464054801333\n",
      "iteration number =  128 Cross Entropy Loss= 201.45293338919248\n",
      "iteration number =  129 Cross Entropy Loss= 201.06330029425163\n",
      "iteration number =  130 Cross Entropy Loss= 200.67572563117082\n",
      "iteration number =  131 Cross Entropy Loss= 200.29019390920143\n",
      "iteration number =  132 Cross Entropy Loss= 199.90668977758372\n",
      "iteration number =  133 Cross Entropy Loss= 199.52519802426986\n",
      "iteration number =  134 Cross Entropy Loss= 199.14570357465414\n",
      "iteration number =  135 Cross Entropy Loss= 198.76819149030987\n",
      "iteration number =  136 Cross Entropy Loss= 198.39264696773287\n",
      "iteration number =  137 Cross Entropy Loss= 198.01905533709242\n",
      "iteration number =  138 Cross Entropy Loss= 197.6474020609894\n",
      "iteration number =  139 Cross Entropy Loss= 197.27767273322178\n",
      "iteration number =  140 Cross Entropy Loss= 196.90985307755795\n",
      "iteration number =  141 Cross Entropy Loss= 196.54392894651764\n",
      "iteration number =  142 Cross Entropy Loss= 196.17988632016056\n",
      "iteration number =  143 Cross Entropy Loss= 195.81771130488346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  144 Cross Entropy Loss= 195.45739013222482\n",
      "iteration number =  145 Cross Entropy Loss= 195.0989091576784\n",
      "iteration number =  146 Cross Entropy Loss= 194.74225485951422\n",
      "iteration number =  147 Cross Entropy Loss= 194.38741383760882\n",
      "iteration number =  148 Cross Entropy Loss= 194.0343728122835\n",
      "iteration number =  149 Cross Entropy Loss= 193.68311862315124\n",
      "iteration number =  150 Cross Entropy Loss= 193.33363822797236\n",
      "iteration number =  151 Cross Entropy Loss= 192.9859187015187\n",
      "iteration number =  152 Cross Entropy Loss= 192.6399472344467\n",
      "iteration number =  153 Cross Entropy Loss= 192.2957111321793\n",
      "iteration number =  154 Cross Entropy Loss= 191.9531978137964\n",
      "iteration number =  155 Cross Entropy Loss= 191.61239481093463\n",
      "iteration number =  156 Cross Entropy Loss= 191.27328976669577\n",
      "iteration number =  157 Cross Entropy Loss= 190.93587043456398\n",
      "iteration number =  158 Cross Entropy Loss= 190.60012467733247\n",
      "iteration number =  159 Cross Entropy Loss= 190.2660404660386\n",
      "iteration number =  160 Cross Entropy Loss= 189.93360587890828\n",
      "iteration number =  161 Cross Entropy Loss= 189.6028091003095\n",
      "iteration number =  162 Cross Entropy Loss= 189.27363841971447\n",
      "iteration number =  163 Cross Entropy Loss= 188.94608223067107\n",
      "iteration number =  164 Cross Entropy Loss= 188.6201290297832\n",
      "iteration number =  165 Cross Entropy Loss= 188.29576741570023\n",
      "iteration number =  166 Cross Entropy Loss= 187.97298608811514\n",
      "iteration number =  167 Cross Entropy Loss= 187.65177384677222\n",
      "iteration number =  168 Cross Entropy Loss= 187.33211959048293\n",
      "iteration number =  169 Cross Entropy Loss= 187.01401231615142\n",
      "iteration number =  170 Cross Entropy Loss= 186.6974411178086\n",
      "iteration number =  171 Cross Entropy Loss= 186.3823951856551\n",
      "iteration number =  172 Cross Entropy Loss= 186.06886380511318\n",
      "iteration number =  173 Cross Entropy Loss= 185.75683635588757\n",
      "iteration number =  174 Cross Entropy Loss= 185.44630231103474\n",
      "iteration number =  175 Cross Entropy Loss= 185.1372512360415\n",
      "iteration number =  176 Cross Entropy Loss= 184.82967278791168\n",
      "iteration number =  177 Cross Entropy Loss= 184.52355671426216\n",
      "iteration number =  178 Cross Entropy Loss= 184.2188928524269\n",
      "iteration number =  179 Cross Entropy Loss= 183.91567112857012\n",
      "iteration number =  180 Cross Entropy Loss= 183.61388155680777\n",
      "iteration number =  181 Cross Entropy Loss= 183.31351423833735\n",
      "iteration number =  182 Cross Entropy Loss= 183.0145593605767\n",
      "iteration number =  183 Cross Entropy Loss= 182.71700719631076\n",
      "iteration number =  184 Cross Entropy Loss= 182.42084810284683\n",
      "iteration number =  185 Cross Entropy Loss= 182.12607252117823\n",
      "iteration number =  186 Cross Entropy Loss= 181.83267097515608\n",
      "iteration number =  187 Cross Entropy Loss= 181.54063407066948\n",
      "iteration number =  188 Cross Entropy Loss= 181.24995249483374\n",
      "iteration number =  189 Cross Entropy Loss= 180.96061701518653\n",
      "iteration number =  190 Cross Entropy Loss= 180.6726184788925\n",
      "iteration number =  191 Cross Entropy Loss= 180.38594781195556\n",
      "iteration number =  192 Cross Entropy Loss= 180.10059601843915\n",
      "iteration number =  193 Cross Entropy Loss= 179.8165541796944\n",
      "iteration number =  194 Cross Entropy Loss= 179.5338134535962\n",
      "iteration number =  195 Cross Entropy Loss= 179.25236507378685\n",
      "iteration number =  196 Cross Entropy Loss= 178.9722003489276\n",
      "iteration number =  197 Cross Entropy Loss= 178.69331066195764\n",
      "iteration number =  198 Cross Entropy Loss= 178.4156874693608\n",
      "iteration number =  199 Cross Entropy Loss= 178.13932230043963\n",
      "iteration number =  200 Cross Entropy Loss= 177.86420675659707\n",
      "iteration number =  201 Cross Entropy Loss= 177.5903325106255\n",
      "iteration number =  202 Cross Entropy Loss= 177.31769130600296\n",
      "iteration number =  203 Cross Entropy Loss= 177.04627495619673\n",
      "iteration number =  204 Cross Entropy Loss= 176.77607534397413\n",
      "iteration number =  205 Cross Entropy Loss= 176.50708442072062\n",
      "iteration number =  206 Cross Entropy Loss= 176.2392942057645\n",
      "iteration number =  207 Cross Entropy Loss= 175.9726967857091\n",
      "iteration number =  208 Cross Entropy Loss= 175.70728431377185\n",
      "iteration number =  209 Cross Entropy Loss= 175.44304900912982\n",
      "iteration number =  210 Cross Entropy Loss= 175.17998315627273\n",
      "iteration number =  211 Cross Entropy Loss= 174.91807910436208\n",
      "iteration number =  212 Cross Entropy Loss= 174.65732926659757\n",
      "iteration number =  213 Cross Entropy Loss= 174.39772611958972\n",
      "iteration number =  214 Cross Entropy Loss= 174.13926220273925\n",
      "iteration number =  215 Cross Entropy Loss= 173.88193011762291\n",
      "iteration number =  216 Cross Entropy Loss= 173.62572252738573\n",
      "iteration number =  217 Cross Entropy Loss= 173.37063215614\n",
      "iteration number =  218 Cross Entropy Loss= 173.11665178836992\n",
      "iteration number =  219 Cross Entropy Loss= 172.86377426834315\n",
      "iteration number =  220 Cross Entropy Loss= 172.61199249952818\n",
      "iteration number =  221 Cross Entropy Loss= 172.361299444018\n",
      "iteration number =  222 Cross Entropy Loss= 172.1116881219598\n",
      "iteration number =  223 Cross Entropy Loss= 171.86315161099068\n",
      "iteration number =  224 Cross Entropy Loss= 171.61568304567948\n",
      "iteration number =  225 Cross Entropy Loss= 171.36927561697416\n",
      "iteration number =  226 Cross Entropy Loss= 171.12392257165544\n",
      "iteration number =  227 Cross Entropy Loss= 170.8796172117959\n",
      "iteration number =  228 Cross Entropy Loss= 170.6363528942249\n",
      "iteration number =  229 Cross Entropy Loss= 170.3941230299993\n",
      "iteration number =  230 Cross Entropy Loss= 170.15292108387962\n",
      "iteration number =  231 Cross Entropy Loss= 169.9127405738118\n",
      "iteration number =  232 Cross Entropy Loss= 169.6735750704143\n",
      "iteration number =  233 Cross Entropy Loss= 169.43541819647123\n",
      "iteration number =  234 Cross Entropy Loss= 169.19826362642988\n",
      "iteration number =  235 Cross Entropy Loss= 168.96210508590445\n",
      "iteration number =  236 Cross Entropy Loss= 168.72693635118458\n",
      "iteration number =  237 Cross Entropy Loss= 168.4927512487492\n",
      "iteration number =  238 Cross Entropy Loss= 168.25954365478566\n",
      "iteration number =  239 Cross Entropy Loss= 168.02730749471377\n",
      "iteration number =  240 Cross Entropy Loss= 167.79603674271488\n",
      "iteration number =  241 Cross Entropy Loss= 167.5657254212662\n",
      "iteration number =  242 Cross Entropy Loss= 167.3363676006798\n",
      "iteration number =  243 Cross Entropy Loss= 167.10795739864665\n",
      "iteration number =  244 Cross Entropy Loss= 166.88048897978535\n",
      "iteration number =  245 Cross Entropy Loss= 166.65395655519583\n",
      "iteration number =  246 Cross Entropy Loss= 166.42835438201757\n",
      "iteration number =  247 Cross Entropy Loss= 166.2036767629928\n",
      "iteration number =  248 Cross Entropy Loss= 165.97991804603396\n",
      "iteration number =  249 Cross Entropy Loss= 165.75707262379595\n",
      "iteration number =  250 Cross Entropy Loss= 165.5351349332529\n",
      "iteration number =  251 Cross Entropy Loss= 165.31409945527946\n",
      "iteration number =  252 Cross Entropy Loss= 165.09396071423646\n",
      "iteration number =  253 Cross Entropy Loss= 164.87471327756072\n",
      "iteration number =  254 Cross Entropy Loss= 164.65635175535982\n",
      "iteration number =  255 Cross Entropy Loss= 164.43887080001042\n",
      "iteration number =  256 Cross Entropy Loss= 164.22226510576135\n",
      "iteration number =  257 Cross Entropy Loss= 164.00652940834067\n",
      "iteration number =  258 Cross Entropy Loss= 163.79165848456688\n",
      "iteration number =  259 Cross Entropy Loss= 163.57764715196427\n",
      "iteration number =  260 Cross Entropy Loss= 163.36449026838244\n",
      "iteration number =  261 Cross Entropy Loss= 163.15218273161958\n",
      "iteration number =  262 Cross Entropy Loss= 162.9407194790499\n",
      "iteration number =  263 Cross Entropy Loss= 162.73009548725506\n",
      "iteration number =  264 Cross Entropy Loss= 162.52030577165903\n",
      "iteration number =  265 Cross Entropy Loss= 162.31134538616757\n",
      "iteration number =  266 Cross Entropy Loss= 162.10320942281072\n",
      "iteration number =  267 Cross Entropy Loss= 161.89589301138943\n",
      "iteration number =  268 Cross Entropy Loss= 161.68939131912597\n",
      "iteration number =  269 Cross Entropy Loss= 161.48369955031785\n",
      "iteration number =  270 Cross Entropy Loss= 161.27881294599524\n",
      "iteration number =  271 Cross Entropy Loss= 161.07472678358238\n",
      "iteration number =  272 Cross Entropy Loss= 160.8714363765622\n",
      "iteration number =  273 Cross Entropy Loss= 160.66893707414442\n",
      "iteration number =  274 Cross Entropy Loss= 160.46722426093754\n",
      "iteration number =  275 Cross Entropy Loss= 160.26629335662375\n",
      "iteration number =  276 Cross Entropy Loss= 160.06613981563748\n",
      "iteration number =  277 Cross Entropy Loss= 159.86675912684737\n",
      "iteration number =  278 Cross Entropy Loss= 159.66814681324138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  279 Cross Entropy Loss= 159.47029843161528\n",
      "iteration number =  280 Cross Entropy Loss= 159.27320957226456\n",
      "iteration number =  281 Cross Entropy Loss= 159.07687585867893\n",
      "iteration number =  282 Cross Entropy Loss= 158.88129294724095\n",
      "iteration number =  283 Cross Entropy Loss= 158.6864565269268\n",
      "iteration number =  284 Cross Entropy Loss= 158.49236231901097\n",
      "iteration number =  285 Cross Entropy Loss= 158.29900607677342\n",
      "iteration number =  286 Cross Entropy Loss= 158.1063835852101\n",
      "iteration number =  287 Cross Entropy Loss= 157.91449066074648\n",
      "iteration number =  288 Cross Entropy Loss= 157.7233231509538\n",
      "iteration number =  289 Cross Entropy Loss= 157.53287693426847\n",
      "iteration number =  290 Cross Entropy Loss= 157.34314791971428\n",
      "iteration number =  291 Cross Entropy Loss= 157.15413204662752\n",
      "iteration number =  292 Cross Entropy Loss= 156.96582528438483\n",
      "iteration number =  293 Cross Entropy Loss= 156.77822363213403\n",
      "iteration number =  294 Cross Entropy Loss= 156.5913231185275\n",
      "iteration number =  295 Cross Entropy Loss= 156.40511980145862\n",
      "iteration number =  296 Cross Entropy Loss= 156.21960976780048\n",
      "iteration number =  297 Cross Entropy Loss= 156.03478913314757\n",
      "iteration number =  298 Cross Entropy Loss= 155.85065404156018\n",
      "iteration number =  299 Cross Entropy Loss= 155.66720066531116\n",
      "iteration number =  300 Cross Entropy Loss= 155.48442520463541\n",
      "iteration number =  301 Cross Entropy Loss= 155.30232388748206\n",
      "iteration number =  302 Cross Entropy Loss= 155.12089296926877\n",
      "iteration number =  303 Cross Entropy Loss= 154.94012873263912\n",
      "iteration number =  304 Cross Entropy Loss= 154.76002748722195\n",
      "iteration number =  305 Cross Entropy Loss= 154.58058556939346\n",
      "iteration number =  306 Cross Entropy Loss= 154.40179934204156\n",
      "iteration number =  307 Cross Entropy Loss= 154.22366519433268\n",
      "iteration number =  308 Cross Entropy Loss= 154.0461795414811\n",
      "iteration number =  309 Cross Entropy Loss= 153.8693388245203\n",
      "iteration number =  310 Cross Entropy Loss= 153.69313951007683\n",
      "iteration number =  311 Cross Entropy Loss= 153.51757809014651\n",
      "iteration number =  312 Cross Entropy Loss= 153.34265108187273\n",
      "iteration number =  313 Cross Entropy Loss= 153.16835502732715\n",
      "iteration number =  314 Cross Entropy Loss= 152.99468649329245\n",
      "iteration number =  315 Cross Entropy Loss= 152.82164207104745\n",
      "iteration number =  316 Cross Entropy Loss= 152.64921837615412\n",
      "iteration number =  317 Cross Entropy Loss= 152.47741204824726\n",
      "iteration number =  318 Cross Entropy Loss= 152.3062197508255\n",
      "iteration number =  319 Cross Entropy Loss= 152.1356381710452\n",
      "iteration number =  320 Cross Entropy Loss= 151.96566401951583\n",
      "iteration number =  321 Cross Entropy Loss= 151.79629403009773\n",
      "iteration number =  322 Cross Entropy Loss= 151.62752495970176\n",
      "iteration number =  323 Cross Entropy Loss= 151.4593535880909\n",
      "iteration number =  324 Cross Entropy Loss= 151.2917767176841\n",
      "iteration number =  325 Cross Entropy Loss= 151.12479117336164\n",
      "iteration number =  326 Cross Entropy Loss= 150.9583938022729\n",
      "iteration number =  327 Cross Entropy Loss= 150.7925814736456\n",
      "iteration number =  328 Cross Entropy Loss= 150.62735107859743\n",
      "iteration number =  329 Cross Entropy Loss= 150.46269952994896\n",
      "iteration number =  330 Cross Entropy Loss= 150.29862376203894\n",
      "iteration number =  331 Cross Entropy Loss= 150.13512073054116\n",
      "iteration number =  332 Cross Entropy Loss= 149.97218741228306\n",
      "iteration number =  333 Cross Entropy Loss= 149.80982080506644\n",
      "iteration number =  334 Cross Entropy Loss= 149.64801792748946\n",
      "iteration number =  335 Cross Entropy Loss= 149.48677581877095\n",
      "iteration number =  336 Cross Entropy Loss= 149.32609153857595\n",
      "iteration number =  337 Cross Entropy Loss= 149.16596216684337\n",
      "iteration number =  338 Cross Entropy Loss= 149.006384803615\n",
      "iteration number =  339 Cross Entropy Loss= 148.8473565688664\n",
      "iteration number =  340 Cross Entropy Loss= 148.68887460233947\n",
      "iteration number =  341 Cross Entropy Loss= 148.5309360633765\n",
      "iteration number =  342 Cross Entropy Loss= 148.373538130756\n",
      "iteration number =  343 Cross Entropy Loss= 148.21667800253002\n",
      "iteration number =  344 Cross Entropy Loss= 148.06035289586305\n",
      "iteration number =  345 Cross Entropy Loss= 147.90456004687275\n",
      "iteration number =  346 Cross Entropy Loss= 147.74929671047164\n",
      "iteration number =  347 Cross Entropy Loss= 147.59456016021124\n",
      "iteration number =  348 Cross Entropy Loss= 147.44034768812656\n",
      "iteration number =  349 Cross Entropy Loss= 147.28665660458324\n",
      "iteration number =  350 Cross Entropy Loss= 147.1334842381255\n",
      "iteration number =  351 Cross Entropy Loss= 146.98082793532552\n",
      "iteration number =  352 Cross Entropy Loss= 146.82868506063483\n",
      "iteration number =  353 Cross Entropy Loss= 146.6770529962364\n",
      "iteration number =  354 Cross Entropy Loss= 146.52592914189887\n",
      "iteration number =  355 Cross Entropy Loss= 146.3753109148316\n",
      "iteration number =  356 Cross Entropy Loss= 146.22519574954143\n",
      "iteration number =  357 Cross Entropy Loss= 146.07558109769082\n",
      "iteration number =  358 Cross Entropy Loss= 145.9264644279572\n",
      "iteration number =  359 Cross Entropy Loss= 145.7778432258937\n",
      "iteration number =  360 Cross Entropy Loss= 145.6297149937913\n",
      "iteration number =  361 Cross Entropy Loss= 145.48207725054237\n",
      "iteration number =  362 Cross Entropy Loss= 145.33492753150506\n",
      "iteration number =  363 Cross Entropy Loss= 145.18826338836973\n",
      "iteration number =  364 Cross Entropy Loss= 145.04208238902592\n",
      "iteration number =  365 Cross Entropy Loss= 144.89638211743107\n",
      "iteration number =  366 Cross Entropy Loss= 144.75116017348017\n",
      "iteration number =  367 Cross Entropy Loss= 144.60641417287695\n",
      "iteration number =  368 Cross Entropy Loss= 144.46214174700603\n",
      "iteration number =  369 Cross Entropy Loss= 144.31834054280648\n",
      "iteration number =  370 Cross Entropy Loss= 144.17500822264634\n",
      "iteration number =  371 Cross Entropy Loss= 144.0321424641985\n",
      "iteration number =  372 Cross Entropy Loss= 143.88974096031788\n",
      "iteration number =  373 Cross Entropy Loss= 143.74780141891938\n",
      "iteration number =  374 Cross Entropy Loss= 143.60632156285726\n",
      "iteration number =  375 Cross Entropy Loss= 143.4652991298057\n",
      "iteration number =  376 Cross Entropy Loss= 143.32473187214015\n",
      "iteration number =  377 Cross Entropy Loss= 143.18461755682029\n",
      "iteration number =  378 Cross Entropy Loss= 143.0449539652735\n",
      "iteration number =  379 Cross Entropy Loss= 142.90573889328\n",
      "iteration number =  380 Cross Entropy Loss= 142.76697015085864\n",
      "iteration number =  381 Cross Entropy Loss= 142.6286455621539\n",
      "iteration number =  382 Cross Entropy Loss= 142.49076296532397\n",
      "iteration number =  383 Cross Entropy Loss= 142.35332021242988\n",
      "iteration number =  384 Cross Entropy Loss= 142.21631516932558\n",
      "iteration number =  385 Cross Entropy Loss= 142.07974571554905\n",
      "iteration number =  386 Cross Entropy Loss= 141.94360974421454\n",
      "iteration number =  387 Cross Entropy Loss= 141.80790516190575\n",
      "iteration number =  388 Cross Entropy Loss= 141.6726298885697\n",
      "iteration number =  389 Cross Entropy Loss= 141.5377818574122\n",
      "iteration number =  390 Cross Entropy Loss= 141.4033590147937\n",
      "iteration number =  391 Cross Entropy Loss= 141.2693593201264\n",
      "iteration number =  392 Cross Entropy Loss= 141.13578074577225\n",
      "iteration number =  393 Cross Entropy Loss= 141.00262127694185\n",
      "iteration number =  394 Cross Entropy Loss= 140.86987891159424\n",
      "iteration number =  395 Cross Entropy Loss= 140.73755166033783\n",
      "iteration number =  396 Cross Entropy Loss= 140.60563754633188\n",
      "iteration number =  397 Cross Entropy Loss= 140.47413460518925\n",
      "iteration number =  398 Cross Entropy Loss= 140.3430408848796\n",
      "iteration number =  399 Cross Entropy Loss= 140.2123544456341\n",
      "iteration number =  400 Cross Entropy Loss= 140.08207335985034\n",
      "iteration number =  401 Cross Entropy Loss= 139.95219571199854\n",
      "iteration number =  402 Cross Entropy Loss= 139.82271959852844\n",
      "iteration number =  403 Cross Entropy Loss= 139.69364312777705\n",
      "iteration number =  404 Cross Entropy Loss= 139.56496441987736\n",
      "iteration number =  405 Cross Entropy Loss= 139.4366816066676\n",
      "iteration number =  406 Cross Entropy Loss= 139.30879283160164\n",
      "iteration number =  407 Cross Entropy Loss= 139.18129624965997\n",
      "iteration number =  408 Cross Entropy Loss= 139.05419002726165\n",
      "iteration number =  409 Cross Entropy Loss= 138.92747234217683\n",
      "iteration number =  410 Cross Entropy Loss= 138.80114138344032\n",
      "iteration number =  411 Cross Entropy Loss= 138.67519535126576\n",
      "iteration number =  412 Cross Entropy Loss= 138.54963245696052\n",
      "iteration number =  413 Cross Entropy Loss= 138.42445092284163\n",
      "iteration number =  414 Cross Entropy Loss= 138.29964898215218\n",
      "iteration number =  415 Cross Entropy Loss= 138.1752248789785\n",
      "iteration number =  416 Cross Entropy Loss= 138.05117686816823\n",
      "iteration number =  417 Cross Entropy Loss= 137.92750321524912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  418 Cross Entropy Loss= 137.80420219634829\n",
      "iteration number =  419 Cross Entropy Loss= 137.68127209811246\n",
      "iteration number =  420 Cross Entropy Loss= 137.55871121762897\n",
      "iteration number =  421 Cross Entropy Loss= 137.436517862347\n",
      "iteration number =  422 Cross Entropy Loss= 137.31469035000018\n",
      "iteration number =  423 Cross Entropy Loss= 137.1932270085294\n",
      "iteration number =  424 Cross Entropy Loss= 137.07212617600632\n",
      "iteration number =  425 Cross Entropy Loss= 136.95138620055798\n",
      "iteration number =  426 Cross Entropy Loss= 136.8310054402914\n",
      "iteration number =  427 Cross Entropy Loss= 136.71098226321945\n",
      "iteration number =  428 Cross Entropy Loss= 136.59131504718718\n",
      "iteration number =  429 Cross Entropy Loss= 136.4720021797985\n",
      "iteration number =  430 Cross Entropy Loss= 136.353042058344\n",
      "iteration number =  431 Cross Entropy Loss= 136.23443308972898\n",
      "iteration number =  432 Cross Entropy Loss= 136.11617369040243\n",
      "iteration number =  433 Cross Entropy Loss= 135.9982622862864\n",
      "iteration number =  434 Cross Entropy Loss= 135.88069731270608\n",
      "iteration number =  435 Cross Entropy Loss= 135.76347721432046\n",
      "iteration number =  436 Cross Entropy Loss= 135.64660044505368\n",
      "iteration number =  437 Cross Entropy Loss= 135.53006546802683\n",
      "iteration number =  438 Cross Entropy Loss= 135.4138707554905\n",
      "iteration number =  439 Cross Entropy Loss= 135.29801478875783\n",
      "iteration number =  440 Cross Entropy Loss= 135.18249605813799\n",
      "iteration number =  441 Cross Entropy Loss= 135.0673130628706\n",
      "iteration number =  442 Cross Entropy Loss= 134.95246431106045\n",
      "iteration number =  443 Cross Entropy Loss= 134.8379483196128\n",
      "iteration number =  444 Cross Entropy Loss= 134.7237636141692\n",
      "iteration number =  445 Cross Entropy Loss= 134.60990872904415\n",
      "iteration number =  446 Cross Entropy Loss= 134.49638220716196\n",
      "iteration number =  447 Cross Entropy Loss= 134.3831825999942\n",
      "iteration number =  448 Cross Entropy Loss= 134.27030846749798\n",
      "iteration number =  449 Cross Entropy Loss= 134.1577583780544\n",
      "iteration number =  450 Cross Entropy Loss= 134.04553090840776\n",
      "iteration number =  451 Cross Entropy Loss= 133.93362464360507\n",
      "iteration number =  452 Cross Entropy Loss= 133.82203817693647\n",
      "iteration number =  453 Cross Entropy Loss= 133.71077010987557\n",
      "iteration number =  454 Cross Entropy Loss= 133.59981905202088\n",
      "iteration number =  455 Cross Entropy Loss= 133.48918362103745\n",
      "iteration number =  456 Cross Entropy Loss= 133.3788624425989\n",
      "iteration number =  457 Cross Entropy Loss= 133.26885415033016\n",
      "iteration number =  458 Cross Entropy Loss= 133.1591573857507\n",
      "iteration number =  459 Cross Entropy Loss= 133.04977079821805\n",
      "iteration number =  460 Cross Entropy Loss= 132.94069304487198\n",
      "iteration number =  461 Cross Entropy Loss= 132.83192279057894\n",
      "iteration number =  462 Cross Entropy Loss= 132.72345870787723\n",
      "iteration number =  463 Cross Entropy Loss= 132.61529947692244\n",
      "iteration number =  464 Cross Entropy Loss= 132.50744378543357\n",
      "iteration number =  465 Cross Entropy Loss= 132.39989032863906\n",
      "iteration number =  466 Cross Entropy Loss= 132.29263780922412\n",
      "iteration number =  467 Cross Entropy Loss= 132.1856849372777\n",
      "iteration number =  468 Cross Entropy Loss= 132.0790304302404\n",
      "iteration number =  469 Cross Entropy Loss= 131.97267301285265\n",
      "iteration number =  470 Cross Entropy Loss= 131.86661141710317\n",
      "iteration number =  471 Cross Entropy Loss= 131.76084438217828\n",
      "iteration number =  472 Cross Entropy Loss= 131.6553706544111\n",
      "iteration number =  473 Cross Entropy Loss= 131.55018898723165\n",
      "iteration number =  474 Cross Entropy Loss= 131.44529814111704\n",
      "iteration number =  475 Cross Entropy Loss= 131.3406968835422\n",
      "iteration number =  476 Cross Entropy Loss= 131.236383988931\n",
      "iteration number =  477 Cross Entropy Loss= 131.13235823860788\n",
      "iteration number =  478 Cross Entropy Loss= 131.02861842074964\n",
      "iteration number =  479 Cross Entropy Loss= 130.92516333033782\n",
      "iteration number =  480 Cross Entropy Loss= 130.8219917691114\n",
      "iteration number =  481 Cross Entropy Loss= 130.71910254552003\n",
      "iteration number =  482 Cross Entropy Loss= 130.61649447467727\n",
      "iteration number =  483 Cross Entropy Loss= 130.5141663783147\n",
      "iteration number =  484 Cross Entropy Loss= 130.4121170847362\n",
      "iteration number =  485 Cross Entropy Loss= 130.31034542877222\n",
      "iteration number =  486 Cross Entropy Loss= 130.20885025173519\n",
      "iteration number =  487 Cross Entropy Loss= 130.1076304013747\n",
      "iteration number =  488 Cross Entropy Loss= 130.00668473183322\n",
      "iteration number =  489 Cross Entropy Loss= 129.9060121036022\n",
      "iteration number =  490 Cross Entropy Loss= 129.8056113834786\n",
      "iteration number =  491 Cross Entropy Loss= 129.70548144452158\n",
      "iteration number =  492 Cross Entropy Loss= 129.6056211660097\n",
      "iteration number =  493 Cross Entropy Loss= 129.50602943339848\n",
      "iteration number =  494 Cross Entropy Loss= 129.40670513827806\n",
      "iteration number =  495 Cross Entropy Loss= 129.30764717833165\n",
      "iteration number =  496 Cross Entropy Loss= 129.2088544572938\n",
      "iteration number =  497 Cross Entropy Loss= 129.11032588490943\n",
      "iteration number =  498 Cross Entropy Loss= 129.01206037689292\n",
      "iteration number =  499 Cross Entropy Loss= 128.91405685488775\n",
      "iteration number =  500 Cross Entropy Loss= 128.81631424642615\n",
      "iteration number =  501 Cross Entropy Loss= 128.71883148488942\n",
      "iteration number =  502 Cross Entropy Loss= 128.62160750946833\n",
      "iteration number =  503 Cross Entropy Loss= 128.52464126512402\n",
      "iteration number =  504 Cross Entropy Loss= 128.42793170254902\n",
      "iteration number =  505 Cross Entropy Loss= 128.33147777812871\n",
      "iteration number =  506 Cross Entropy Loss= 128.23527845390305\n",
      "iteration number =  507 Cross Entropy Loss= 128.13933269752863\n",
      "iteration number =  508 Cross Entropy Loss= 128.043639482241\n",
      "iteration number =  509 Cross Entropy Loss= 127.94819778681725\n",
      "iteration number =  510 Cross Entropy Loss= 127.8530065955391\n",
      "iteration number =  511 Cross Entropy Loss= 127.75806489815596\n",
      "iteration number =  512 Cross Entropy Loss= 127.66337168984863\n",
      "iteration number =  513 Cross Entropy Loss= 127.5689259711929\n",
      "iteration number =  514 Cross Entropy Loss= 127.47472674812374\n",
      "iteration number =  515 Cross Entropy Loss= 127.38077303189984\n",
      "iteration number =  516 Cross Entropy Loss= 127.28706383906803\n",
      "iteration number =  517 Cross Entropy Loss= 127.19359819142831\n",
      "iteration number =  518 Cross Entropy Loss= 127.1003751159991\n",
      "iteration number =  519 Cross Entropy Loss= 127.00739364498268\n",
      "iteration number =  520 Cross Entropy Loss= 126.91465281573102\n",
      "iteration number =  521 Cross Entropy Loss= 126.82215167071166\n",
      "iteration number =  522 Cross Entropy Loss= 126.72988925747424\n",
      "iteration number =  523 Cross Entropy Loss= 126.63786462861678\n",
      "iteration number =  524 Cross Entropy Loss= 126.54607684175272\n",
      "iteration number =  525 Cross Entropy Loss= 126.45452495947788\n",
      "iteration number =  526 Cross Entropy Loss= 126.36320804933786\n",
      "iteration number =  527 Cross Entropy Loss= 126.27212518379554\n",
      "iteration number =  528 Cross Entropy Loss= 126.18127544019903\n",
      "iteration number =  529 Cross Entropy Loss= 126.09065790074969\n",
      "iteration number =  530 Cross Entropy Loss= 126.00027165247045\n",
      "iteration number =  531 Cross Entropy Loss= 125.91011578717449\n",
      "iteration number =  532 Cross Entropy Loss= 125.82018940143391\n",
      "iteration number =  533 Cross Entropy Loss= 125.73049159654896\n",
      "iteration number =  534 Cross Entropy Loss= 125.64102147851726\n",
      "iteration number =  535 Cross Entropy Loss= 125.55177815800332\n",
      "iteration number =  536 Cross Entropy Loss= 125.46276075030836\n",
      "iteration number =  537 Cross Entropy Loss= 125.37396837534031\n",
      "iteration number =  538 Cross Entropy Loss= 125.28540015758406\n",
      "iteration number =  539 Cross Entropy Loss= 125.1970552260719\n",
      "iteration number =  540 Cross Entropy Loss= 125.10893271435418\n",
      "iteration number =  541 Cross Entropy Loss= 125.02103176047041\n",
      "iteration number =  542 Cross Entropy Loss= 124.93335150692009\n",
      "iteration number =  543 Cross Entropy Loss= 124.84589110063436\n",
      "iteration number =  544 Cross Entropy Loss= 124.75864969294744\n",
      "iteration number =  545 Cross Entropy Loss= 124.67162643956843\n",
      "iteration number =  546 Cross Entropy Loss= 124.58482050055332\n",
      "iteration number =  547 Cross Entropy Loss= 124.49823104027732\n",
      "iteration number =  548 Cross Entropy Loss= 124.4118572274071\n",
      "iteration number =  549 Cross Entropy Loss= 124.32569823487358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  550 Cross Entropy Loss= 124.2397532398448\n",
      "iteration number =  551 Cross Entropy Loss= 124.15402142369882\n",
      "iteration number =  552 Cross Entropy Loss= 124.06850197199711\n",
      "iteration number =  553 Cross Entropy Loss= 123.983194074458\n",
      "iteration number =  554 Cross Entropy Loss= 123.89809692493026\n",
      "iteration number =  555 Cross Entropy Loss= 123.81320972136703\n",
      "iteration number =  556 Cross Entropy Loss= 123.72853166579989\n",
      "iteration number =  557 Cross Entropy Loss= 123.6440619643131\n",
      "iteration number =  558 Cross Entropy Loss= 123.55979982701794\n",
      "iteration number =  559 Cross Entropy Loss= 123.4757444680275\n",
      "iteration number =  560 Cross Entropy Loss= 123.39189510543142\n",
      "iteration number =  561 Cross Entropy Loss= 123.30825096127089\n",
      "iteration number =  562 Cross Entropy Loss= 123.22481126151388\n",
      "iteration number =  563 Cross Entropy Loss= 123.14157523603058\n",
      "iteration number =  564 Cross Entropy Loss= 123.05854211856888\n",
      "iteration number =  565 Cross Entropy Loss= 122.97571114673013\n",
      "iteration number =  566 Cross Entropy Loss= 122.89308156194512\n",
      "iteration number =  567 Cross Entropy Loss= 122.81065260945016\n",
      "iteration number =  568 Cross Entropy Loss= 122.72842353826334\n",
      "iteration number =  569 Cross Entropy Loss= 122.64639360116111\n",
      "iteration number =  570 Cross Entropy Loss= 122.56456205465471\n",
      "iteration number =  571 Cross Entropy Loss= 122.48292815896718\n",
      "iteration number =  572 Cross Entropy Loss= 122.40149117801023\n",
      "iteration number =  573 Cross Entropy Loss= 122.32025037936144\n",
      "iteration number =  574 Cross Entropy Loss= 122.23920503424156\n",
      "iteration number =  575 Cross Entropy Loss= 122.15835441749195\n",
      "iteration number =  576 Cross Entropy Loss= 122.07769780755228\n",
      "iteration number =  577 Cross Entropy Loss= 121.99723448643843\n",
      "iteration number =  578 Cross Entropy Loss= 121.91696373972025\n",
      "iteration number =  579 Cross Entropy Loss= 121.8368848564999\n",
      "iteration number =  580 Cross Entropy Loss= 121.7569971293901\n",
      "iteration number =  581 Cross Entropy Loss= 121.67729985449249\n",
      "iteration number =  582 Cross Entropy Loss= 121.59779233137634\n",
      "iteration number =  583 Cross Entropy Loss= 121.51847386305732\n",
      "iteration number =  584 Cross Entropy Loss= 121.43934375597641\n",
      "iteration number =  585 Cross Entropy Loss= 121.36040131997892\n",
      "iteration number =  586 Cross Entropy Loss= 121.28164586829386\n",
      "iteration number =  587 Cross Entropy Loss= 121.20307671751316\n",
      "iteration number =  588 Cross Entropy Loss= 121.1246931875713\n",
      "iteration number =  589 Cross Entropy Loss= 121.04649460172496\n",
      "iteration number =  590 Cross Entropy Loss= 120.96848028653284\n",
      "iteration number =  591 Cross Entropy Loss= 120.89064957183571\n",
      "iteration number =  592 Cross Entropy Loss= 120.81300179073634\n",
      "iteration number =  593 Cross Entropy Loss= 120.73553627958\n",
      "iteration number =  594 Cross Entropy Loss= 120.65825237793466\n",
      "iteration number =  595 Cross Entropy Loss= 120.58114942857162\n",
      "iteration number =  596 Cross Entropy Loss= 120.50422677744629\n",
      "iteration number =  597 Cross Entropy Loss= 120.42748377367872\n",
      "iteration number =  598 Cross Entropy Loss= 120.35091976953491\n",
      "iteration number =  599 Cross Entropy Loss= 120.2745341204077\n",
      "iteration number =  600 Cross Entropy Loss= 120.19832618479793\n",
      "iteration number =  601 Cross Entropy Loss= 120.12229532429613\n",
      "iteration number =  602 Cross Entropy Loss= 120.04644090356362\n",
      "iteration number =  603 Cross Entropy Loss= 119.97076229031441\n",
      "iteration number =  604 Cross Entropy Loss= 119.89525885529687\n",
      "iteration number =  605 Cross Entropy Loss= 119.81992997227557\n",
      "iteration number =  606 Cross Entropy Loss= 119.7447750180134\n",
      "iteration number =  607 Cross Entropy Loss= 119.66979337225368\n",
      "iteration number =  608 Cross Entropy Loss= 119.59498441770238\n",
      "iteration number =  609 Cross Entropy Loss= 119.5203475400106\n",
      "iteration number =  610 Cross Entropy Loss= 119.44588212775702\n",
      "iteration number =  611 Cross Entropy Loss= 119.37158757243057\n",
      "iteration number =  612 Cross Entropy Loss= 119.29746326841325\n",
      "iteration number =  613 Cross Entropy Loss= 119.22350861296297\n",
      "iteration number =  614 Cross Entropy Loss= 119.14972300619655\n",
      "iteration number =  615 Cross Entropy Loss= 119.07610585107288\n",
      "iteration number =  616 Cross Entropy Loss= 119.00265655337614\n",
      "iteration number =  617 Cross Entropy Loss= 118.9293745216992\n",
      "iteration number =  618 Cross Entropy Loss= 118.85625916742707\n",
      "iteration number =  619 Cross Entropy Loss= 118.78330990472057\n",
      "iteration number =  620 Cross Entropy Loss= 118.71052615049993\n",
      "iteration number =  621 Cross Entropy Loss= 118.63790732442868\n",
      "iteration number =  622 Cross Entropy Loss= 118.56545284889764\n",
      "iteration number =  623 Cross Entropy Loss= 118.49316214900884\n",
      "iteration number =  624 Cross Entropy Loss= 118.42103465255983\n",
      "iteration number =  625 Cross Entropy Loss= 118.34906979002784\n",
      "iteration number =  626 Cross Entropy Loss= 118.27726699455425\n",
      "iteration number =  627 Cross Entropy Loss= 118.20562570192901\n",
      "iteration number =  628 Cross Entropy Loss= 118.13414535057524\n",
      "iteration number =  629 Cross Entropy Loss= 118.06282538153405\n",
      "iteration number =  630 Cross Entropy Loss= 117.99166523844917\n",
      "iteration number =  631 Cross Entropy Loss= 117.92066436755202\n",
      "iteration number =  632 Cross Entropy Loss= 117.84982221764663\n",
      "iteration number =  633 Cross Entropy Loss= 117.77913824009485\n",
      "iteration number =  634 Cross Entropy Loss= 117.70861188880149\n",
      "iteration number =  635 Cross Entropy Loss= 117.63824262019966\n",
      "iteration number =  636 Cross Entropy Loss= 117.56802989323633\n",
      "iteration number =  637 Cross Entropy Loss= 117.49797316935768\n",
      "iteration number =  638 Cross Entropy Loss= 117.42807191249476\n",
      "iteration number =  639 Cross Entropy Loss= 117.35832558904939\n",
      "iteration number =  640 Cross Entropy Loss= 117.28873366787971\n",
      "iteration number =  641 Cross Entropy Loss= 117.21929562028636\n",
      "iteration number =  642 Cross Entropy Loss= 117.15001091999828\n",
      "iteration number =  643 Cross Entropy Loss= 117.080879043159\n",
      "iteration number =  644 Cross Entropy Loss= 117.01189946831275\n",
      "iteration number =  645 Cross Entropy Loss= 116.94307167639077\n",
      "iteration number =  646 Cross Entropy Loss= 116.87439515069772\n",
      "iteration number =  647 Cross Entropy Loss= 116.80586937689816\n",
      "iteration number =  648 Cross Entropy Loss= 116.73749384300316\n",
      "iteration number =  649 Cross Entropy Loss= 116.6692680393569\n",
      "iteration number =  650 Cross Entropy Loss= 116.60119145862345\n",
      "iteration number =  651 Cross Entropy Loss= 116.53326359577375\n",
      "iteration number =  652 Cross Entropy Loss= 116.4654839480723\n",
      "iteration number =  653 Cross Entropy Loss= 116.39785201506443\n",
      "iteration number =  654 Cross Entropy Loss= 116.33036729856323\n",
      "iteration number =  655 Cross Entropy Loss= 116.2630293026369\n",
      "iteration number =  656 Cross Entropy Loss= 116.19583753359586\n",
      "iteration number =  657 Cross Entropy Loss= 116.12879149998034\n",
      "iteration number =  658 Cross Entropy Loss= 116.06189071254768\n",
      "iteration number =  659 Cross Entropy Loss= 115.99513468425988\n",
      "iteration number =  660 Cross Entropy Loss= 115.9285229302713\n",
      "iteration number =  661 Cross Entropy Loss= 115.8620549679163\n",
      "iteration number =  662 Cross Entropy Loss= 115.79573031669706\n",
      "iteration number =  663 Cross Entropy Loss= 115.7295484982715\n",
      "iteration number =  664 Cross Entropy Loss= 115.66350903644104\n",
      "iteration number =  665 Cross Entropy Loss= 115.59761145713895\n",
      "iteration number =  666 Cross Entropy Loss= 115.53185528841814\n",
      "iteration number =  667 Cross Entropy Loss= 115.46624006043953\n",
      "iteration number =  668 Cross Entropy Loss= 115.40076530546033\n",
      "iteration number =  669 Cross Entropy Loss= 115.33543055782229\n",
      "iteration number =  670 Cross Entropy Loss= 115.27023535394015\n",
      "iteration number =  671 Cross Entropy Loss= 115.20517923229025\n",
      "iteration number =  672 Cross Entropy Loss= 115.1402617333989\n",
      "iteration number =  673 Cross Entropy Loss= 115.07548239983129\n",
      "iteration number =  674 Cross Entropy Loss= 115.01084077617999\n",
      "iteration number =  675 Cross Entropy Loss= 114.94633640905386\n",
      "iteration number =  676 Cross Entropy Loss= 114.8819688470669\n",
      "iteration number =  677 Cross Entropy Loss= 114.81773764082727\n",
      "iteration number =  678 Cross Entropy Loss= 114.75364234292616\n",
      "iteration number =  679 Cross Entropy Loss= 114.68968250792702\n",
      "iteration number =  680 Cross Entropy Loss= 114.62585769235466\n",
      "iteration number =  681 Cross Entropy Loss= 114.56216745468451\n",
      "iteration number =  682 Cross Entropy Loss= 114.49861135533196\n",
      "iteration number =  683 Cross Entropy Loss= 114.43518895664161\n",
      "iteration number =  684 Cross Entropy Loss= 114.3718998228769\n",
      "iteration number =  685 Cross Entropy Loss= 114.30874352020945\n",
      "iteration number =  686 Cross Entropy Loss= 114.24571961670873\n",
      "iteration number =  687 Cross Entropy Loss= 114.18282768233173\n",
      "iteration number =  688 Cross Entropy Loss= 114.12006728891265\n",
      "iteration number =  689 Cross Entropy Loss= 114.05743801015267\n",
      "iteration number =  690 Cross Entropy Loss= 113.99493942160979\n",
      "iteration number =  691 Cross Entropy Loss= 113.93257110068883\n",
      "iteration number =  692 Cross Entropy Loss= 113.8703326266313\n",
      "iteration number =  693 Cross Entropy Loss= 113.80822358050551\n",
      "iteration number =  694 Cross Entropy Loss= 113.7462435451967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  695 Cross Entropy Loss= 113.68439210539721\n",
      "iteration number =  696 Cross Entropy Loss= 113.62266884759663\n",
      "iteration number =  697 Cross Entropy Loss= 113.56107336007223\n",
      "iteration number =  698 Cross Entropy Loss= 113.49960523287919\n",
      "iteration number =  699 Cross Entropy Loss= 113.43826405784122\n",
      "iteration number =  700 Cross Entropy Loss= 113.37704942854083\n",
      "iteration number =  701 Cross Entropy Loss= 113.31596094030996\n",
      "iteration number =  702 Cross Entropy Loss= 113.25499819022072\n",
      "iteration number =  703 Cross Entropy Loss= 113.19416077707582\n",
      "iteration number =  704 Cross Entropy Loss= 113.13344830139957\n",
      "iteration number =  705 Cross Entropy Loss= 113.07286036542834\n",
      "iteration number =  706 Cross Entropy Loss= 113.01239657310173\n",
      "iteration number =  707 Cross Entropy Loss= 112.95205653005331\n",
      "iteration number =  708 Cross Entropy Loss= 112.89183984360156\n",
      "iteration number =  709 Cross Entropy Loss= 112.83174612274101\n",
      "iteration number =  710 Cross Entropy Loss= 112.77177497813318\n",
      "iteration number =  711 Cross Entropy Loss= 112.71192602209788\n",
      "iteration number =  712 Cross Entropy Loss= 112.6521988686042\n",
      "iteration number =  713 Cross Entropy Loss= 112.59259313326191\n",
      "iteration number =  714 Cross Entropy Loss= 112.53310843331279\n",
      "iteration number =  715 Cross Entropy Loss= 112.47374438762175\n",
      "iteration number =  716 Cross Entropy Loss= 112.41450061666853\n",
      "iteration number =  717 Cross Entropy Loss= 112.35537674253905\n",
      "iteration number =  718 Cross Entropy Loss= 112.29637238891684\n",
      "iteration number =  719 Cross Entropy Loss= 112.23748718107475\n",
      "iteration number =  720 Cross Entropy Loss= 112.1787207458665\n",
      "iteration number =  721 Cross Entropy Loss= 112.12007271171842\n",
      "iteration number =  722 Cross Entropy Loss= 112.06154270862112\n",
      "iteration number =  723 Cross Entropy Loss= 112.00313036812128\n",
      "iteration number =  724 Cross Entropy Loss= 111.94483532331353\n",
      "iteration number =  725 Cross Entropy Loss= 111.8866572088323\n",
      "iteration number =  726 Cross Entropy Loss= 111.82859566084376\n",
      "iteration number =  727 Cross Entropy Loss= 111.77065031703782\n",
      "iteration number =  728 Cross Entropy Loss= 111.7128208166202\n",
      "iteration number =  729 Cross Entropy Loss= 111.6551068003044\n",
      "iteration number =  730 Cross Entropy Loss= 111.59750791030397\n",
      "iteration number =  731 Cross Entropy Loss= 111.54002379032464\n",
      "iteration number =  732 Cross Entropy Loss= 111.48265408555653\n",
      "iteration number =  733 Cross Entropy Loss= 111.42539844266645\n",
      "iteration number =  734 Cross Entropy Loss= 111.3682565097903\n",
      "iteration number =  735 Cross Entropy Loss= 111.31122793652531\n",
      "iteration number =  736 Cross Entropy Loss= 111.2543123739226\n",
      "iteration number =  737 Cross Entropy Loss= 111.19750947447955\n",
      "iteration number =  738 Cross Entropy Loss= 111.14081889213239\n",
      "iteration number =  739 Cross Entropy Loss= 111.0842402822487\n",
      "iteration number =  740 Cross Entropy Loss= 111.02777330162009\n",
      "iteration number =  741 Cross Entropy Loss= 110.97141760845483\n",
      "iteration number =  742 Cross Entropy Loss= 110.9151728623705\n",
      "iteration number =  743 Cross Entropy Loss= 110.85903872438679\n",
      "iteration number =  744 Cross Entropy Loss= 110.80301485691834\n",
      "iteration number =  745 Cross Entropy Loss= 110.74710092376746\n",
      "iteration number =  746 Cross Entropy Loss= 110.69129659011705\n",
      "iteration number =  747 Cross Entropy Loss= 110.6356015225236\n",
      "iteration number =  748 Cross Entropy Loss= 110.58001538891006\n",
      "iteration number =  749 Cross Entropy Loss= 110.52453785855883\n",
      "iteration number =  750 Cross Entropy Loss= 110.46916860210497\n",
      "iteration number =  751 Cross Entropy Loss= 110.41390729152911\n",
      "iteration number =  752 Cross Entropy Loss= 110.35875360015075\n",
      "iteration number =  753 Cross Entropy Loss= 110.30370720262123\n",
      "iteration number =  754 Cross Entropy Loss= 110.24876777491713\n",
      "iteration number =  755 Cross Entropy Loss= 110.19393499433355\n",
      "iteration number =  756 Cross Entropy Loss= 110.13920853947724\n",
      "iteration number =  757 Cross Entropy Loss= 110.08458809026003\n",
      "iteration number =  758 Cross Entropy Loss= 110.03007332789232\n",
      "iteration number =  759 Cross Entropy Loss= 109.97566393487632\n",
      "iteration number =  760 Cross Entropy Loss= 109.92135959499964\n",
      "iteration number =  761 Cross Entropy Loss= 109.86715999332871\n",
      "iteration number =  762 Cross Entropy Loss= 109.8130648162024\n",
      "iteration number =  763 Cross Entropy Loss= 109.75907375122554\n",
      "iteration number =  764 Cross Entropy Loss= 109.70518648726255\n",
      "iteration number =  765 Cross Entropy Loss= 109.65140271443116\n",
      "iteration number =  766 Cross Entropy Loss= 109.59772212409598\n",
      "iteration number =  767 Cross Entropy Loss= 109.54414440886238\n",
      "iteration number =  768 Cross Entropy Loss= 109.49066926257015\n",
      "iteration number =  769 Cross Entropy Loss= 109.43729638028734\n",
      "iteration number =  770 Cross Entropy Loss= 109.38402545830414\n",
      "iteration number =  771 Cross Entropy Loss= 109.33085619412672\n",
      "iteration number =  772 Cross Entropy Loss= 109.27778828647118\n",
      "iteration number =  773 Cross Entropy Loss= 109.22482143525747\n",
      "iteration number =  774 Cross Entropy Loss= 109.1719553416034\n",
      "iteration number =  775 Cross Entropy Loss= 109.1191897078187\n",
      "iteration number =  776 Cross Entropy Loss= 109.06652423739905\n",
      "iteration number =  777 Cross Entropy Loss= 109.01395863502012\n",
      "iteration number =  778 Cross Entropy Loss= 108.96149260653186\n",
      "iteration number =  779 Cross Entropy Loss= 108.9091258589525\n",
      "iteration number =  780 Cross Entropy Loss= 108.85685810046286\n",
      "iteration number =  781 Cross Entropy Loss= 108.80468904040055\n",
      "iteration number =  782 Cross Entropy Loss= 108.75261838925421\n",
      "iteration number =  783 Cross Entropy Loss= 108.70064585865791\n",
      "iteration number =  784 Cross Entropy Loss= 108.64877116138537\n",
      "iteration number =  785 Cross Entropy Loss= 108.59699401134444\n",
      "iteration number =  786 Cross Entropy Loss= 108.54531412357142\n",
      "iteration number =  787 Cross Entropy Loss= 108.49373121422556\n",
      "iteration number =  788 Cross Entropy Loss= 108.44224500058351\n",
      "iteration number =  789 Cross Entropy Loss= 108.39085520103382\n",
      "iteration number =  790 Cross Entropy Loss= 108.33956153507143\n",
      "iteration number =  791 Cross Entropy Loss= 108.28836372329233\n",
      "iteration number =  792 Cross Entropy Loss= 108.23726148738814\n",
      "iteration number =  793 Cross Entropy Loss= 108.18625455014066\n",
      "iteration number =  794 Cross Entropy Loss= 108.13534263541663\n",
      "iteration number =  795 Cross Entropy Loss= 108.08452546816235\n",
      "iteration number =  796 Cross Entropy Loss= 108.03380277439852\n",
      "iteration number =  797 Cross Entropy Loss= 107.98317428121481\n",
      "iteration number =  798 Cross Entropy Loss= 107.93263971676483\n",
      "iteration number =  799 Cross Entropy Loss= 107.88219881026085\n",
      "iteration number =  800 Cross Entropy Loss= 107.83185129196866\n",
      "iteration number =  801 Cross Entropy Loss= 107.78159689320245\n",
      "iteration number =  802 Cross Entropy Loss= 107.73143534631978\n",
      "iteration number =  803 Cross Entropy Loss= 107.6813663847164\n",
      "iteration number =  804 Cross Entropy Loss= 107.63138974282128\n",
      "iteration number =  805 Cross Entropy Loss= 107.58150515609162\n",
      "iteration number =  806 Cross Entropy Loss= 107.53171236100789\n",
      "iteration number =  807 Cross Entropy Loss= 107.4820110950688\n",
      "iteration number =  808 Cross Entropy Loss= 107.43240109678644\n",
      "iteration number =  809 Cross Entropy Loss= 107.38288210568139\n",
      "iteration number =  810 Cross Entropy Loss= 107.33345386227786\n",
      "iteration number =  811 Cross Entropy Loss= 107.28411610809884\n",
      "iteration number =  812 Cross Entropy Loss= 107.2348685856613\n",
      "iteration number =  813 Cross Entropy Loss= 107.1857110384714\n",
      "iteration number =  814 Cross Entropy Loss= 107.13664321101977\n",
      "iteration number =  815 Cross Entropy Loss= 107.08766484877677\n",
      "iteration number =  816 Cross Entropy Loss= 107.03877569818778\n",
      "iteration number =  817 Cross Entropy Loss= 106.98997550666854\n",
      "iteration number =  818 Cross Entropy Loss= 106.94126402260054\n",
      "iteration number =  819 Cross Entropy Loss= 106.89264099532633\n",
      "iteration number =  820 Cross Entropy Loss= 106.84410617514501\n",
      "iteration number =  821 Cross Entropy Loss= 106.79565931330765\n",
      "iteration number =  822 Cross Entropy Loss= 106.74730016201266\n",
      "iteration number =  823 Cross Entropy Loss= 106.69902847440143\n",
      "iteration number =  824 Cross Entropy Loss= 106.6508440045537\n",
      "iteration number =  825 Cross Entropy Loss= 106.60274650748323\n",
      "iteration number =  826 Cross Entropy Loss= 106.55473573913326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  827 Cross Entropy Loss= 106.50681145637216\n",
      "iteration number =  828 Cross Entropy Loss= 106.45897341698901\n",
      "iteration number =  829 Cross Entropy Loss= 106.41122137968927\n",
      "iteration number =  830 Cross Entropy Loss= 106.36355510409042\n",
      "iteration number =  831 Cross Entropy Loss= 106.31597435071768\n",
      "iteration number =  832 Cross Entropy Loss= 106.26847888099971\n",
      "iteration number =  833 Cross Entropy Loss= 106.22106845726428\n",
      "iteration number =  834 Cross Entropy Loss= 106.17374284273419\n",
      "iteration number =  835 Cross Entropy Loss= 106.12650180152288\n",
      "iteration number =  836 Cross Entropy Loss= 106.07934509863034\n",
      "iteration number =  837 Cross Entropy Loss= 106.03227249993895\n",
      "iteration number =  838 Cross Entropy Loss= 105.98528377220933\n",
      "iteration number =  839 Cross Entropy Loss= 105.93837868307611\n",
      "iteration number =  840 Cross Entropy Loss= 105.89155700104396\n",
      "iteration number =  841 Cross Entropy Loss= 105.84481849548352\n",
      "iteration number =  842 Cross Entropy Loss= 105.7981629366273\n",
      "iteration number =  843 Cross Entropy Loss= 105.7515900955656\n",
      "iteration number =  844 Cross Entropy Loss= 105.7050997442426\n",
      "iteration number =  845 Cross Entropy Loss= 105.65869165545237\n",
      "iteration number =  846 Cross Entropy Loss= 105.61236560283483\n",
      "iteration number =  847 Cross Entropy Loss= 105.56612136087185\n",
      "iteration number =  848 Cross Entropy Loss= 105.51995870488338\n",
      "iteration number =  849 Cross Entropy Loss= 105.47387741102352\n",
      "iteration number =  850 Cross Entropy Loss= 105.42787725627657\n",
      "iteration number =  851 Cross Entropy Loss= 105.38195801845333\n",
      "iteration number =  852 Cross Entropy Loss= 105.33611947618715\n",
      "iteration number =  853 Cross Entropy Loss= 105.29036140893018\n",
      "iteration number =  854 Cross Entropy Loss= 105.2446835969495\n",
      "iteration number =  855 Cross Entropy Loss= 105.19908582132345\n",
      "iteration number =  856 Cross Entropy Loss= 105.15356786393792\n",
      "iteration number =  857 Cross Entropy Loss= 105.10812950748239\n",
      "iteration number =  858 Cross Entropy Loss= 105.06277053544653\n",
      "iteration number =  859 Cross Entropy Loss= 105.01749073211624\n",
      "iteration number =  860 Cross Entropy Loss= 104.97228988257025\n",
      "iteration number =  861 Cross Entropy Loss= 104.92716777267623\n",
      "iteration number =  862 Cross Entropy Loss= 104.88212418908729\n",
      "iteration number =  863 Cross Entropy Loss= 104.83715891923843\n",
      "iteration number =  864 Cross Entropy Loss= 104.79227175134281\n",
      "iteration number =  865 Cross Entropy Loss= 104.74746247438824\n",
      "iteration number =  866 Cross Entropy Loss= 104.70273087813379\n",
      "iteration number =  867 Cross Entropy Loss= 104.65807675310593\n",
      "iteration number =  868 Cross Entropy Loss= 104.61349989059539\n",
      "iteration number =  869 Cross Entropy Loss= 104.56900008265342\n",
      "iteration number =  870 Cross Entropy Loss= 104.52457712208842\n",
      "iteration number =  871 Cross Entropy Loss= 104.48023080246244\n",
      "iteration number =  872 Cross Entropy Loss= 104.43596091808786\n",
      "iteration number =  873 Cross Entropy Loss= 104.39176726402384\n",
      "iteration number =  874 Cross Entropy Loss= 104.347649636073\n",
      "iteration number =  875 Cross Entropy Loss= 104.30360783077802\n",
      "iteration number =  876 Cross Entropy Loss= 104.25964164541834\n",
      "iteration number =  877 Cross Entropy Loss= 104.21575087800673\n",
      "iteration number =  878 Cross Entropy Loss= 104.17193532728602\n",
      "iteration number =  879 Cross Entropy Loss= 104.12819479272582\n",
      "iteration number =  880 Cross Entropy Loss= 104.0845290745192\n",
      "iteration number =  881 Cross Entropy Loss= 104.04093797357939\n",
      "iteration number =  882 Cross Entropy Loss= 103.99742129153664\n",
      "iteration number =  883 Cross Entropy Loss= 103.95397883073485\n",
      "iteration number =  884 Cross Entropy Loss= 103.9106103942285\n",
      "iteration number =  885 Cross Entropy Loss= 103.86731578577933\n",
      "iteration number =  886 Cross Entropy Loss= 103.8240948098532\n",
      "iteration number =  887 Cross Entropy Loss= 103.78094727161698\n",
      "iteration number =  888 Cross Entropy Loss= 103.73787297693531\n",
      "iteration number =  889 Cross Entropy Loss= 103.69487173236757\n",
      "iteration number =  890 Cross Entropy Loss= 103.65194334516471\n",
      "iteration number =  891 Cross Entropy Loss= 103.60908762326613\n",
      "iteration number =  892 Cross Entropy Loss= 103.5663043752967\n",
      "iteration number =  893 Cross Entropy Loss= 103.52359341056354\n",
      "iteration number =  894 Cross Entropy Loss= 103.48095453905317\n",
      "iteration number =  895 Cross Entropy Loss= 103.43838757142832\n",
      "iteration number =  896 Cross Entropy Loss= 103.39589231902497\n",
      "iteration number =  897 Cross Entropy Loss= 103.35346859384941\n",
      "iteration number =  898 Cross Entropy Loss= 103.31111620857516\n",
      "iteration number =  899 Cross Entropy Loss= 103.26883497654003\n",
      "iteration number =  900 Cross Entropy Loss= 103.22662471174331\n",
      "iteration number =  901 Cross Entropy Loss= 103.18448522884255\n",
      "iteration number =  902 Cross Entropy Loss= 103.14241634315096\n",
      "iteration number =  903 Cross Entropy Loss= 103.10041787063422\n",
      "iteration number =  904 Cross Entropy Loss= 103.05848962790787\n",
      "iteration number =  905 Cross Entropy Loss= 103.01663143223416\n",
      "iteration number =  906 Cross Entropy Loss= 102.97484310151947\n",
      "iteration number =  907 Cross Entropy Loss= 102.93312445431121\n",
      "iteration number =  908 Cross Entropy Loss= 102.89147530979514\n",
      "iteration number =  909 Cross Entropy Loss= 102.84989548779258\n",
      "iteration number =  910 Cross Entropy Loss= 102.8083848087575\n",
      "iteration number =  911 Cross Entropy Loss= 102.76694309377385\n",
      "iteration number =  912 Cross Entropy Loss= 102.72557016455266\n",
      "iteration number =  913 Cross Entropy Loss= 102.68426584342951\n",
      "iteration number =  914 Cross Entropy Loss= 102.64302995336152\n",
      "iteration number =  915 Cross Entropy Loss= 102.60186231792478\n",
      "iteration number =  916 Cross Entropy Loss= 102.56076276131166\n",
      "iteration number =  917 Cross Entropy Loss= 102.51973110832805\n",
      "iteration number =  918 Cross Entropy Loss= 102.47876718439068\n",
      "iteration number =  919 Cross Entropy Loss= 102.43787081552449\n",
      "iteration number =  920 Cross Entropy Loss= 102.39704182835993\n",
      "iteration number =  921 Cross Entropy Loss= 102.35628005013041\n",
      "iteration number =  922 Cross Entropy Loss= 102.31558530866951\n",
      "iteration number =  923 Cross Entropy Loss= 102.27495743240854\n",
      "iteration number =  924 Cross Entropy Loss= 102.23439625037389\n",
      "iteration number =  925 Cross Entropy Loss= 102.19390159218435\n",
      "iteration number =  926 Cross Entropy Loss= 102.15347328804866\n",
      "iteration number =  927 Cross Entropy Loss= 102.11311116876288\n",
      "iteration number =  928 Cross Entropy Loss= 102.07281506570793\n",
      "iteration number =  929 Cross Entropy Loss= 102.03258481084691\n",
      "iteration number =  930 Cross Entropy Loss= 101.9924202367227\n",
      "iteration number =  931 Cross Entropy Loss= 101.95232117645546\n",
      "iteration number =  932 Cross Entropy Loss= 101.91228746374003\n",
      "iteration number =  933 Cross Entropy Loss= 101.8723189328436\n",
      "iteration number =  934 Cross Entropy Loss= 101.8324154186031\n",
      "iteration number =  935 Cross Entropy Loss= 101.79257675642282\n",
      "iteration number =  936 Cross Entropy Loss= 101.752802782272\n",
      "iteration number =  937 Cross Entropy Loss= 101.71309333268232\n",
      "iteration number =  938 Cross Entropy Loss= 101.67344824474549\n",
      "iteration number =  939 Cross Entropy Loss= 101.63386735611093\n",
      "iteration number =  940 Cross Entropy Loss= 101.59435050498331\n",
      "iteration number =  941 Cross Entropy Loss= 101.55489753012019\n",
      "iteration number =  942 Cross Entropy Loss= 101.51550827082963\n",
      "iteration number =  943 Cross Entropy Loss= 101.47618256696786\n",
      "iteration number =  944 Cross Entropy Loss= 101.43692025893696\n",
      "iteration number =  945 Cross Entropy Loss= 101.39772118768246\n",
      "iteration number =  946 Cross Entropy Loss= 101.35858519469106\n",
      "iteration number =  947 Cross Entropy Loss= 101.31951212198835\n",
      "iteration number =  948 Cross Entropy Loss= 101.28050181213649\n",
      "iteration number =  949 Cross Entropy Loss= 101.2415541082319\n",
      "iteration number =  950 Cross Entropy Loss= 101.20266885390298\n",
      "iteration number =  951 Cross Entropy Loss= 101.16384589330796\n",
      "iteration number =  952 Cross Entropy Loss= 101.12508507113253\n",
      "iteration number =  953 Cross Entropy Loss= 101.08638623258764\n",
      "iteration number =  954 Cross Entropy Loss= 101.04774922340727\n",
      "iteration number =  955 Cross Entropy Loss= 101.00917388984625\n",
      "iteration number =  956 Cross Entropy Loss= 100.97066007867802\n",
      "iteration number =  957 Cross Entropy Loss= 100.93220763719243\n",
      "iteration number =  958 Cross Entropy Loss= 100.89381641319363\n",
      "iteration number =  959 Cross Entropy Loss= 100.85548625499773\n",
      "iteration number =  960 Cross Entropy Loss= 100.81721701143088\n",
      "iteration number =  961 Cross Entropy Loss= 100.77900853182692\n",
      "iteration number =  962 Cross Entropy Loss= 100.74086066602537\n",
      "iteration number =  963 Cross Entropy Loss= 100.70277326436921\n",
      "iteration number =  964 Cross Entropy Loss= 100.66474617770282\n",
      "iteration number =  965 Cross Entropy Loss= 100.6267792573699\n",
      "iteration number =  966 Cross Entropy Loss= 100.58887235521125\n",
      "iteration number =  967 Cross Entropy Loss= 100.55102532356283\n",
      "iteration number =  968 Cross Entropy Loss= 100.51323801525365\n",
      "iteration number =  969 Cross Entropy Loss= 100.47551028360363\n",
      "iteration number =  970 Cross Entropy Loss= 100.43784198242162\n",
      "iteration number =  971 Cross Entropy Loss= 100.40023296600336\n",
      "iteration number =  972 Cross Entropy Loss= 100.3626830891294\n",
      "iteration number =  973 Cross Entropy Loss= 100.32519220706308\n",
      "iteration number =  974 Cross Entropy Loss= 100.28776017554864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  975 Cross Entropy Loss= 100.250386850809\n",
      "iteration number =  976 Cross Entropy Loss= 100.21307208954394\n",
      "iteration number =  977 Cross Entropy Loss= 100.17581574892807\n",
      "iteration number =  978 Cross Entropy Loss= 100.13861768660882\n",
      "iteration number =  979 Cross Entropy Loss= 100.10147776070457\n",
      "iteration number =  980 Cross Entropy Loss= 100.06439582980256\n",
      "iteration number =  981 Cross Entropy Loss= 100.02737175295707\n",
      "iteration number =  982 Cross Entropy Loss= 99.99040538968741\n",
      "iteration number =  983 Cross Entropy Loss= 99.95349659997605\n",
      "iteration number =  984 Cross Entropy Loss= 99.9166452442667\n",
      "iteration number =  985 Cross Entropy Loss= 99.8798511834623\n",
      "iteration number =  986 Cross Entropy Loss= 99.84311427892331\n",
      "iteration number =  987 Cross Entropy Loss= 99.80643439246566\n",
      "iteration number =  988 Cross Entropy Loss= 99.76981138635898\n",
      "iteration number =  989 Cross Entropy Loss= 99.73324512332462\n",
      "iteration number =  990 Cross Entropy Loss= 99.69673546653397\n",
      "iteration number =  991 Cross Entropy Loss= 99.66028227960638\n",
      "iteration number =  992 Cross Entropy Loss= 99.62388542660753\n",
      "iteration number =  993 Cross Entropy Loss= 99.5875447720475\n",
      "iteration number =  994 Cross Entropy Loss= 99.5512601808789\n",
      "iteration number =  995 Cross Entropy Loss= 99.51503151849518\n",
      "iteration number =  996 Cross Entropy Loss= 99.47885865072872\n",
      "iteration number =  997 Cross Entropy Loss= 99.44274144384912\n",
      "iteration number =  998 Cross Entropy Loss= 99.40667976456132\n",
      "iteration number =  999 Cross Entropy Loss= 99.37067348000393\n",
      "iteration number =  1000 Cross Entropy Loss= 99.33472245774732\n",
      "iteration number =  1001 Cross Entropy Loss= 99.29882656579201\n",
      "iteration number =  1002 Cross Entropy Loss= 99.26298567256688\n",
      "iteration number =  1003 Cross Entropy Loss= 99.22719964692728\n",
      "iteration number =  1004 Cross Entropy Loss= 99.19146835815349\n",
      "iteration number =  1005 Cross Entropy Loss= 99.15579167594896\n",
      "iteration number =  1006 Cross Entropy Loss= 99.12016947043846\n",
      "iteration number =  1007 Cross Entropy Loss= 99.0846016121665\n",
      "iteration number =  1008 Cross Entropy Loss= 99.04908797209555\n",
      "iteration number =  1009 Cross Entropy Loss= 99.01362842160451\n",
      "iteration number =  1010 Cross Entropy Loss= 98.97822283248671\n",
      "iteration number =  1011 Cross Entropy Loss= 98.94287107694859\n",
      "iteration number =  1012 Cross Entropy Loss= 98.90757302760777\n",
      "iteration number =  1013 Cross Entropy Loss= 98.8723285574915\n",
      "iteration number =  1014 Cross Entropy Loss= 98.83713754003503\n",
      "iteration number =  1015 Cross Entropy Loss= 98.8019998490799\n",
      "iteration number =  1016 Cross Entropy Loss= 98.76691535887232\n",
      "iteration number =  1017 Cross Entropy Loss= 98.7318839440616\n",
      "iteration number =  1018 Cross Entropy Loss= 98.69690547969842\n",
      "iteration number =  1019 Cross Entropy Loss= 98.66197984123332\n",
      "iteration number =  1020 Cross Entropy Loss= 98.62710690451505\n",
      "iteration number =  1021 Cross Entropy Loss= 98.59228654578897\n",
      "iteration number =  1022 Cross Entropy Loss= 98.5575186416955\n",
      "iteration number =  1023 Cross Entropy Loss= 98.52280306926842\n",
      "iteration number =  1024 Cross Entropy Loss= 98.48813970593349\n",
      "iteration number =  1025 Cross Entropy Loss= 98.4535284295067\n",
      "iteration number =  1026 Cross Entropy Loss= 98.41896911819283\n",
      "iteration number =  1027 Cross Entropy Loss= 98.3844616505838\n",
      "iteration number =  1028 Cross Entropy Loss= 98.35000590565727\n",
      "iteration number =  1029 Cross Entropy Loss= 98.31560176277489\n",
      "iteration number =  1030 Cross Entropy Loss= 98.28124910168101\n",
      "iteration number =  1031 Cross Entropy Loss= 98.24694780250096\n",
      "iteration number =  1032 Cross Entropy Loss= 98.21269774573963\n",
      "iteration number =  1033 Cross Entropy Loss= 98.17849881227998\n",
      "iteration number =  1034 Cross Entropy Loss= 98.14435088338148\n",
      "iteration number =  1035 Cross Entropy Loss= 98.11025384067865\n",
      "iteration number =  1036 Cross Entropy Loss= 98.07620756617956\n",
      "iteration number =  1037 Cross Entropy Loss= 98.04221194226439\n",
      "iteration number =  1038 Cross Entropy Loss= 98.00826685168384\n",
      "iteration number =  1039 Cross Entropy Loss= 97.97437217755785\n",
      "iteration number =  1040 Cross Entropy Loss= 97.94052780337398\n",
      "iteration number =  1041 Cross Entropy Loss= 97.90673361298605\n",
      "iteration number =  1042 Cross Entropy Loss= 97.8729894906126\n",
      "iteration number =  1043 Cross Entropy Loss= 97.83929532083562\n",
      "iteration number =  1044 Cross Entropy Loss= 97.80565098859891\n",
      "iteration number =  1045 Cross Entropy Loss= 97.77205637920687\n",
      "iteration number =  1046 Cross Entropy Loss= 97.73851137832285\n",
      "iteration number =  1047 Cross Entropy Loss= 97.70501587196799\n",
      "iteration number =  1048 Cross Entropy Loss= 97.67156974651958\n",
      "iteration number =  1049 Cross Entropy Loss= 97.63817288870985\n",
      "iteration number =  1050 Cross Entropy Loss= 97.60482518562443\n",
      "iteration number =  1051 Cross Entropy Loss= 97.57152652470111\n",
      "iteration number =  1052 Cross Entropy Loss= 97.53827679372833\n",
      "iteration number =  1053 Cross Entropy Loss= 97.50507588084388\n",
      "iteration number =  1054 Cross Entropy Loss= 97.47192367453351\n",
      "iteration number =  1055 Cross Entropy Loss= 97.43882006362966\n",
      "iteration number =  1056 Cross Entropy Loss= 97.40576493730993\n",
      "iteration number =  1057 Cross Entropy Loss= 97.37275818509582\n",
      "iteration number =  1058 Cross Entropy Loss= 97.33979969685153\n",
      "iteration number =  1059 Cross Entropy Loss= 97.30688936278241\n",
      "iteration number =  1060 Cross Entropy Loss= 97.2740270734337\n",
      "iteration number =  1061 Cross Entropy Loss= 97.24121271968932\n",
      "iteration number =  1062 Cross Entropy Loss= 97.2084461927704\n",
      "iteration number =  1063 Cross Entropy Loss= 97.17572738423402\n",
      "iteration number =  1064 Cross Entropy Loss= 97.14305618597201\n",
      "iteration number =  1065 Cross Entropy Loss= 97.11043249020952\n",
      "iteration number =  1066 Cross Entropy Loss= 97.07785618950373\n",
      "iteration number =  1067 Cross Entropy Loss= 97.04532717674275\n",
      "iteration number =  1068 Cross Entropy Loss= 97.01284534514403\n",
      "iteration number =  1069 Cross Entropy Loss= 96.98041058825339\n",
      "iteration number =  1070 Cross Entropy Loss= 96.9480227999436\n",
      "iteration number =  1071 Cross Entropy Loss= 96.91568187441311\n",
      "iteration number =  1072 Cross Entropy Loss= 96.88338770618488\n",
      "iteration number =  1073 Cross Entropy Loss= 96.85114019010504\n",
      "iteration number =  1074 Cross Entropy Loss= 96.81893922134168\n",
      "iteration number =  1075 Cross Entropy Loss= 96.78678469538367\n",
      "iteration number =  1076 Cross Entropy Loss= 96.75467650803934\n",
      "iteration number =  1077 Cross Entropy Loss= 96.72261455543529\n",
      "iteration number =  1078 Cross Entropy Loss= 96.69059873401517\n",
      "iteration number =  1079 Cross Entropy Loss= 96.6586289405385\n",
      "iteration number =  1080 Cross Entropy Loss= 96.62670507207936\n",
      "iteration number =  1081 Cross Entropy Loss= 96.59482702602533\n",
      "iteration number =  1082 Cross Entropy Loss= 96.56299470007613\n",
      "iteration number =  1083 Cross Entropy Loss= 96.53120799224259\n",
      "iteration number =  1084 Cross Entropy Loss= 96.49946680084531\n",
      "iteration number =  1085 Cross Entropy Loss= 96.46777102451361\n",
      "iteration number =  1086 Cross Entropy Loss= 96.43612056218424\n",
      "iteration number =  1087 Cross Entropy Loss= 96.4045153131003\n",
      "iteration number =  1088 Cross Entropy Loss= 96.37295517681004\n",
      "iteration number =  1089 Cross Entropy Loss= 96.34144005316567\n",
      "iteration number =  1090 Cross Entropy Loss= 96.30996984232225\n",
      "iteration number =  1091 Cross Entropy Loss= 96.27854444473648\n",
      "iteration number =  1092 Cross Entropy Loss= 96.24716376116567\n",
      "iteration number =  1093 Cross Entropy Loss= 96.21582769266647\n",
      "iteration number =  1094 Cross Entropy Loss= 96.18453614059379\n",
      "iteration number =  1095 Cross Entropy Loss= 96.15328900659969\n",
      "iteration number =  1096 Cross Entropy Loss= 96.12208619263224\n",
      "iteration number =  1097 Cross Entropy Loss= 96.0909276009344\n",
      "iteration number =  1098 Cross Entropy Loss= 96.0598131340429\n",
      "iteration number =  1099 Cross Entropy Loss= 96.02874269478713\n",
      "iteration number =  1100 Cross Entropy Loss= 95.99771618628805\n",
      "iteration number =  1101 Cross Entropy Loss= 95.96673351195707\n",
      "iteration number =  1102 Cross Entropy Loss= 95.935794575495\n",
      "iteration number =  1103 Cross Entropy Loss= 95.9048992808909\n",
      "iteration number =  1104 Cross Entropy Loss= 95.87404753242106\n",
      "iteration number =  1105 Cross Entropy Loss= 95.84323923464783\n",
      "iteration number =  1106 Cross Entropy Loss= 95.81247429241867\n",
      "iteration number =  1107 Cross Entropy Loss= 95.78175261086497\n",
      "iteration number =  1108 Cross Entropy Loss= 95.75107409540105\n",
      "iteration number =  1109 Cross Entropy Loss= 95.72043865172304\n",
      "iteration number =  1110 Cross Entropy Loss= 95.68984618580797\n",
      "iteration number =  1111 Cross Entropy Loss= 95.65929660391245\n",
      "iteration number =  1112 Cross Entropy Loss= 95.62878981257194\n",
      "iteration number =  1113 Cross Entropy Loss= 95.59832571859948\n",
      "iteration number =  1114 Cross Entropy Loss= 95.56790422908472\n",
      "iteration number =  1115 Cross Entropy Loss= 95.53752525139295\n",
      "iteration number =  1116 Cross Entropy Loss= 95.50718869316401\n",
      "iteration number =  1117 Cross Entropy Loss= 95.47689446231124\n",
      "iteration number =  1118 Cross Entropy Loss= 95.44664246702055\n",
      "iteration number =  1119 Cross Entropy Loss= 95.41643261574933\n",
      "iteration number =  1120 Cross Entropy Loss= 95.38626481722548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1121 Cross Entropy Loss= 95.35613898044639\n",
      "iteration number =  1122 Cross Entropy Loss= 95.32605501467796\n",
      "iteration number =  1123 Cross Entropy Loss= 95.29601282945357\n",
      "iteration number =  1124 Cross Entropy Loss= 95.26601233457313\n",
      "iteration number =  1125 Cross Entropy Loss= 95.23605344010207\n",
      "iteration number =  1126 Cross Entropy Loss= 95.20613605637033\n",
      "iteration number =  1127 Cross Entropy Loss= 95.17626009397145\n",
      "iteration number =  1128 Cross Entropy Loss= 95.14642546376156\n",
      "iteration number =  1129 Cross Entropy Loss= 95.11663207685837\n",
      "iteration number =  1130 Cross Entropy Loss= 95.08687984464032\n",
      "iteration number =  1131 Cross Entropy Loss= 95.05716867874546\n",
      "iteration number =  1132 Cross Entropy Loss= 95.02749849107062\n",
      "iteration number =  1133 Cross Entropy Loss= 94.99786919377041\n",
      "iteration number =  1134 Cross Entropy Loss= 94.96828069925633\n",
      "iteration number =  1135 Cross Entropy Loss= 94.93873292019566\n",
      "iteration number =  1136 Cross Entropy Loss= 94.90922576951073\n",
      "iteration number =  1137 Cross Entropy Loss= 94.87975916037789\n",
      "iteration number =  1138 Cross Entropy Loss= 94.85033300622655\n",
      "iteration number =  1139 Cross Entropy Loss= 94.82094722073825\n",
      "iteration number =  1140 Cross Entropy Loss= 94.79160171784582\n",
      "iteration number =  1141 Cross Entropy Loss= 94.76229641173241\n",
      "iteration number =  1142 Cross Entropy Loss= 94.73303121683051\n",
      "iteration number =  1143 Cross Entropy Loss= 94.70380604782122\n",
      "iteration number =  1144 Cross Entropy Loss= 94.67462081963312\n",
      "iteration number =  1145 Cross Entropy Loss= 94.6454754474415\n",
      "iteration number =  1146 Cross Entropy Loss= 94.61636984666745\n",
      "iteration number =  1147 Cross Entropy Loss= 94.58730393297698\n",
      "iteration number =  1148 Cross Entropy Loss= 94.55827762228\n",
      "iteration number =  1149 Cross Entropy Loss= 94.52929083072965\n",
      "iteration number =  1150 Cross Entropy Loss= 94.50034347472123\n",
      "iteration number =  1151 Cross Entropy Loss= 94.47143547089138\n",
      "iteration number =  1152 Cross Entropy Loss= 94.44256673611723\n",
      "iteration number =  1153 Cross Entropy Loss= 94.41373718751552\n",
      "iteration number =  1154 Cross Entropy Loss= 94.38494674244171\n",
      "iteration number =  1155 Cross Entropy Loss= 94.35619531848913\n",
      "iteration number =  1156 Cross Entropy Loss= 94.3274828334881\n",
      "iteration number =  1157 Cross Entropy Loss= 94.29880920550515\n",
      "iteration number =  1158 Cross Entropy Loss= 94.27017435284202\n",
      "iteration number =  1159 Cross Entropy Loss= 94.24157819403493\n",
      "iteration number =  1160 Cross Entropy Loss= 94.21302064785374\n",
      "iteration number =  1161 Cross Entropy Loss= 94.18450163330104\n",
      "iteration number =  1162 Cross Entropy Loss= 94.15602106961131\n",
      "iteration number =  1163 Cross Entropy Loss= 94.12757887625018\n",
      "iteration number =  1164 Cross Entropy Loss= 94.09917497291349\n",
      "iteration number =  1165 Cross Entropy Loss= 94.07080927952649\n",
      "iteration number =  1166 Cross Entropy Loss= 94.04248171624309\n",
      "iteration number =  1167 Cross Entropy Loss= 94.01419220344495\n",
      "iteration number =  1168 Cross Entropy Loss= 93.9859406617407\n",
      "iteration number =  1169 Cross Entropy Loss= 93.95772701196512\n",
      "iteration number =  1170 Cross Entropy Loss= 93.92955117517829\n",
      "iteration number =  1171 Cross Entropy Loss= 93.90141307266492\n",
      "iteration number =  1172 Cross Entropy Loss= 93.87331262593338\n",
      "iteration number =  1173 Cross Entropy Loss= 93.84524975671498\n",
      "iteration number =  1174 Cross Entropy Loss= 93.81722438696319\n",
      "iteration number =  1175 Cross Entropy Loss= 93.78923643885281\n",
      "iteration number =  1176 Cross Entropy Loss= 93.76128583477919\n",
      "iteration number =  1177 Cross Entropy Loss= 93.73337249735746\n",
      "iteration number =  1178 Cross Entropy Loss= 93.70549634942174\n",
      "iteration number =  1179 Cross Entropy Loss= 93.67765731402432\n",
      "iteration number =  1180 Cross Entropy Loss= 93.64985531443494\n",
      "iteration number =  1181 Cross Entropy Loss= 93.62209027414008\n",
      "iteration number =  1182 Cross Entropy Loss= 93.59436211684195\n",
      "iteration number =  1183 Cross Entropy Loss= 93.566670766458\n",
      "iteration number =  1184 Cross Entropy Loss= 93.53901614712\n",
      "iteration number =  1185 Cross Entropy Loss= 93.51139818317334\n",
      "iteration number =  1186 Cross Entropy Loss= 93.48381679917622\n",
      "iteration number =  1187 Cross Entropy Loss= 93.45627191989897\n",
      "iteration number =  1188 Cross Entropy Loss= 93.42876347032325\n",
      "iteration number =  1189 Cross Entropy Loss= 93.40129137564128\n",
      "iteration number =  1190 Cross Entropy Loss= 93.37385556125517\n",
      "iteration number =  1191 Cross Entropy Loss= 93.34645595277614\n",
      "iteration number =  1192 Cross Entropy Loss= 93.31909247602377\n",
      "iteration number =  1193 Cross Entropy Loss= 93.29176505702526\n",
      "iteration number =  1194 Cross Entropy Loss= 93.26447362201476\n",
      "iteration number =  1195 Cross Entropy Loss= 93.23721809743256\n",
      "iteration number =  1196 Cross Entropy Loss= 93.20999840992445\n",
      "iteration number =  1197 Cross Entropy Loss= 93.1828144863409\n",
      "iteration number =  1198 Cross Entropy Loss= 93.15566625373647\n",
      "iteration number =  1199 Cross Entropy Loss= 93.12855363936892\n",
      "iteration number =  1200 Cross Entropy Loss= 93.10147657069867\n",
      "iteration number =  1201 Cross Entropy Loss= 93.07443497538799\n",
      "iteration number =  1202 Cross Entropy Loss= 93.04742878130034\n",
      "iteration number =  1203 Cross Entropy Loss= 93.02045791649962\n",
      "iteration number =  1204 Cross Entropy Loss= 92.99352230924951\n",
      "iteration number =  1205 Cross Entropy Loss= 92.96662188801275\n",
      "iteration number =  1206 Cross Entropy Loss= 92.93975658145047\n",
      "iteration number =  1207 Cross Entropy Loss= 92.91292631842144\n",
      "iteration number =  1208 Cross Entropy Loss= 92.88613102798149\n",
      "iteration number =  1209 Cross Entropy Loss= 92.85937063938269\n",
      "iteration number =  1210 Cross Entropy Loss= 92.83264508207274\n",
      "iteration number =  1211 Cross Entropy Loss= 92.80595428569433\n",
      "iteration number =  1212 Cross Entropy Loss= 92.77929818008437\n",
      "iteration number =  1213 Cross Entropy Loss= 92.75267669527338\n",
      "iteration number =  1214 Cross Entropy Loss= 92.72608976148479\n",
      "iteration number =  1215 Cross Entropy Loss= 92.69953730913423\n",
      "iteration number =  1216 Cross Entropy Loss= 92.67301926882902\n",
      "iteration number =  1217 Cross Entropy Loss= 92.64653557136734\n",
      "iteration number =  1218 Cross Entropy Loss= 92.62008614773755\n",
      "iteration number =  1219 Cross Entropy Loss= 92.59367092911775\n",
      "iteration number =  1220 Cross Entropy Loss= 92.56728984687487\n",
      "iteration number =  1221 Cross Entropy Loss= 92.54094283256421\n",
      "iteration number =  1222 Cross Entropy Loss= 92.51462981792868\n",
      "iteration number =  1223 Cross Entropy Loss= 92.48835073489815\n",
      "iteration number =  1224 Cross Entropy Loss= 92.46210551558889\n",
      "iteration number =  1225 Cross Entropy Loss= 92.43589409230285\n",
      "iteration number =  1226 Cross Entropy Loss= 92.40971639752703\n",
      "iteration number =  1227 Cross Entropy Loss= 92.38357236393293\n",
      "iteration number =  1228 Cross Entropy Loss= 92.35746192437577\n",
      "iteration number =  1229 Cross Entropy Loss= 92.33138501189399\n",
      "iteration number =  1230 Cross Entropy Loss= 92.30534155970852\n",
      "iteration number =  1231 Cross Entropy Loss= 92.27933150122226\n",
      "iteration number =  1232 Cross Entropy Loss= 92.25335477001931\n",
      "iteration number =  1233 Cross Entropy Loss= 92.22741129986454\n",
      "iteration number =  1234 Cross Entropy Loss= 92.20150102470281\n",
      "iteration number =  1235 Cross Entropy Loss= 92.1756238786584\n",
      "iteration number =  1236 Cross Entropy Loss= 92.14977979603442\n",
      "iteration number =  1237 Cross Entropy Loss= 92.12396871131222\n",
      "iteration number =  1238 Cross Entropy Loss= 92.09819055915071\n",
      "iteration number =  1239 Cross Entropy Loss= 92.07244527438581\n",
      "iteration number =  1240 Cross Entropy Loss= 92.04673279202984\n",
      "iteration number =  1241 Cross Entropy Loss= 92.02105304727088\n",
      "iteration number =  1242 Cross Entropy Loss= 91.99540597547222\n",
      "iteration number =  1243 Cross Entropy Loss= 91.96979151217174\n",
      "iteration number =  1244 Cross Entropy Loss= 91.94420959308134\n",
      "iteration number =  1245 Cross Entropy Loss= 91.91866015408627\n",
      "iteration number =  1246 Cross Entropy Loss= 91.89314313124467\n",
      "iteration number =  1247 Cross Entropy Loss= 91.8676584607869\n",
      "iteration number =  1248 Cross Entropy Loss= 91.84220607911493\n",
      "iteration number =  1249 Cross Entropy Loss= 91.81678592280186\n",
      "iteration number =  1250 Cross Entropy Loss= 91.79139792859124\n",
      "iteration number =  1251 Cross Entropy Loss= 91.7660420333965\n",
      "iteration number =  1252 Cross Entropy Loss= 91.7407181743005\n",
      "iteration number =  1253 Cross Entropy Loss= 91.7154262885548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1254 Cross Entropy Loss= 91.69016631357916\n",
      "iteration number =  1255 Cross Entropy Loss= 91.664938186961\n",
      "iteration number =  1256 Cross Entropy Loss= 91.63974184645475\n",
      "iteration number =  1257 Cross Entropy Loss= 91.61457722998142\n",
      "iteration number =  1258 Cross Entropy Loss= 91.5894442756279\n",
      "iteration number =  1259 Cross Entropy Loss= 91.56434292164647\n",
      "iteration number =  1260 Cross Entropy Loss= 91.53927310645425\n",
      "iteration number =  1261 Cross Entropy Loss= 91.51423476863263\n",
      "iteration number =  1262 Cross Entropy Loss= 91.48922784692671\n",
      "iteration number =  1263 Cross Entropy Loss= 91.46425228024478\n",
      "iteration number =  1264 Cross Entropy Loss= 91.4393080076577\n",
      "iteration number =  1265 Cross Entropy Loss= 91.4143949683985\n",
      "iteration number =  1266 Cross Entropy Loss= 91.38951310186167\n",
      "iteration number =  1267 Cross Entropy Loss= 91.3646623476027\n",
      "iteration number =  1268 Cross Entropy Loss= 91.3398426453376\n",
      "iteration number =  1269 Cross Entropy Loss= 91.31505393494223\n",
      "iteration number =  1270 Cross Entropy Loss= 91.29029615645186\n",
      "iteration number =  1271 Cross Entropy Loss= 91.26556925006062\n",
      "iteration number =  1272 Cross Entropy Loss= 91.24087315612095\n",
      "iteration number =  1273 Cross Entropy Loss= 91.21620781514315\n",
      "iteration number =  1274 Cross Entropy Loss= 91.19157316779467\n",
      "iteration number =  1275 Cross Entropy Loss= 91.16696915489982\n",
      "iteration number =  1276 Cross Entropy Loss= 91.14239571743906\n",
      "iteration number =  1277 Cross Entropy Loss= 91.11785279654862\n",
      "iteration number =  1278 Cross Entropy Loss= 91.09334033351985\n",
      "iteration number =  1279 Cross Entropy Loss= 91.06885826979885\n",
      "iteration number =  1280 Cross Entropy Loss= 91.04440654698583\n",
      "iteration number =  1281 Cross Entropy Loss= 91.01998510683465\n",
      "iteration number =  1282 Cross Entropy Loss= 90.99559389125233\n",
      "iteration number =  1283 Cross Entropy Loss= 90.97123284229853\n",
      "iteration number =  1284 Cross Entropy Loss= 90.94690190218502\n",
      "iteration number =  1285 Cross Entropy Loss= 90.9226010132752\n",
      "iteration number =  1286 Cross Entropy Loss= 90.89833011808358\n",
      "iteration number =  1287 Cross Entropy Loss= 90.87408915927536\n",
      "iteration number =  1288 Cross Entropy Loss= 90.84987807966577\n",
      "iteration number =  1289 Cross Entropy Loss= 90.82569682221975\n",
      "iteration number =  1290 Cross Entropy Loss= 90.80154533005134\n",
      "iteration number =  1291 Cross Entropy Loss= 90.77742354642328\n",
      "iteration number =  1292 Cross Entropy Loss= 90.7533314147464\n",
      "iteration number =  1293 Cross Entropy Loss= 90.72926887857926\n",
      "iteration number =  1294 Cross Entropy Loss= 90.70523588162756\n",
      "iteration number =  1295 Cross Entropy Loss= 90.68123236774375\n",
      "iteration number =  1296 Cross Entropy Loss= 90.6572582809265\n",
      "iteration number =  1297 Cross Entropy Loss= 90.63331356532017\n",
      "iteration number =  1298 Cross Entropy Loss= 90.60939816521442\n",
      "iteration number =  1299 Cross Entropy Loss= 90.58551202504373\n",
      "iteration number =  1300 Cross Entropy Loss= 90.56165508938683\n",
      "iteration number =  1301 Cross Entropy Loss= 90.53782730296635\n",
      "iteration number =  1302 Cross Entropy Loss= 90.51402861064827\n",
      "iteration number =  1303 Cross Entropy Loss= 90.49025895744147\n",
      "iteration number =  1304 Cross Entropy Loss= 90.4665182884973\n",
      "iteration number =  1305 Cross Entropy Loss= 90.44280654910902\n",
      "iteration number =  1306 Cross Entropy Loss= 90.41912368471145\n",
      "iteration number =  1307 Cross Entropy Loss= 90.39546964088045\n",
      "iteration number =  1308 Cross Entropy Loss= 90.37184436333246\n",
      "iteration number =  1309 Cross Entropy Loss= 90.34824779792407\n",
      "iteration number =  1310 Cross Entropy Loss= 90.32467989065154\n",
      "iteration number =  1311 Cross Entropy Loss= 90.30114058765032\n",
      "iteration number =  1312 Cross Entropy Loss= 90.27762983519469\n",
      "iteration number =  1313 Cross Entropy Loss= 90.25414757969722\n",
      "iteration number =  1314 Cross Entropy Loss= 90.2306937677083\n",
      "iteration number =  1315 Cross Entropy Loss= 90.20726834591586\n",
      "iteration number =  1316 Cross Entropy Loss= 90.1838712611447\n",
      "iteration number =  1317 Cross Entropy Loss= 90.16050246035624\n",
      "iteration number =  1318 Cross Entropy Loss= 90.13716189064792\n",
      "iteration number =  1319 Cross Entropy Loss= 90.11384949925291\n",
      "iteration number =  1320 Cross Entropy Loss= 90.09056523353951\n",
      "iteration number =  1321 Cross Entropy Loss= 90.06730904101087\n",
      "iteration number =  1322 Cross Entropy Loss= 90.04408086930448\n",
      "iteration number =  1323 Cross Entropy Loss= 90.02088066619167\n",
      "iteration number =  1324 Cross Entropy Loss= 89.99770837957735\n",
      "iteration number =  1325 Cross Entropy Loss= 89.97456395749943\n",
      "iteration number =  1326 Cross Entropy Loss= 89.95144734812843\n",
      "iteration number =  1327 Cross Entropy Loss= 89.92835849976706\n",
      "iteration number =  1328 Cross Entropy Loss= 89.90529736084986\n",
      "iteration number =  1329 Cross Entropy Loss= 89.88226387994267\n",
      "iteration number =  1330 Cross Entropy Loss= 89.85925800574225\n",
      "iteration number =  1331 Cross Entropy Loss= 89.83627968707589\n",
      "iteration number =  1332 Cross Entropy Loss= 89.81332887290097\n",
      "iteration number =  1333 Cross Entropy Loss= 89.79040551230452\n",
      "iteration number =  1334 Cross Entropy Loss= 89.76750955450282\n",
      "iteration number =  1335 Cross Entropy Loss= 89.74464094884101\n",
      "iteration number =  1336 Cross Entropy Loss= 89.7217996447927\n",
      "iteration number =  1337 Cross Entropy Loss= 89.69898559195943\n",
      "iteration number =  1338 Cross Entropy Loss= 89.67619874007042\n",
      "iteration number =  1339 Cross Entropy Loss= 89.65343903898207\n",
      "iteration number =  1340 Cross Entropy Loss= 89.63070643867759\n",
      "iteration number =  1341 Cross Entropy Loss= 89.60800088926656\n",
      "iteration number =  1342 Cross Entropy Loss= 89.58532234098459\n",
      "iteration number =  1343 Cross Entropy Loss= 89.56267074419284\n",
      "iteration number =  1344 Cross Entropy Loss= 89.5400460493777\n",
      "iteration number =  1345 Cross Entropy Loss= 89.5174482071503\n",
      "iteration number =  1346 Cross Entropy Loss= 89.49487716824623\n",
      "iteration number =  1347 Cross Entropy Loss= 89.47233288352503\n",
      "iteration number =  1348 Cross Entropy Loss= 89.4498153039699\n",
      "iteration number =  1349 Cross Entropy Loss= 89.42732438068721\n",
      "iteration number =  1350 Cross Entropy Loss= 89.40486006490616\n",
      "iteration number =  1351 Cross Entropy Loss= 89.38242230797843\n",
      "iteration number =  1352 Cross Entropy Loss= 89.36001106137772\n",
      "iteration number =  1353 Cross Entropy Loss= 89.33762627669938\n",
      "iteration number =  1354 Cross Entropy Loss= 89.31526790566006\n",
      "iteration number =  1355 Cross Entropy Loss= 89.29293590009732\n",
      "iteration number =  1356 Cross Entropy Loss= 89.27063021196918\n",
      "iteration number =  1357 Cross Entropy Loss= 89.24835079335386\n",
      "iteration number =  1358 Cross Entropy Loss= 89.22609759644932\n",
      "iteration number =  1359 Cross Entropy Loss= 89.20387057357287\n",
      "iteration number =  1360 Cross Entropy Loss= 89.18166967716083\n",
      "iteration number =  1361 Cross Entropy Loss= 89.15949485976817\n",
      "iteration number =  1362 Cross Entropy Loss= 89.13734607406812\n",
      "iteration number =  1363 Cross Entropy Loss= 89.11522327285175\n",
      "iteration number =  1364 Cross Entropy Loss= 89.09312640902769\n",
      "iteration number =  1365 Cross Entropy Loss= 89.0710554356217\n",
      "iteration number =  1366 Cross Entropy Loss= 89.04901030577632\n",
      "iteration number =  1367 Cross Entropy Loss= 89.0269909727505\n",
      "iteration number =  1368 Cross Entropy Loss= 89.0049973899192\n",
      "iteration number =  1369 Cross Entropy Loss= 88.98302951077315\n",
      "iteration number =  1370 Cross Entropy Loss= 88.96108728891832\n",
      "iteration number =  1371 Cross Entropy Loss= 88.93917067807567\n",
      "iteration number =  1372 Cross Entropy Loss= 88.9172796320808\n",
      "iteration number =  1373 Cross Entropy Loss= 88.89541410488349\n",
      "iteration number =  1374 Cross Entropy Loss= 88.87357405054745\n",
      "iteration number =  1375 Cross Entropy Loss= 88.85175942324989\n",
      "iteration number =  1376 Cross Entropy Loss= 88.82997017728125\n",
      "iteration number =  1377 Cross Entropy Loss= 88.80820626704477\n",
      "iteration number =  1378 Cross Entropy Loss= 88.78646764705614\n",
      "iteration number =  1379 Cross Entropy Loss= 88.76475427194322\n",
      "iteration number =  1380 Cross Entropy Loss= 88.74306609644562\n",
      "iteration number =  1381 Cross Entropy Loss= 88.72140307541437\n",
      "iteration number =  1382 Cross Entropy Loss= 88.69976516381163\n",
      "iteration number =  1383 Cross Entropy Loss= 88.67815231671025\n",
      "iteration number =  1384 Cross Entropy Loss= 88.65656448929352\n",
      "iteration number =  1385 Cross Entropy Loss= 88.63500163685472\n",
      "iteration number =  1386 Cross Entropy Loss= 88.61346371479692\n",
      "iteration number =  1387 Cross Entropy Loss= 88.59195067863253\n",
      "iteration number =  1388 Cross Entropy Loss= 88.57046248398296\n",
      "iteration number =  1389 Cross Entropy Loss= 88.54899908657839\n",
      "iteration number =  1390 Cross Entropy Loss= 88.52756044225731\n",
      "iteration number =  1391 Cross Entropy Loss= 88.50614650696625\n",
      "iteration number =  1392 Cross Entropy Loss= 88.48475723675942\n",
      "iteration number =  1393 Cross Entropy Loss= 88.46339258779847\n",
      "iteration number =  1394 Cross Entropy Loss= 88.44205251635194\n",
      "iteration number =  1395 Cross Entropy Loss= 88.42073697879522\n",
      "iteration number =  1396 Cross Entropy Loss= 88.39944593160999\n",
      "iteration number =  1397 Cross Entropy Loss= 88.378179331384\n",
      "iteration number =  1398 Cross Entropy Loss= 88.35693713481072\n",
      "iteration number =  1399 Cross Entropy Loss= 88.33571929868903\n",
      "iteration number =  1400 Cross Entropy Loss= 88.31452577992283\n",
      "iteration number =  1401 Cross Entropy Loss= 88.29335653552084\n",
      "iteration number =  1402 Cross Entropy Loss= 88.27221152259617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1403 Cross Entropy Loss= 88.25109069836606\n",
      "iteration number =  1404 Cross Entropy Loss= 88.22999402015148\n",
      "iteration number =  1405 Cross Entropy Loss= 88.20892144537699\n",
      "iteration number =  1406 Cross Entropy Loss= 88.1878729315702\n",
      "iteration number =  1407 Cross Entropy Loss= 88.1668484363616\n",
      "iteration number =  1408 Cross Entropy Loss= 88.14584791748425\n",
      "iteration number =  1409 Cross Entropy Loss= 88.12487133277332\n",
      "iteration number =  1410 Cross Entropy Loss= 88.10391864016599\n",
      "iteration number =  1411 Cross Entropy Loss= 88.08298979770098\n",
      "iteration number =  1412 Cross Entropy Loss= 88.06208476351827\n",
      "iteration number =  1413 Cross Entropy Loss= 88.0412034958589\n",
      "iteration number =  1414 Cross Entropy Loss= 88.02034595306445\n",
      "iteration number =  1415 Cross Entropy Loss= 87.999512093577\n",
      "iteration number =  1416 Cross Entropy Loss= 87.97870187593857\n",
      "iteration number =  1417 Cross Entropy Loss= 87.95791525879099\n",
      "iteration number =  1418 Cross Entropy Loss= 87.93715220087554\n",
      "iteration number =  1419 Cross Entropy Loss= 87.91641266103265\n",
      "iteration number =  1420 Cross Entropy Loss= 87.89569659820155\n",
      "iteration number =  1421 Cross Entropy Loss= 87.87500397142006\n",
      "iteration number =  1422 Cross Entropy Loss= 87.85433473982425\n",
      "iteration number =  1423 Cross Entropy Loss= 87.83368886264812\n",
      "iteration number =  1424 Cross Entropy Loss= 87.81306629922332\n",
      "iteration number =  1425 Cross Entropy Loss= 87.79246700897892\n",
      "iteration number =  1426 Cross Entropy Loss= 87.77189095144097\n",
      "iteration number =  1427 Cross Entropy Loss= 87.75133808623235\n",
      "iteration number =  1428 Cross Entropy Loss= 87.73080837307242\n",
      "iteration number =  1429 Cross Entropy Loss= 87.7103017717767\n",
      "iteration number =  1430 Cross Entropy Loss= 87.68981824225662\n",
      "iteration number =  1431 Cross Entropy Loss= 87.66935774451926\n",
      "iteration number =  1432 Cross Entropy Loss= 87.64892023866695\n",
      "iteration number =  1433 Cross Entropy Loss= 87.62850568489714\n",
      "iteration number =  1434 Cross Entropy Loss= 87.60811404350197\n",
      "iteration number =  1435 Cross Entropy Loss= 87.5877452748681\n",
      "iteration number =  1436 Cross Entropy Loss= 87.56739933947628\n",
      "iteration number =  1437 Cross Entropy Loss= 87.54707619790129\n",
      "iteration number =  1438 Cross Entropy Loss= 87.52677581081144\n",
      "iteration number =  1439 Cross Entropy Loss= 87.50649813896838\n",
      "iteration number =  1440 Cross Entropy Loss= 87.48624314322689\n",
      "iteration number =  1441 Cross Entropy Loss= 87.46601078453443\n",
      "iteration number =  1442 Cross Entropy Loss= 87.44580102393103\n",
      "iteration number =  1443 Cross Entropy Loss= 87.42561382254894\n",
      "iteration number =  1444 Cross Entropy Loss= 87.40544914161237\n",
      "iteration number =  1445 Cross Entropy Loss= 87.38530694243715\n",
      "iteration number =  1446 Cross Entropy Loss= 87.3651871864306\n",
      "iteration number =  1447 Cross Entropy Loss= 87.3450898350911\n",
      "iteration number =  1448 Cross Entropy Loss= 87.32501485000792\n",
      "iteration number =  1449 Cross Entropy Loss= 87.3049621928609\n",
      "iteration number =  1450 Cross Entropy Loss= 87.28493182542023\n",
      "iteration number =  1451 Cross Entropy Loss= 87.26492370954611\n",
      "iteration number =  1452 Cross Entropy Loss= 87.24493780718856\n",
      "iteration number =  1453 Cross Entropy Loss= 87.22497408038708\n",
      "iteration number =  1454 Cross Entropy Loss= 87.20503249127047\n",
      "iteration number =  1455 Cross Entropy Loss= 87.18511300205645\n",
      "iteration number =  1456 Cross Entropy Loss= 87.16521557505153\n",
      "iteration number =  1457 Cross Entropy Loss= 87.14534017265062\n",
      "iteration number =  1458 Cross Entropy Loss= 87.12548675733686\n",
      "iteration number =  1459 Cross Entropy Loss= 87.1056552916813\n",
      "iteration number =  1460 Cross Entropy Loss= 87.08584573834271\n",
      "iteration number =  1461 Cross Entropy Loss= 87.06605806006722\n",
      "iteration number =  1462 Cross Entropy Loss= 87.04629221968816\n",
      "iteration number =  1463 Cross Entropy Loss= 87.02654818012573\n",
      "iteration number =  1464 Cross Entropy Loss= 87.00682590438679\n",
      "iteration number =  1465 Cross Entropy Loss= 86.98712535556459\n",
      "iteration number =  1466 Cross Entropy Loss= 86.96744649683852\n",
      "iteration number =  1467 Cross Entropy Loss= 86.94778929147381\n",
      "iteration number =  1468 Cross Entropy Loss= 86.92815370282136\n",
      "iteration number =  1469 Cross Entropy Loss= 86.90853969431745\n",
      "iteration number =  1470 Cross Entropy Loss= 86.88894722948342\n",
      "iteration number =  1471 Cross Entropy Loss= 86.86937627192556\n",
      "iteration number =  1472 Cross Entropy Loss= 86.84982678533476\n",
      "iteration number =  1473 Cross Entropy Loss= 86.83029873348622\n",
      "iteration number =  1474 Cross Entropy Loss= 86.8107920802394\n",
      "iteration number =  1475 Cross Entropy Loss= 86.7913067895375\n",
      "iteration number =  1476 Cross Entropy Loss= 86.77184282540748\n",
      "iteration number =  1477 Cross Entropy Loss= 86.75240015195959\n",
      "iteration number =  1478 Cross Entropy Loss= 86.73297873338731\n",
      "iteration number =  1479 Cross Entropy Loss= 86.71357853396695\n",
      "iteration number =  1480 Cross Entropy Loss= 86.69419951805753\n",
      "iteration number =  1481 Cross Entropy Loss= 86.67484165010049\n",
      "iteration number =  1482 Cross Entropy Loss= 86.6555048946194\n",
      "iteration number =  1483 Cross Entropy Loss= 86.63618921621983\n",
      "iteration number =  1484 Cross Entropy Loss= 86.61689457958902\n",
      "iteration number =  1485 Cross Entropy Loss= 86.59762094949568\n",
      "iteration number =  1486 Cross Entropy Loss= 86.57836829078973\n",
      "iteration number =  1487 Cross Entropy Loss= 86.5591365684021\n",
      "iteration number =  1488 Cross Entropy Loss= 86.53992574734447\n",
      "iteration number =  1489 Cross Entropy Loss= 86.520735792709\n",
      "iteration number =  1490 Cross Entropy Loss= 86.50156666966822\n",
      "iteration number =  1491 Cross Entropy Loss= 86.4824183434746\n",
      "iteration number =  1492 Cross Entropy Loss= 86.46329077946052\n",
      "iteration number =  1493 Cross Entropy Loss= 86.44418394303793\n",
      "iteration number =  1494 Cross Entropy Loss= 86.42509779969805\n",
      "iteration number =  1495 Cross Entropy Loss= 86.40603231501137\n",
      "iteration number =  1496 Cross Entropy Loss= 86.38698745462716\n",
      "iteration number =  1497 Cross Entropy Loss= 86.36796318427342\n",
      "iteration number =  1498 Cross Entropy Loss= 86.34895946975656\n",
      "iteration number =  1499 Cross Entropy Loss= 86.32997627696125\n",
      "iteration number =  1500 Cross Entropy Loss= 86.31101357185008\n",
      "iteration number =  1501 Cross Entropy Loss= 86.29207132046346\n",
      "iteration number =  1502 Cross Entropy Loss= 86.2731494889193\n",
      "iteration number =  1503 Cross Entropy Loss= 86.25424804341289\n",
      "iteration number =  1504 Cross Entropy Loss= 86.23536695021653\n",
      "iteration number =  1505 Cross Entropy Loss= 86.21650617567946\n",
      "iteration number =  1506 Cross Entropy Loss= 86.19766568622751\n",
      "iteration number =  1507 Cross Entropy Loss= 86.17884544836298\n",
      "iteration number =  1508 Cross Entropy Loss= 86.16004542866438\n",
      "iteration number =  1509 Cross Entropy Loss= 86.14126559378617\n",
      "iteration number =  1510 Cross Entropy Loss= 86.12250591045866\n",
      "iteration number =  1511 Cross Entropy Loss= 86.10376634548763\n",
      "iteration number =  1512 Cross Entropy Loss= 86.08504686575422\n",
      "iteration number =  1513 Cross Entropy Loss= 86.06634743821473\n",
      "iteration number =  1514 Cross Entropy Loss= 86.04766802990036\n",
      "iteration number =  1515 Cross Entropy Loss= 86.02900860791695\n",
      "iteration number =  1516 Cross Entropy Loss= 86.01036913944486\n",
      "iteration number =  1517 Cross Entropy Loss= 85.99174959173874\n",
      "iteration number =  1518 Cross Entropy Loss= 85.97314993212723\n",
      "iteration number =  1519 Cross Entropy Loss= 85.95457012801285\n",
      "iteration number =  1520 Cross Entropy Loss= 85.93601014687175\n",
      "iteration number =  1521 Cross Entropy Loss= 85.91746995625348\n",
      "iteration number =  1522 Cross Entropy Loss= 85.89894952378086\n",
      "iteration number =  1523 Cross Entropy Loss= 85.88044881714961\n",
      "iteration number =  1524 Cross Entropy Loss= 85.86196780412834\n",
      "iteration number =  1525 Cross Entropy Loss= 85.84350645255822\n",
      "iteration number =  1526 Cross Entropy Loss= 85.82506473035278\n",
      "iteration number =  1527 Cross Entropy Loss= 85.80664260549773\n",
      "iteration number =  1528 Cross Entropy Loss= 85.78824004605077\n",
      "iteration number =  1529 Cross Entropy Loss= 85.76985702014136\n",
      "iteration number =  1530 Cross Entropy Loss= 85.7514934959705\n",
      "iteration number =  1531 Cross Entropy Loss= 85.73314944181058\n",
      "iteration number =  1532 Cross Entropy Loss= 85.71482482600513\n",
      "iteration number =  1533 Cross Entropy Loss= 85.69651961696863\n",
      "iteration number =  1534 Cross Entropy Loss= 85.67823378318631\n",
      "iteration number =  1535 Cross Entropy Loss= 85.65996729321401\n",
      "iteration number =  1536 Cross Entropy Loss= 85.64172011567786\n",
      "iteration number =  1537 Cross Entropy Loss= 85.62349221927414\n",
      "iteration number =  1538 Cross Entropy Loss= 85.60528357276914\n",
      "iteration number =  1539 Cross Entropy Loss= 85.58709414499887\n",
      "iteration number =  1540 Cross Entropy Loss= 85.56892390486892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1541 Cross Entropy Loss= 85.55077282135426\n",
      "iteration number =  1542 Cross Entropy Loss= 85.53264086349897\n",
      "iteration number =  1543 Cross Entropy Loss= 85.51452800041619\n",
      "iteration number =  1544 Cross Entropy Loss= 85.4964342012878\n",
      "iteration number =  1545 Cross Entropy Loss= 85.47835943536424\n",
      "iteration number =  1546 Cross Entropy Loss= 85.4603036719644\n",
      "iteration number =  1547 Cross Entropy Loss= 85.44226688047536\n",
      "iteration number =  1548 Cross Entropy Loss= 85.42424903035214\n",
      "iteration number =  1549 Cross Entropy Loss= 85.40625009111773\n",
      "iteration number =  1550 Cross Entropy Loss= 85.38827003236261\n",
      "iteration number =  1551 Cross Entropy Loss= 85.37030882374476\n",
      "iteration number =  1552 Cross Entropy Loss= 85.3523664349894\n",
      "iteration number =  1553 Cross Entropy Loss= 85.33444283588884\n",
      "iteration number =  1554 Cross Entropy Loss= 85.31653799630223\n",
      "iteration number =  1555 Cross Entropy Loss= 85.29865188615543\n",
      "iteration number =  1556 Cross Entropy Loss= 85.28078447544078\n",
      "iteration number =  1557 Cross Entropy Loss= 85.26293573421697\n",
      "iteration number =  1558 Cross Entropy Loss= 85.24510563260881\n",
      "iteration number =  1559 Cross Entropy Loss= 85.22729414080703\n",
      "iteration number =  1560 Cross Entropy Loss= 85.20950122906814\n",
      "iteration number =  1561 Cross Entropy Loss= 85.19172686771425\n",
      "iteration number =  1562 Cross Entropy Loss= 85.17397102713284\n",
      "iteration number =  1563 Cross Entropy Loss= 85.15623367777661\n",
      "iteration number =  1564 Cross Entropy Loss= 85.1385147901633\n",
      "iteration number =  1565 Cross Entropy Loss= 85.12081433487548\n",
      "iteration number =  1566 Cross Entropy Loss= 85.10313228256044\n",
      "iteration number =  1567 Cross Entropy Loss= 85.08546860392991\n",
      "iteration number =  1568 Cross Entropy Loss= 85.06782326975994\n",
      "iteration number =  1569 Cross Entropy Loss= 85.05019625089075\n",
      "iteration number =  1570 Cross Entropy Loss= 85.0325875182265\n",
      "iteration number =  1571 Cross Entropy Loss= 85.0149970427351\n",
      "iteration number =  1572 Cross Entropy Loss= 84.99742479544811\n",
      "iteration number =  1573 Cross Entropy Loss= 84.97987074746047\n",
      "iteration number =  1574 Cross Entropy Loss= 84.9623348699304\n",
      "iteration number =  1575 Cross Entropy Loss= 84.94481713407917\n",
      "iteration number =  1576 Cross Entropy Loss= 84.927317511191\n",
      "iteration number =  1577 Cross Entropy Loss= 84.90983597261281\n",
      "iteration number =  1578 Cross Entropy Loss= 84.89237248975405\n",
      "iteration number =  1579 Cross Entropy Loss= 84.87492703408657\n",
      "iteration number =  1580 Cross Entropy Loss= 84.85749957714444\n",
      "iteration number =  1581 Cross Entropy Loss= 84.84009009052377\n",
      "iteration number =  1582 Cross Entropy Loss= 84.8226985458825\n",
      "iteration number =  1583 Cross Entropy Loss= 84.80532491494034\n",
      "iteration number =  1584 Cross Entropy Loss= 84.78796916947843\n",
      "iteration number =  1585 Cross Entropy Loss= 84.77063128133935\n",
      "iteration number =  1586 Cross Entropy Loss= 84.75331122242683\n",
      "iteration number =  1587 Cross Entropy Loss= 84.73600896470565\n",
      "iteration number =  1588 Cross Entropy Loss= 84.71872448020139\n",
      "iteration number =  1589 Cross Entropy Loss= 84.70145774100037\n",
      "iteration number =  1590 Cross Entropy Loss= 84.6842087192494\n",
      "iteration number =  1591 Cross Entropy Loss= 84.66697738715564\n",
      "iteration number =  1592 Cross Entropy Loss= 84.64976371698648\n",
      "iteration number =  1593 Cross Entropy Loss= 84.6325676810693\n",
      "iteration number =  1594 Cross Entropy Loss= 84.61538925179136\n",
      "iteration number =  1595 Cross Entropy Loss= 84.59822840159956\n",
      "iteration number =  1596 Cross Entropy Loss= 84.58108510300039\n",
      "iteration number =  1597 Cross Entropy Loss= 84.56395932855972\n",
      "iteration number =  1598 Cross Entropy Loss= 84.54685105090257\n",
      "iteration number =  1599 Cross Entropy Loss= 84.52976024271305\n",
      "iteration number =  1600 Cross Entropy Loss= 84.51268687673416\n",
      "iteration number =  1601 Cross Entropy Loss= 84.49563092576761\n",
      "iteration number =  1602 Cross Entropy Loss= 84.47859236267364\n",
      "iteration number =  1603 Cross Entropy Loss= 84.46157116037097\n",
      "iteration number =  1604 Cross Entropy Loss= 84.44456729183649\n",
      "iteration number =  1605 Cross Entropy Loss= 84.42758073010525\n",
      "iteration number =  1606 Cross Entropy Loss= 84.41061144827017\n",
      "iteration number =  1607 Cross Entropy Loss= 84.39365941948196\n",
      "iteration number =  1608 Cross Entropy Loss= 84.37672461694899\n",
      "iteration number =  1609 Cross Entropy Loss= 84.35980701393703\n",
      "iteration number =  1610 Cross Entropy Loss= 84.34290658376915\n",
      "iteration number =  1611 Cross Entropy Loss= 84.32602329982565\n",
      "iteration number =  1612 Cross Entropy Loss= 84.30915713554371\n",
      "iteration number =  1613 Cross Entropy Loss= 84.29230806441745\n",
      "iteration number =  1614 Cross Entropy Loss= 84.27547605999763\n",
      "iteration number =  1615 Cross Entropy Loss= 84.25866109589151\n",
      "iteration number =  1616 Cross Entropy Loss= 84.24186314576282\n",
      "iteration number =  1617 Cross Entropy Loss= 84.22508218333141\n",
      "iteration number =  1618 Cross Entropy Loss= 84.20831818237326\n",
      "iteration number =  1619 Cross Entropy Loss= 84.1915711167203\n",
      "iteration number =  1620 Cross Entropy Loss= 84.17484096026018\n",
      "iteration number =  1621 Cross Entropy Loss= 84.15812768693621\n",
      "iteration number =  1622 Cross Entropy Loss= 84.14143127074715\n",
      "iteration number =  1623 Cross Entropy Loss= 84.12475168574709\n",
      "iteration number =  1624 Cross Entropy Loss= 84.10808890604531\n",
      "iteration number =  1625 Cross Entropy Loss= 84.09144290580608\n",
      "iteration number =  1626 Cross Entropy Loss= 84.07481365924858\n",
      "iteration number =  1627 Cross Entropy Loss= 84.0582011406467\n",
      "iteration number =  1628 Cross Entropy Loss= 84.04160532432893\n",
      "iteration number =  1629 Cross Entropy Loss= 84.02502618467821\n",
      "iteration number =  1630 Cross Entropy Loss= 84.00846369613171\n",
      "iteration number =  1631 Cross Entropy Loss= 83.99191783318086\n",
      "iteration number =  1632 Cross Entropy Loss= 83.97538857037094\n",
      "iteration number =  1633 Cross Entropy Loss= 83.95887588230121\n",
      "iteration number =  1634 Cross Entropy Loss= 83.94237974362457\n",
      "iteration number =  1635 Cross Entropy Loss= 83.92590012904756\n",
      "iteration number =  1636 Cross Entropy Loss= 83.90943701333005\n",
      "iteration number =  1637 Cross Entropy Loss= 83.89299037128525\n",
      "iteration number =  1638 Cross Entropy Loss= 83.87656017777951\n",
      "iteration number =  1639 Cross Entropy Loss= 83.86014640773217\n",
      "iteration number =  1640 Cross Entropy Loss= 83.84374903611544\n",
      "iteration number =  1641 Cross Entropy Loss= 83.82736803795422\n",
      "iteration number =  1642 Cross Entropy Loss= 83.81100338832596\n",
      "iteration number =  1643 Cross Entropy Loss= 83.79465506236063\n",
      "iteration number =  1644 Cross Entropy Loss= 83.77832303524042\n",
      "iteration number =  1645 Cross Entropy Loss= 83.76200728219973\n",
      "iteration number =  1646 Cross Entropy Loss= 83.74570777852493\n",
      "iteration number =  1647 Cross Entropy Loss= 83.72942449955428\n",
      "iteration number =  1648 Cross Entropy Loss= 83.71315742067783\n",
      "iteration number =  1649 Cross Entropy Loss= 83.69690651733715\n",
      "iteration number =  1650 Cross Entropy Loss= 83.68067176502537\n",
      "iteration number =  1651 Cross Entropy Loss= 83.66445313928689\n",
      "iteration number =  1652 Cross Entropy Loss= 83.6482506157173\n",
      "iteration number =  1653 Cross Entropy Loss= 83.63206416996326\n",
      "iteration number =  1654 Cross Entropy Loss= 83.61589377772242\n",
      "iteration number =  1655 Cross Entropy Loss= 83.59973941474311\n",
      "iteration number =  1656 Cross Entropy Loss= 83.5836010568244\n",
      "iteration number =  1657 Cross Entropy Loss= 83.56747867981582\n",
      "iteration number =  1658 Cross Entropy Loss= 83.55137225961735\n",
      "iteration number =  1659 Cross Entropy Loss= 83.53528177217918\n",
      "iteration number =  1660 Cross Entropy Loss= 83.51920719350161\n",
      "iteration number =  1661 Cross Entropy Loss= 83.50314849963499\n",
      "iteration number =  1662 Cross Entropy Loss= 83.48710566667945\n",
      "iteration number =  1663 Cross Entropy Loss= 83.47107867078492\n",
      "iteration number =  1664 Cross Entropy Loss= 83.4550674881509\n",
      "iteration number =  1665 Cross Entropy Loss= 83.43907209502632\n",
      "iteration number =  1666 Cross Entropy Loss= 83.42309246770952\n",
      "iteration number =  1667 Cross Entropy Loss= 83.40712858254794\n",
      "iteration number =  1668 Cross Entropy Loss= 83.39118041593818\n",
      "iteration number =  1669 Cross Entropy Loss= 83.37524794432582\n",
      "iteration number =  1670 Cross Entropy Loss= 83.35933114420511\n",
      "iteration number =  1671 Cross Entropy Loss= 83.34342999211914\n",
      "iteration number =  1672 Cross Entropy Loss= 83.3275444646595\n",
      "iteration number =  1673 Cross Entropy Loss= 83.3116745384662\n",
      "iteration number =  1674 Cross Entropy Loss= 83.29582019022763\n",
      "iteration number =  1675 Cross Entropy Loss= 83.27998139668026\n",
      "iteration number =  1676 Cross Entropy Loss= 83.26415813460869\n",
      "iteration number =  1677 Cross Entropy Loss= 83.24835038084544\n",
      "iteration number =  1678 Cross Entropy Loss= 83.23255811227084\n",
      "iteration number =  1679 Cross Entropy Loss= 83.21678130581286\n",
      "iteration number =  1680 Cross Entropy Loss= 83.20101993844706\n",
      "iteration number =  1681 Cross Entropy Loss= 83.18527398719644\n",
      "iteration number =  1682 Cross Entropy Loss= 83.1695434291313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1683 Cross Entropy Loss= 83.1538282413691\n",
      "iteration number =  1684 Cross Entropy Loss= 83.13812840107438\n",
      "iteration number =  1685 Cross Entropy Loss= 83.12244388545865\n",
      "iteration number =  1686 Cross Entropy Loss= 83.10677467178019\n",
      "iteration number =  1687 Cross Entropy Loss= 83.09112073734401\n",
      "iteration number =  1688 Cross Entropy Loss= 83.07548205950167\n",
      "iteration number =  1689 Cross Entropy Loss= 83.05985861565122\n",
      "iteration number =  1690 Cross Entropy Loss= 83.04425038323697\n",
      "iteration number =  1691 Cross Entropy Loss= 83.02865733974953\n",
      "iteration number =  1692 Cross Entropy Loss= 83.01307946272554\n",
      "iteration number =  1693 Cross Entropy Loss= 82.99751672974764\n",
      "iteration number =  1694 Cross Entropy Loss= 82.98196911844435\n",
      "iteration number =  1695 Cross Entropy Loss= 82.96643660648985\n",
      "iteration number =  1696 Cross Entropy Loss= 82.95091917160403\n",
      "iteration number =  1697 Cross Entropy Loss= 82.9354167915522\n",
      "iteration number =  1698 Cross Entropy Loss= 82.91992944414507\n",
      "iteration number =  1699 Cross Entropy Loss= 82.90445710723868\n",
      "iteration number =  1700 Cross Entropy Loss= 82.88899975873412\n",
      "iteration number =  1701 Cross Entropy Loss= 82.87355737657758\n",
      "iteration number =  1702 Cross Entropy Loss= 82.85812993876013\n",
      "iteration number =  1703 Cross Entropy Loss= 82.84271742331765\n",
      "iteration number =  1704 Cross Entropy Loss= 82.82731980833071\n",
      "iteration number =  1705 Cross Entropy Loss= 82.81193707192443\n",
      "iteration number =  1706 Cross Entropy Loss= 82.79656919226841\n",
      "iteration number =  1707 Cross Entropy Loss= 82.78121614757654\n",
      "iteration number =  1708 Cross Entropy Loss= 82.765877916107\n",
      "iteration number =  1709 Cross Entropy Loss= 82.75055447616205\n",
      "iteration number =  1710 Cross Entropy Loss= 82.73524580608793\n",
      "iteration number =  1711 Cross Entropy Loss= 82.71995188427479\n",
      "iteration number =  1712 Cross Entropy Loss= 82.70467268915655\n",
      "iteration number =  1713 Cross Entropy Loss= 82.68940819921079\n",
      "iteration number =  1714 Cross Entropy Loss= 82.67415839295862\n",
      "iteration number =  1715 Cross Entropy Loss= 82.6589232489646\n",
      "iteration number =  1716 Cross Entropy Loss= 82.64370274583663\n",
      "iteration number =  1717 Cross Entropy Loss= 82.62849686222579\n",
      "iteration number =  1718 Cross Entropy Loss= 82.61330557682629\n",
      "iteration number =  1719 Cross Entropy Loss= 82.59812886837534\n",
      "iteration number =  1720 Cross Entropy Loss= 82.58296671565303\n",
      "iteration number =  1721 Cross Entropy Loss= 82.56781909748219\n",
      "iteration number =  1722 Cross Entropy Loss= 82.55268599272833\n",
      "iteration number =  1723 Cross Entropy Loss= 82.53756738029958\n",
      "iteration number =  1724 Cross Entropy Loss= 82.52246323914642\n",
      "iteration number =  1725 Cross Entropy Loss= 82.50737354826173\n",
      "iteration number =  1726 Cross Entropy Loss= 82.4922982866806\n",
      "iteration number =  1727 Cross Entropy Loss= 82.47723743348024\n",
      "iteration number =  1728 Cross Entropy Loss= 82.46219096777993\n",
      "iteration number =  1729 Cross Entropy Loss= 82.44715886874079\n",
      "iteration number =  1730 Cross Entropy Loss= 82.43214111556574\n",
      "iteration number =  1731 Cross Entropy Loss= 82.41713768749949\n",
      "iteration number =  1732 Cross Entropy Loss= 82.40214856382825\n",
      "iteration number =  1733 Cross Entropy Loss= 82.38717372387973\n",
      "iteration number =  1734 Cross Entropy Loss= 82.37221314702306\n",
      "iteration number =  1735 Cross Entropy Loss= 82.3572668126686\n",
      "iteration number =  1736 Cross Entropy Loss= 82.34233470026794\n",
      "iteration number =  1737 Cross Entropy Loss= 82.32741678931366\n",
      "iteration number =  1738 Cross Entropy Loss= 82.31251305933935\n",
      "iteration number =  1739 Cross Entropy Loss= 82.29762348991949\n",
      "iteration number =  1740 Cross Entropy Loss= 82.28274806066923\n",
      "iteration number =  1741 Cross Entropy Loss= 82.26788675124445\n",
      "iteration number =  1742 Cross Entropy Loss= 82.25303954134154\n",
      "iteration number =  1743 Cross Entropy Loss= 82.23820641069734\n",
      "iteration number =  1744 Cross Entropy Loss= 82.22338733908906\n",
      "iteration number =  1745 Cross Entropy Loss= 82.20858230633412\n",
      "iteration number =  1746 Cross Entropy Loss= 82.19379129229014\n",
      "iteration number =  1747 Cross Entropy Loss= 82.1790142768547\n",
      "iteration number =  1748 Cross Entropy Loss= 82.16425123996538\n",
      "iteration number =  1749 Cross Entropy Loss= 82.14950216159957\n",
      "iteration number =  1750 Cross Entropy Loss= 82.13476702177445\n",
      "iteration number =  1751 Cross Entropy Loss= 82.12004580054676\n",
      "iteration number =  1752 Cross Entropy Loss= 82.10533847801284\n",
      "iteration number =  1753 Cross Entropy Loss= 82.09064503430844\n",
      "iteration number =  1754 Cross Entropy Loss= 82.07596544960867\n",
      "iteration number =  1755 Cross Entropy Loss= 82.06129970412792\n",
      "iteration number =  1756 Cross Entropy Loss= 82.04664777811962\n",
      "iteration number =  1757 Cross Entropy Loss= 82.03200965187635\n",
      "iteration number =  1758 Cross Entropy Loss= 82.01738530572958\n",
      "iteration number =  1759 Cross Entropy Loss= 82.00277472004966\n",
      "iteration number =  1760 Cross Entropy Loss= 81.98817787524567\n",
      "iteration number =  1761 Cross Entropy Loss= 81.97359475176538\n",
      "iteration number =  1762 Cross Entropy Loss= 81.95902533009507\n",
      "iteration number =  1763 Cross Entropy Loss= 81.94446959075955\n",
      "iteration number =  1764 Cross Entropy Loss= 81.92992751432193\n",
      "iteration number =  1765 Cross Entropy Loss= 81.91539908138367\n",
      "iteration number =  1766 Cross Entropy Loss= 81.9008842725843\n",
      "iteration number =  1767 Cross Entropy Loss= 81.88638306860153\n",
      "iteration number =  1768 Cross Entropy Loss= 81.87189545015104\n",
      "iteration number =  1769 Cross Entropy Loss= 81.85742139798634\n",
      "iteration number =  1770 Cross Entropy Loss= 81.8429608928988\n",
      "iteration number =  1771 Cross Entropy Loss= 81.82851391571747\n",
      "iteration number =  1772 Cross Entropy Loss= 81.81408044730901\n",
      "iteration number =  1773 Cross Entropy Loss= 81.7996604685776\n",
      "iteration number =  1774 Cross Entropy Loss= 81.78525396046484\n",
      "iteration number =  1775 Cross Entropy Loss= 81.77086090394967\n",
      "iteration number =  1776 Cross Entropy Loss= 81.75648128004823\n",
      "iteration number =  1777 Cross Entropy Loss= 81.74211506981388\n",
      "iteration number =  1778 Cross Entropy Loss= 81.72776225433694\n",
      "iteration number =  1779 Cross Entropy Loss= 81.71342281474477\n",
      "iteration number =  1780 Cross Entropy Loss= 81.69909673220158\n",
      "iteration number =  1781 Cross Entropy Loss= 81.6847839879083\n",
      "iteration number =  1782 Cross Entropy Loss= 81.67048456310262\n",
      "iteration number =  1783 Cross Entropy Loss= 81.65619843905878\n",
      "iteration number =  1784 Cross Entropy Loss= 81.64192559708759\n",
      "iteration number =  1785 Cross Entropy Loss= 81.62766601853619\n",
      "iteration number =  1786 Cross Entropy Loss= 81.6134196847881\n",
      "iteration number =  1787 Cross Entropy Loss= 81.59918657726305\n",
      "iteration number =  1788 Cross Entropy Loss= 81.58496667741696\n",
      "iteration number =  1789 Cross Entropy Loss= 81.57075996674172\n",
      "iteration number =  1790 Cross Entropy Loss= 81.55656642676531\n",
      "iteration number =  1791 Cross Entropy Loss= 81.54238603905151\n",
      "iteration number =  1792 Cross Entropy Loss= 81.52821878519988\n",
      "iteration number =  1793 Cross Entropy Loss= 81.51406464684572\n",
      "iteration number =  1794 Cross Entropy Loss= 81.49992360565997\n",
      "iteration number =  1795 Cross Entropy Loss= 81.48579564334905\n",
      "iteration number =  1796 Cross Entropy Loss= 81.47168074165481\n",
      "iteration number =  1797 Cross Entropy Loss= 81.4575788823545\n",
      "iteration number =  1798 Cross Entropy Loss= 81.44349004726062\n",
      "iteration number =  1799 Cross Entropy Loss= 81.42941421822087\n",
      "iteration number =  1800 Cross Entropy Loss= 81.41535137711799\n",
      "iteration number =  1801 Cross Entropy Loss= 81.40130150586977\n",
      "iteration number =  1802 Cross Entropy Loss= 81.38726458642893\n",
      "iteration number =  1803 Cross Entropy Loss= 81.37324060078299\n",
      "iteration number =  1804 Cross Entropy Loss= 81.35922953095427\n",
      "iteration number =  1805 Cross Entropy Loss= 81.34523135899968\n",
      "iteration number =  1806 Cross Entropy Loss= 81.33124606701084\n",
      "iteration number =  1807 Cross Entropy Loss= 81.31727363711373\n",
      "iteration number =  1808 Cross Entropy Loss= 81.30331405146882\n",
      "iteration number =  1809 Cross Entropy Loss= 81.2893672922709\n",
      "iteration number =  1810 Cross Entropy Loss= 81.27543334174901\n",
      "iteration number =  1811 Cross Entropy Loss= 81.2615121821663\n",
      "iteration number =  1812 Cross Entropy Loss= 81.2476037958201\n",
      "iteration number =  1813 Cross Entropy Loss= 81.2337081650416\n",
      "iteration number =  1814 Cross Entropy Loss= 81.21982527219606\n",
      "iteration number =  1815 Cross Entropy Loss= 81.20595509968243\n",
      "iteration number =  1816 Cross Entropy Loss= 81.1920976299335\n",
      "iteration number =  1817 Cross Entropy Loss= 81.17825284541567\n",
      "iteration number =  1818 Cross Entropy Loss= 81.16442072862895\n",
      "iteration number =  1819 Cross Entropy Loss= 81.15060126210689\n",
      "iteration number =  1820 Cross Entropy Loss= 81.13679442841638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1821 Cross Entropy Loss= 81.12300021015773\n",
      "iteration number =  1822 Cross Entropy Loss= 81.10921858996443\n",
      "iteration number =  1823 Cross Entropy Loss= 81.0954495505032\n",
      "iteration number =  1824 Cross Entropy Loss= 81.08169307447389\n",
      "iteration number =  1825 Cross Entropy Loss= 81.06794914460929\n",
      "iteration number =  1826 Cross Entropy Loss= 81.05421774367518\n",
      "iteration number =  1827 Cross Entropy Loss= 81.04049885447017\n",
      "iteration number =  1828 Cross Entropy Loss= 81.0267924598257\n",
      "iteration number =  1829 Cross Entropy Loss= 81.01309854260586\n",
      "iteration number =  1830 Cross Entropy Loss= 80.99941708570736\n",
      "iteration number =  1831 Cross Entropy Loss= 80.98574807205947\n",
      "iteration number =  1832 Cross Entropy Loss= 80.97209148462395\n",
      "iteration number =  1833 Cross Entropy Loss= 80.9584473063949\n",
      "iteration number =  1834 Cross Entropy Loss= 80.94481552039875\n",
      "iteration number =  1835 Cross Entropy Loss= 80.93119610969416\n",
      "iteration number =  1836 Cross Entropy Loss= 80.91758905737193\n",
      "iteration number =  1837 Cross Entropy Loss= 80.90399434655495\n",
      "iteration number =  1838 Cross Entropy Loss= 80.89041196039807\n",
      "iteration number =  1839 Cross Entropy Loss= 80.87684188208814\n",
      "iteration number =  1840 Cross Entropy Loss= 80.86328409484378\n",
      "iteration number =  1841 Cross Entropy Loss= 80.8497385819154\n",
      "iteration number =  1842 Cross Entropy Loss= 80.83620532658512\n",
      "iteration number =  1843 Cross Entropy Loss= 80.82268431216661\n",
      "iteration number =  1844 Cross Entropy Loss= 80.80917552200519\n",
      "iteration number =  1845 Cross Entropy Loss= 80.79567893947754\n",
      "iteration number =  1846 Cross Entropy Loss= 80.78219454799179\n",
      "iteration number =  1847 Cross Entropy Loss= 80.76872233098734\n",
      "iteration number =  1848 Cross Entropy Loss= 80.75526227193487\n",
      "iteration number =  1849 Cross Entropy Loss= 80.74181435433619\n",
      "iteration number =  1850 Cross Entropy Loss= 80.72837856172424\n",
      "iteration number =  1851 Cross Entropy Loss= 80.7149548776629\n",
      "iteration number =  1852 Cross Entropy Loss= 80.70154328574708\n",
      "iteration number =  1853 Cross Entropy Loss= 80.68814376960249\n",
      "iteration number =  1854 Cross Entropy Loss= 80.67475631288566\n",
      "iteration number =  1855 Cross Entropy Loss= 80.66138089928384\n",
      "iteration number =  1856 Cross Entropy Loss= 80.64801751251493\n",
      "iteration number =  1857 Cross Entropy Loss= 80.63466613632737\n",
      "iteration number =  1858 Cross Entropy Loss= 80.62132675450015\n",
      "iteration number =  1859 Cross Entropy Loss= 80.60799935084265\n",
      "iteration number =  1860 Cross Entropy Loss= 80.59468390919461\n",
      "iteration number =  1861 Cross Entropy Loss= 80.58138041342609\n",
      "iteration number =  1862 Cross Entropy Loss= 80.56808884743731\n",
      "iteration number =  1863 Cross Entropy Loss= 80.55480919515865\n",
      "iteration number =  1864 Cross Entropy Loss= 80.54154144055057\n",
      "iteration number =  1865 Cross Entropy Loss= 80.5282855676035\n",
      "iteration number =  1866 Cross Entropy Loss= 80.51504156033783\n",
      "iteration number =  1867 Cross Entropy Loss= 80.50180940280377\n",
      "iteration number =  1868 Cross Entropy Loss= 80.48858907908135\n",
      "iteration number =  1869 Cross Entropy Loss= 80.47538057328023\n",
      "iteration number =  1870 Cross Entropy Loss= 80.46218386953986\n",
      "iteration number =  1871 Cross Entropy Loss= 80.44899895202911\n",
      "iteration number =  1872 Cross Entropy Loss= 80.43582580494643\n",
      "iteration number =  1873 Cross Entropy Loss= 80.42266441251972\n",
      "iteration number =  1874 Cross Entropy Loss= 80.40951475900617\n",
      "iteration number =  1875 Cross Entropy Loss= 80.39637682869235\n",
      "iteration number =  1876 Cross Entropy Loss= 80.38325060589399\n",
      "iteration number =  1877 Cross Entropy Loss= 80.370136074956\n",
      "iteration number =  1878 Cross Entropy Loss= 80.35703322025236\n",
      "iteration number =  1879 Cross Entropy Loss= 80.34394202618608\n",
      "iteration number =  1880 Cross Entropy Loss= 80.33086247718916\n",
      "iteration number =  1881 Cross Entropy Loss= 80.31779455772241\n",
      "iteration number =  1882 Cross Entropy Loss= 80.30473825227548\n",
      "iteration number =  1883 Cross Entropy Loss= 80.29169354536681\n",
      "iteration number =  1884 Cross Entropy Loss= 80.27866042154344\n",
      "iteration number =  1885 Cross Entropy Loss= 80.26563886538108\n",
      "iteration number =  1886 Cross Entropy Loss= 80.25262886148397\n",
      "iteration number =  1887 Cross Entropy Loss= 80.2396303944848\n",
      "iteration number =  1888 Cross Entropy Loss= 80.22664344904473\n",
      "iteration number =  1889 Cross Entropy Loss= 80.21366800985318\n",
      "iteration number =  1890 Cross Entropy Loss= 80.20070406162793\n",
      "iteration number =  1891 Cross Entropy Loss= 80.18775158911488\n",
      "iteration number =  1892 Cross Entropy Loss= 80.17481057708818\n",
      "iteration number =  1893 Cross Entropy Loss= 80.16188101034999\n",
      "iteration number =  1894 Cross Entropy Loss= 80.14896287373045\n",
      "iteration number =  1895 Cross Entropy Loss= 80.13605615208776\n",
      "iteration number =  1896 Cross Entropy Loss= 80.1231608303079\n",
      "iteration number =  1897 Cross Entropy Loss= 80.1102768933047\n",
      "iteration number =  1898 Cross Entropy Loss= 80.09740432601974\n",
      "iteration number =  1899 Cross Entropy Loss= 80.0845431134223\n",
      "iteration number =  1900 Cross Entropy Loss= 80.07169324050928\n",
      "iteration number =  1901 Cross Entropy Loss= 80.0588546923051\n",
      "iteration number =  1902 Cross Entropy Loss= 80.04602745386174\n",
      "iteration number =  1903 Cross Entropy Loss= 80.03321151025854\n",
      "iteration number =  1904 Cross Entropy Loss= 80.02040684660227\n",
      "iteration number =  1905 Cross Entropy Loss= 80.00761344802692\n",
      "iteration number =  1906 Cross Entropy Loss= 79.99483129969383\n",
      "iteration number =  1907 Cross Entropy Loss= 79.98206038679137\n",
      "iteration number =  1908 Cross Entropy Loss= 79.9693006945352\n",
      "iteration number =  1909 Cross Entropy Loss= 79.95655220816788\n",
      "iteration number =  1910 Cross Entropy Loss= 79.943814912959\n",
      "iteration number =  1911 Cross Entropy Loss= 79.93108879420511\n",
      "iteration number =  1912 Cross Entropy Loss= 79.91837383722958\n",
      "iteration number =  1913 Cross Entropy Loss= 79.90567002738258\n",
      "iteration number =  1914 Cross Entropy Loss= 79.89297735004106\n",
      "iteration number =  1915 Cross Entropy Loss= 79.88029579060859\n",
      "iteration number =  1916 Cross Entropy Loss= 79.86762533451537\n",
      "iteration number =  1917 Cross Entropy Loss= 79.85496596721816\n",
      "iteration number =  1918 Cross Entropy Loss= 79.84231767420022\n",
      "iteration number =  1919 Cross Entropy Loss= 79.8296804409712\n",
      "iteration number =  1920 Cross Entropy Loss= 79.81705425306717\n",
      "iteration number =  1921 Cross Entropy Loss= 79.80443909605046\n",
      "iteration number =  1922 Cross Entropy Loss= 79.79183495550967\n",
      "iteration number =  1923 Cross Entropy Loss= 79.77924181705959\n",
      "iteration number =  1924 Cross Entropy Loss= 79.76665966634108\n",
      "iteration number =  1925 Cross Entropy Loss= 79.75408848902114\n",
      "iteration number =  1926 Cross Entropy Loss= 79.74152827079274\n",
      "iteration number =  1927 Cross Entropy Loss= 79.72897899737481\n",
      "iteration number =  1928 Cross Entropy Loss= 79.71644065451213\n",
      "iteration number =  1929 Cross Entropy Loss= 79.70391322797532\n",
      "iteration number =  1930 Cross Entropy Loss= 79.69139670356078\n",
      "iteration number =  1931 Cross Entropy Loss= 79.67889106709063\n",
      "iteration number =  1932 Cross Entropy Loss= 79.6663963044126\n",
      "iteration number =  1933 Cross Entropy Loss= 79.6539124014\n",
      "iteration number =  1934 Cross Entropy Loss= 79.64143934395173\n",
      "iteration number =  1935 Cross Entropy Loss= 79.62897711799212\n",
      "iteration number =  1936 Cross Entropy Loss= 79.6165257094709\n",
      "iteration number =  1937 Cross Entropy Loss= 79.60408510436321\n",
      "iteration number =  1938 Cross Entropy Loss= 79.59165528866939\n",
      "iteration number =  1939 Cross Entropy Loss= 79.57923624841513\n",
      "iteration number =  1940 Cross Entropy Loss= 79.5668279696512\n",
      "iteration number =  1941 Cross Entropy Loss= 79.55443043845362\n",
      "iteration number =  1942 Cross Entropy Loss= 79.54204364092331\n",
      "iteration number =  1943 Cross Entropy Loss= 79.52966756318634\n",
      "iteration number =  1944 Cross Entropy Loss= 79.51730219139364\n",
      "iteration number =  1945 Cross Entropy Loss= 79.50494751172108\n",
      "iteration number =  1946 Cross Entropy Loss= 79.49260351036939\n",
      "iteration number =  1947 Cross Entropy Loss= 79.48027017356401\n",
      "iteration number =  1948 Cross Entropy Loss= 79.46794748755516\n",
      "iteration number =  1949 Cross Entropy Loss= 79.45563543861772\n",
      "iteration number =  1950 Cross Entropy Loss= 79.44333401305116\n",
      "iteration number =  1951 Cross Entropy Loss= 79.43104319717949\n",
      "iteration number =  1952 Cross Entropy Loss= 79.4187629773513\n",
      "iteration number =  1953 Cross Entropy Loss= 79.40649333993954\n",
      "iteration number =  1954 Cross Entropy Loss= 79.39423427134159\n",
      "iteration number =  1955 Cross Entropy Loss= 79.38198575797914\n",
      "iteration number =  1956 Cross Entropy Loss= 79.36974778629816\n",
      "iteration number =  1957 Cross Entropy Loss= 79.35752034276888\n",
      "iteration number =  1958 Cross Entropy Loss= 79.34530341388567\n",
      "iteration number =  1959 Cross Entropy Loss= 79.33309698616698\n",
      "iteration number =  1960 Cross Entropy Loss= 79.32090104615538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  1961 Cross Entropy Loss= 79.30871558041741\n",
      "iteration number =  1962 Cross Entropy Loss= 79.29654057554353\n",
      "iteration number =  1963 Cross Entropy Loss= 79.28437601814818\n",
      "iteration number =  1964 Cross Entropy Loss= 79.27222189486956\n",
      "iteration number =  1965 Cross Entropy Loss= 79.26007819236968\n",
      "iteration number =  1966 Cross Entropy Loss= 79.24794489733429\n",
      "iteration number =  1967 Cross Entropy Loss= 79.23582199647282\n",
      "iteration number =  1968 Cross Entropy Loss= 79.22370947651834\n",
      "iteration number =  1969 Cross Entropy Loss= 79.21160732422743\n",
      "iteration number =  1970 Cross Entropy Loss= 79.19951552638028\n",
      "iteration number =  1971 Cross Entropy Loss= 79.1874340697805\n",
      "iteration number =  1972 Cross Entropy Loss= 79.17536294125506\n",
      "iteration number =  1973 Cross Entropy Loss= 79.16330212765442\n",
      "iteration number =  1974 Cross Entropy Loss= 79.15125161585222\n",
      "iteration number =  1975 Cross Entropy Loss= 79.13921139274542\n",
      "iteration number =  1976 Cross Entropy Loss= 79.12718144525417\n",
      "iteration number =  1977 Cross Entropy Loss= 79.11516176032178\n",
      "iteration number =  1978 Cross Entropy Loss= 79.10315232491467\n",
      "iteration number =  1979 Cross Entropy Loss= 79.09115312602222\n",
      "iteration number =  1980 Cross Entropy Loss= 79.07916415065696\n",
      "iteration number =  1981 Cross Entropy Loss= 79.06718538585422\n",
      "iteration number =  1982 Cross Entropy Loss= 79.05521681867228\n",
      "iteration number =  1983 Cross Entropy Loss= 79.04325843619227\n",
      "iteration number =  1984 Cross Entropy Loss= 79.03131022551811\n",
      "iteration number =  1985 Cross Entropy Loss= 79.01937217377645\n",
      "iteration number =  1986 Cross Entropy Loss= 79.00744426811659\n",
      "iteration number =  1987 Cross Entropy Loss= 78.9955264957105\n",
      "iteration number =  1988 Cross Entropy Loss= 78.98361884375278\n",
      "iteration number =  1989 Cross Entropy Loss= 78.97172129946048\n",
      "iteration number =  1990 Cross Entropy Loss= 78.95983385007321\n",
      "iteration number =  1991 Cross Entropy Loss= 78.94795648285296\n",
      "iteration number =  1992 Cross Entropy Loss= 78.93608918508414\n",
      "iteration number =  1993 Cross Entropy Loss= 78.92423194407345\n",
      "iteration number =  1994 Cross Entropy Loss= 78.91238474714993\n",
      "iteration number =  1995 Cross Entropy Loss= 78.90054758166484\n",
      "iteration number =  1996 Cross Entropy Loss= 78.8887204349916\n",
      "iteration number =  1997 Cross Entropy Loss= 78.87690329452577\n",
      "iteration number =  1998 Cross Entropy Loss= 78.86509614768505\n",
      "iteration number =  1999 Cross Entropy Loss= 78.85329898190909\n",
      "iteration number =  2000 Cross Entropy Loss= 78.8415117846596\n",
      "iteration number =  2001 Cross Entropy Loss= 78.82973454342019\n",
      "iteration number =  2002 Cross Entropy Loss= 78.8179672456964\n",
      "iteration number =  2003 Cross Entropy Loss= 78.80620987901555\n",
      "iteration number =  2004 Cross Entropy Loss= 78.79446243092684\n",
      "iteration number =  2005 Cross Entropy Loss= 78.78272488900112\n",
      "iteration number =  2006 Cross Entropy Loss= 78.77099724083101\n",
      "iteration number =  2007 Cross Entropy Loss= 78.75927947403073\n",
      "iteration number =  2008 Cross Entropy Loss= 78.74757157623613\n",
      "iteration number =  2009 Cross Entropy Loss= 78.73587353510462\n",
      "iteration number =  2010 Cross Entropy Loss= 78.7241853383151\n",
      "iteration number =  2011 Cross Entropy Loss= 78.7125069735679\n",
      "iteration number =  2012 Cross Entropy Loss= 78.7008384285848\n",
      "iteration number =  2013 Cross Entropy Loss= 78.68917969110896\n",
      "iteration number =  2014 Cross Entropy Loss= 78.67753074890479\n",
      "iteration number =  2015 Cross Entropy Loss= 78.66589158975802\n",
      "iteration number =  2016 Cross Entropy Loss= 78.65426220147558\n",
      "iteration number =  2017 Cross Entropy Loss= 78.64264257188557\n",
      "iteration number =  2018 Cross Entropy Loss= 78.63103268883728\n",
      "iteration number =  2019 Cross Entropy Loss= 78.61943254020096\n",
      "iteration number =  2020 Cross Entropy Loss= 78.607842113868\n",
      "iteration number =  2021 Cross Entropy Loss= 78.59626139775074\n",
      "iteration number =  2022 Cross Entropy Loss= 78.58469037978246\n",
      "iteration number =  2023 Cross Entropy Loss= 78.57312904791733\n",
      "iteration number =  2024 Cross Entropy Loss= 78.5615773901304\n",
      "iteration number =  2025 Cross Entropy Loss= 78.55003539441748\n",
      "iteration number =  2026 Cross Entropy Loss= 78.53850304879519\n",
      "iteration number =  2027 Cross Entropy Loss= 78.52698034130081\n",
      "iteration number =  2028 Cross Entropy Loss= 78.51546725999233\n",
      "iteration number =  2029 Cross Entropy Loss= 78.50396379294833\n",
      "iteration number =  2030 Cross Entropy Loss= 78.49246992826797\n",
      "iteration number =  2031 Cross Entropy Loss= 78.48098565407098\n",
      "iteration number =  2032 Cross Entropy Loss= 78.46951095849752\n",
      "iteration number =  2033 Cross Entropy Loss= 78.45804582970825\n",
      "iteration number =  2034 Cross Entropy Loss= 78.44659025588416\n",
      "iteration number =  2035 Cross Entropy Loss= 78.43514422522665\n",
      "iteration number =  2036 Cross Entropy Loss= 78.42370772595739\n",
      "iteration number =  2037 Cross Entropy Loss= 78.41228074631834\n",
      "iteration number =  2038 Cross Entropy Loss= 78.40086327457166\n",
      "iteration number =  2039 Cross Entropy Loss= 78.3894552989997\n",
      "iteration number =  2040 Cross Entropy Loss= 78.37805680790491\n",
      "iteration number =  2041 Cross Entropy Loss= 78.36666778960986\n",
      "iteration number =  2042 Cross Entropy Loss= 78.35528823245716\n",
      "iteration number =  2043 Cross Entropy Loss= 78.3439181248094\n",
      "iteration number =  2044 Cross Entropy Loss= 78.33255745504911\n",
      "iteration number =  2045 Cross Entropy Loss= 78.32120621157878\n",
      "iteration number =  2046 Cross Entropy Loss= 78.30986438282076\n",
      "iteration number =  2047 Cross Entropy Loss= 78.29853195721716\n",
      "iteration number =  2048 Cross Entropy Loss= 78.28720892322997\n",
      "iteration number =  2049 Cross Entropy Loss= 78.27589526934082\n",
      "iteration number =  2050 Cross Entropy Loss= 78.26459098405113\n",
      "iteration number =  2051 Cross Entropy Loss= 78.25329605588189\n",
      "iteration number =  2052 Cross Entropy Loss= 78.24201047337377\n",
      "iteration number =  2053 Cross Entropy Loss= 78.23073422508693\n",
      "iteration number =  2054 Cross Entropy Loss= 78.21946729960116\n",
      "iteration number =  2055 Cross Entropy Loss= 78.20820968551563\n",
      "iteration number =  2056 Cross Entropy Loss= 78.196961371449\n",
      "iteration number =  2057 Cross Entropy Loss= 78.18572234603933\n",
      "iteration number =  2058 Cross Entropy Loss= 78.17449259794402\n",
      "iteration number =  2059 Cross Entropy Loss= 78.16327211583982\n",
      "iteration number =  2060 Cross Entropy Loss= 78.15206088842268\n",
      "iteration number =  2061 Cross Entropy Loss= 78.14085890440785\n",
      "iteration number =  2062 Cross Entropy Loss= 78.12966615252977\n",
      "iteration number =  2063 Cross Entropy Loss= 78.11848262154194\n",
      "iteration number =  2064 Cross Entropy Loss= 78.10730830021708\n",
      "iteration number =  2065 Cross Entropy Loss= 78.09614317734692\n",
      "iteration number =  2066 Cross Entropy Loss= 78.0849872417422\n",
      "iteration number =  2067 Cross Entropy Loss= 78.07384048223268\n",
      "iteration number =  2068 Cross Entropy Loss= 78.06270288766702\n",
      "iteration number =  2069 Cross Entropy Loss= 78.05157444691281\n",
      "iteration number =  2070 Cross Entropy Loss= 78.04045514885651\n",
      "iteration number =  2071 Cross Entropy Loss= 78.02934498240339\n",
      "iteration number =  2072 Cross Entropy Loss= 78.01824393647746\n",
      "iteration number =  2073 Cross Entropy Loss= 78.0071520000215\n",
      "iteration number =  2074 Cross Entropy Loss= 77.99606916199701\n",
      "iteration number =  2075 Cross Entropy Loss= 77.98499541138415\n",
      "iteration number =  2076 Cross Entropy Loss= 77.97393073718165\n",
      "iteration number =  2077 Cross Entropy Loss= 77.96287512840682\n",
      "iteration number =  2078 Cross Entropy Loss= 77.95182857409557\n",
      "iteration number =  2079 Cross Entropy Loss= 77.94079106330227\n",
      "iteration number =  2080 Cross Entropy Loss= 77.9297625850997\n",
      "iteration number =  2081 Cross Entropy Loss= 77.91874312857917\n",
      "iteration number =  2082 Cross Entropy Loss= 77.90773268285027\n",
      "iteration number =  2083 Cross Entropy Loss= 77.89673123704097\n",
      "iteration number =  2084 Cross Entropy Loss= 77.88573878029756\n",
      "iteration number =  2085 Cross Entropy Loss= 77.87475530178455\n",
      "iteration number =  2086 Cross Entropy Loss= 77.86378079068469\n",
      "iteration number =  2087 Cross Entropy Loss= 77.85281523619892\n",
      "iteration number =  2088 Cross Entropy Loss= 77.84185862754634\n",
      "iteration number =  2089 Cross Entropy Loss= 77.83091095396412\n",
      "iteration number =  2090 Cross Entropy Loss= 77.8199722047075\n",
      "iteration number =  2091 Cross Entropy Loss= 77.80904236904976\n",
      "iteration number =  2092 Cross Entropy Loss= 77.79812143628222\n",
      "iteration number =  2093 Cross Entropy Loss= 77.78720939571406\n",
      "iteration number =  2094 Cross Entropy Loss= 77.77630623667243\n",
      "iteration number =  2095 Cross Entropy Loss= 77.76541194850233\n",
      "iteration number =  2096 Cross Entropy Loss= 77.75452652056664\n",
      "iteration number =  2097 Cross Entropy Loss= 77.74364994224597\n",
      "iteration number =  2098 Cross Entropy Loss= 77.73278220293876\n",
      "iteration number =  2099 Cross Entropy Loss= 77.72192329206115\n",
      "iteration number =  2100 Cross Entropy Loss= 77.71107319904692\n",
      "iteration number =  2101 Cross Entropy Loss= 77.70023191334757\n",
      "iteration number =  2102 Cross Entropy Loss= 77.68939942443218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2103 Cross Entropy Loss= 77.67857572178738\n",
      "iteration number =  2104 Cross Entropy Loss= 77.66776079491737\n",
      "iteration number =  2105 Cross Entropy Loss= 77.65695463334384\n",
      "iteration number =  2106 Cross Entropy Loss= 77.64615722660595\n",
      "iteration number =  2107 Cross Entropy Loss= 77.63536856426026\n",
      "iteration number =  2108 Cross Entropy Loss= 77.62458863588071\n",
      "iteration number =  2109 Cross Entropy Loss= 77.61381743105864\n",
      "iteration number =  2110 Cross Entropy Loss= 77.60305493940267\n",
      "iteration number =  2111 Cross Entropy Loss= 77.59230115053869\n",
      "iteration number =  2112 Cross Entropy Loss= 77.58155605410985\n",
      "iteration number =  2113 Cross Entropy Loss= 77.5708196397765\n",
      "iteration number =  2114 Cross Entropy Loss= 77.56009189721618\n",
      "iteration number =  2115 Cross Entropy Loss= 77.5493728161235\n",
      "iteration number =  2116 Cross Entropy Loss= 77.5386623862102\n",
      "iteration number =  2117 Cross Entropy Loss= 77.52796059720514\n",
      "iteration number =  2118 Cross Entropy Loss= 77.51726743885409\n",
      "iteration number =  2119 Cross Entropy Loss= 77.50658290091988\n",
      "iteration number =  2120 Cross Entropy Loss= 77.49590697318229\n",
      "iteration number =  2121 Cross Entropy Loss= 77.48523964543799\n",
      "iteration number =  2122 Cross Entropy Loss= 77.47458090750055\n",
      "iteration number =  2123 Cross Entropy Loss= 77.46393074920037\n",
      "iteration number =  2124 Cross Entropy Loss= 77.45328916038468\n",
      "iteration number =  2125 Cross Entropy Loss= 77.44265613091746\n",
      "iteration number =  2126 Cross Entropy Loss= 77.43203165067943\n",
      "iteration number =  2127 Cross Entropy Loss= 77.42141570956805\n",
      "iteration number =  2128 Cross Entropy Loss= 77.41080829749741\n",
      "iteration number =  2129 Cross Entropy Loss= 77.40020940439823\n",
      "iteration number =  2130 Cross Entropy Loss= 77.38961902021788\n",
      "iteration number =  2131 Cross Entropy Loss= 77.37903713492022\n",
      "iteration number =  2132 Cross Entropy Loss= 77.36846373848569\n",
      "iteration number =  2133 Cross Entropy Loss= 77.35789882091123\n",
      "iteration number =  2134 Cross Entropy Loss= 77.3473423722102\n",
      "iteration number =  2135 Cross Entropy Loss= 77.33679438241242\n",
      "iteration number =  2136 Cross Entropy Loss= 77.3262548415641\n",
      "iteration number =  2137 Cross Entropy Loss= 77.31572373972774\n",
      "iteration number =  2138 Cross Entropy Loss= 77.30520106698228\n",
      "iteration number =  2139 Cross Entropy Loss= 77.29468681342286\n",
      "iteration number =  2140 Cross Entropy Loss= 77.2841809691609\n",
      "iteration number =  2141 Cross Entropy Loss= 77.27368352432404\n",
      "iteration number =  2142 Cross Entropy Loss= 77.26319446905612\n",
      "iteration number =  2143 Cross Entropy Loss= 77.25271379351712\n",
      "iteration number =  2144 Cross Entropy Loss= 77.24224148788309\n",
      "iteration number =  2145 Cross Entropy Loss= 77.23177754234628\n",
      "iteration number =  2146 Cross Entropy Loss= 77.22132194711489\n",
      "iteration number =  2147 Cross Entropy Loss= 77.21087469241319\n",
      "iteration number =  2148 Cross Entropy Loss= 77.2004357684814\n",
      "iteration number =  2149 Cross Entropy Loss= 77.19000516557571\n",
      "iteration number =  2150 Cross Entropy Loss= 77.17958287396829\n",
      "iteration number =  2151 Cross Entropy Loss= 77.16916888394704\n",
      "iteration number =  2152 Cross Entropy Loss= 77.15876318581587\n",
      "iteration number =  2153 Cross Entropy Loss= 77.14836576989444\n",
      "iteration number =  2154 Cross Entropy Loss= 77.1379766265182\n",
      "iteration number =  2155 Cross Entropy Loss= 77.12759574603835\n",
      "iteration number =  2156 Cross Entropy Loss= 77.11722311882183\n",
      "iteration number =  2157 Cross Entropy Loss= 77.10685873525125\n",
      "iteration number =  2158 Cross Entropy Loss= 77.09650258572489\n",
      "iteration number =  2159 Cross Entropy Loss= 77.08615466065666\n",
      "iteration number =  2160 Cross Entropy Loss= 77.075814950476\n",
      "iteration number =  2161 Cross Entropy Loss= 77.06548344562802\n",
      "iteration number =  2162 Cross Entropy Loss= 77.05516013657326\n",
      "iteration number =  2163 Cross Entropy Loss= 77.04484501378778\n",
      "iteration number =  2164 Cross Entropy Loss= 77.03453806776317\n",
      "iteration number =  2165 Cross Entropy Loss= 77.0242392890063\n",
      "iteration number =  2166 Cross Entropy Loss= 77.0139486680396\n",
      "iteration number =  2167 Cross Entropy Loss= 77.0036661954008\n",
      "iteration number =  2168 Cross Entropy Loss= 76.99339186164293\n",
      "iteration number =  2169 Cross Entropy Loss= 76.98312565733436\n",
      "iteration number =  2170 Cross Entropy Loss= 76.97286757305875\n",
      "iteration number =  2171 Cross Entropy Loss= 76.96261759941497\n",
      "iteration number =  2172 Cross Entropy Loss= 76.95237572701711\n",
      "iteration number =  2173 Cross Entropy Loss= 76.94214194649443\n",
      "iteration number =  2174 Cross Entropy Loss= 76.93191624849135\n",
      "iteration number =  2175 Cross Entropy Loss= 76.9216986236674\n",
      "iteration number =  2176 Cross Entropy Loss= 76.91148906269717\n",
      "iteration number =  2177 Cross Entropy Loss= 76.90128755627036\n",
      "iteration number =  2178 Cross Entropy Loss= 76.89109409509163\n",
      "iteration number =  2179 Cross Entropy Loss= 76.8809086698807\n",
      "iteration number =  2180 Cross Entropy Loss= 76.87073127137215\n",
      "iteration number =  2181 Cross Entropy Loss= 76.86056189031561\n",
      "iteration number =  2182 Cross Entropy Loss= 76.8504005174755\n",
      "iteration number =  2183 Cross Entropy Loss= 76.84024714363119\n",
      "iteration number =  2184 Cross Entropy Loss= 76.83010175957683\n",
      "iteration number =  2185 Cross Entropy Loss= 76.81996435612145\n",
      "iteration number =  2186 Cross Entropy Loss= 76.80983492408876\n",
      "iteration number =  2187 Cross Entropy Loss= 76.7997134543173\n",
      "iteration number =  2188 Cross Entropy Loss= 76.78959993766028\n",
      "iteration number =  2189 Cross Entropy Loss= 76.77949436498562\n",
      "iteration number =  2190 Cross Entropy Loss= 76.76939672717589\n",
      "iteration number =  2191 Cross Entropy Loss= 76.7593070151283\n",
      "iteration number =  2192 Cross Entropy Loss= 76.74922521975462\n",
      "iteration number =  2193 Cross Entropy Loss= 76.73915133198122\n",
      "iteration number =  2194 Cross Entropy Loss= 76.72908534274899\n",
      "iteration number =  2195 Cross Entropy Loss= 76.71902724301336\n",
      "iteration number =  2196 Cross Entropy Loss= 76.7089770237442\n",
      "iteration number =  2197 Cross Entropy Loss= 76.69893467592584\n",
      "iteration number =  2198 Cross Entropy Loss= 76.68890019055704\n",
      "iteration number =  2199 Cross Entropy Loss= 76.67887355865093\n",
      "iteration number =  2200 Cross Entropy Loss= 76.66885477123503\n",
      "iteration number =  2201 Cross Entropy Loss= 76.65884381935116\n",
      "iteration number =  2202 Cross Entropy Loss= 76.64884069405547\n",
      "iteration number =  2203 Cross Entropy Loss= 76.63884538641835\n",
      "iteration number =  2204 Cross Entropy Loss= 76.62885788752449\n",
      "iteration number =  2205 Cross Entropy Loss= 76.61887818847273\n",
      "iteration number =  2206 Cross Entropy Loss= 76.60890628037612\n",
      "iteration number =  2207 Cross Entropy Loss= 76.5989421543619\n",
      "iteration number =  2208 Cross Entropy Loss= 76.58898580157138\n",
      "iteration number =  2209 Cross Entropy Loss= 76.57903721316002\n",
      "iteration number =  2210 Cross Entropy Loss= 76.56909638029734\n",
      "iteration number =  2211 Cross Entropy Loss= 76.55916329416688\n",
      "iteration number =  2212 Cross Entropy Loss= 76.54923794596621\n",
      "iteration number =  2213 Cross Entropy Loss= 76.53932032690692\n",
      "iteration number =  2214 Cross Entropy Loss= 76.52941042821449\n",
      "iteration number =  2215 Cross Entropy Loss= 76.51950824112836\n",
      "iteration number =  2216 Cross Entropy Loss= 76.5096137569019\n",
      "iteration number =  2217 Cross Entropy Loss= 76.49972696680234\n",
      "iteration number =  2218 Cross Entropy Loss= 76.48984786211071\n",
      "iteration number =  2219 Cross Entropy Loss= 76.4799764341219\n",
      "iteration number =  2220 Cross Entropy Loss= 76.47011267414459\n",
      "iteration number =  2221 Cross Entropy Loss= 76.4602565735012\n",
      "iteration number =  2222 Cross Entropy Loss= 76.45040812352792\n",
      "iteration number =  2223 Cross Entropy Loss= 76.44056731557461\n",
      "iteration number =  2224 Cross Entropy Loss= 76.43073414100482\n",
      "iteration number =  2225 Cross Entropy Loss= 76.42090859119577\n",
      "iteration number =  2226 Cross Entropy Loss= 76.41109065753824\n",
      "iteration number =  2227 Cross Entropy Loss= 76.40128033143671\n",
      "iteration number =  2228 Cross Entropy Loss= 76.39147760430913\n",
      "iteration number =  2229 Cross Entropy Loss= 76.38168246758704\n",
      "iteration number =  2230 Cross Entropy Loss= 76.3718949127155\n",
      "iteration number =  2231 Cross Entropy Loss= 76.36211493115306\n",
      "iteration number =  2232 Cross Entropy Loss= 76.35234251437166\n",
      "iteration number =  2233 Cross Entropy Loss= 76.34257765385675\n",
      "iteration number =  2234 Cross Entropy Loss= 76.33282034110718\n",
      "iteration number =  2235 Cross Entropy Loss= 76.32307056763514\n",
      "iteration number =  2236 Cross Entropy Loss= 76.31332832496622\n",
      "iteration number =  2237 Cross Entropy Loss= 76.30359360463927\n",
      "iteration number =  2238 Cross Entropy Loss= 76.29386639820652\n",
      "iteration number =  2239 Cross Entropy Loss= 76.28414669723338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2240 Cross Entropy Loss= 76.2744344932986\n",
      "iteration number =  2241 Cross Entropy Loss= 76.26472977799408\n",
      "iteration number =  2242 Cross Entropy Loss= 76.25503254292494\n",
      "iteration number =  2243 Cross Entropy Loss= 76.24534277970947\n",
      "iteration number =  2244 Cross Entropy Loss= 76.23566047997912\n",
      "iteration number =  2245 Cross Entropy Loss= 76.22598563537836\n",
      "iteration number =  2246 Cross Entropy Loss= 76.21631823756488\n",
      "iteration number =  2247 Cross Entropy Loss= 76.20665827820932\n",
      "iteration number =  2248 Cross Entropy Loss= 76.19700574899541\n",
      "iteration number =  2249 Cross Entropy Loss= 76.1873606416199\n",
      "iteration number =  2250 Cross Entropy Loss= 76.17772294779249\n",
      "iteration number =  2251 Cross Entropy Loss= 76.16809265923587\n",
      "iteration number =  2252 Cross Entropy Loss= 76.1584697676856\n",
      "iteration number =  2253 Cross Entropy Loss= 76.14885426489022\n",
      "iteration number =  2254 Cross Entropy Loss= 76.13924614261111\n",
      "iteration number =  2255 Cross Entropy Loss= 76.12964539262252\n",
      "iteration number =  2256 Cross Entropy Loss= 76.12005200671152\n",
      "iteration number =  2257 Cross Entropy Loss= 76.11046597667799\n",
      "iteration number =  2258 Cross Entropy Loss= 76.10088729433457\n",
      "iteration number =  2259 Cross Entropy Loss= 76.09131595150669\n",
      "iteration number =  2260 Cross Entropy Loss= 76.08175194003245\n",
      "iteration number =  2261 Cross Entropy Loss= 76.07219525176271\n",
      "iteration number =  2262 Cross Entropy Loss= 76.06264587856097\n",
      "iteration number =  2263 Cross Entropy Loss= 76.05310381230342\n",
      "iteration number =  2264 Cross Entropy Loss= 76.04356904487884\n",
      "iteration number =  2265 Cross Entropy Loss= 76.0340415681886\n",
      "iteration number =  2266 Cross Entropy Loss= 76.0245213741467\n",
      "iteration number =  2267 Cross Entropy Loss= 76.01500845467964\n",
      "iteration number =  2268 Cross Entropy Loss= 76.0055028017265\n",
      "iteration number =  2269 Cross Entropy Loss= 75.9960044072388\n",
      "iteration number =  2270 Cross Entropy Loss= 75.98651326318057\n",
      "iteration number =  2271 Cross Entropy Loss= 75.97702936152834\n",
      "iteration number =  2272 Cross Entropy Loss= 75.96755269427095\n",
      "iteration number =  2273 Cross Entropy Loss= 75.95808325340975\n",
      "iteration number =  2274 Cross Entropy Loss= 75.94862103095845\n",
      "iteration number =  2275 Cross Entropy Loss= 75.93916601894307\n",
      "iteration number =  2276 Cross Entropy Loss= 75.92971820940198\n",
      "iteration number =  2277 Cross Entropy Loss= 75.92027759438591\n",
      "iteration number =  2278 Cross Entropy Loss= 75.91084416595778\n",
      "iteration number =  2279 Cross Entropy Loss= 75.90141791619283\n",
      "iteration number =  2280 Cross Entropy Loss= 75.89199883717853\n",
      "iteration number =  2281 Cross Entropy Loss= 75.88258692101455\n",
      "iteration number =  2282 Cross Entropy Loss= 75.87318215981271\n",
      "iteration number =  2283 Cross Entropy Loss= 75.86378454569706\n",
      "iteration number =  2284 Cross Entropy Loss= 75.85439407080372\n",
      "iteration number =  2285 Cross Entropy Loss= 75.845010727281\n",
      "iteration number =  2286 Cross Entropy Loss= 75.8356345072892\n",
      "iteration number =  2287 Cross Entropy Loss= 75.82626540300079\n",
      "iteration number =  2288 Cross Entropy Loss= 75.8169034066002\n",
      "iteration number =  2289 Cross Entropy Loss= 75.80754851028394\n",
      "iteration number =  2290 Cross Entropy Loss= 75.79820070626049\n",
      "iteration number =  2291 Cross Entropy Loss= 75.78885998675028\n",
      "iteration number =  2292 Cross Entropy Loss= 75.77952634398574\n",
      "iteration number =  2293 Cross Entropy Loss= 75.77019977021114\n",
      "iteration number =  2294 Cross Entropy Loss= 75.76088025768274\n",
      "iteration number =  2295 Cross Entropy Loss= 75.75156779866866\n",
      "iteration number =  2296 Cross Entropy Loss= 75.74226238544885\n",
      "iteration number =  2297 Cross Entropy Loss= 75.73296401031509\n",
      "iteration number =  2298 Cross Entropy Loss= 75.72367266557099\n",
      "iteration number =  2299 Cross Entropy Loss= 75.71438834353191\n",
      "iteration number =  2300 Cross Entropy Loss= 75.70511103652505\n",
      "iteration number =  2301 Cross Entropy Loss= 75.69584073688927\n",
      "iteration number =  2302 Cross Entropy Loss= 75.68657743697516\n",
      "iteration number =  2303 Cross Entropy Loss= 75.67732112914504\n",
      "iteration number =  2304 Cross Entropy Loss= 75.66807180577288\n",
      "iteration number =  2305 Cross Entropy Loss= 75.6588294592443\n",
      "iteration number =  2306 Cross Entropy Loss= 75.64959408195655\n",
      "iteration number =  2307 Cross Entropy Loss= 75.64036566631847\n",
      "iteration number =  2308 Cross Entropy Loss= 75.6311442047505\n",
      "iteration number =  2309 Cross Entropy Loss= 75.6219296896846\n",
      "iteration number =  2310 Cross Entropy Loss= 75.61272211356433\n",
      "iteration number =  2311 Cross Entropy Loss= 75.60352146884472\n",
      "iteration number =  2312 Cross Entropy Loss= 75.59432774799225\n",
      "iteration number =  2313 Cross Entropy Loss= 75.58514094348499\n",
      "iteration number =  2314 Cross Entropy Loss= 75.57596104781233\n",
      "iteration number =  2315 Cross Entropy Loss= 75.56678805347515\n",
      "iteration number =  2316 Cross Entropy Loss= 75.55762195298573\n",
      "iteration number =  2317 Cross Entropy Loss= 75.54846273886768\n",
      "iteration number =  2318 Cross Entropy Loss= 75.53931040365606\n",
      "iteration number =  2319 Cross Entropy Loss= 75.53016493989718\n",
      "iteration number =  2320 Cross Entropy Loss= 75.52102634014868\n",
      "iteration number =  2321 Cross Entropy Loss= 75.51189459697954\n",
      "iteration number =  2322 Cross Entropy Loss= 75.50276970296996\n",
      "iteration number =  2323 Cross Entropy Loss= 75.4936516507114\n",
      "iteration number =  2324 Cross Entropy Loss= 75.48454043280658\n",
      "iteration number =  2325 Cross Entropy Loss= 75.47543604186936\n",
      "iteration number =  2326 Cross Entropy Loss= 75.46633847052483\n",
      "iteration number =  2327 Cross Entropy Loss= 75.45724771140924\n",
      "iteration number =  2328 Cross Entropy Loss= 75.44816375716997\n",
      "iteration number =  2329 Cross Entropy Loss= 75.4390866004655\n",
      "iteration number =  2330 Cross Entropy Loss= 75.43001623396546\n",
      "iteration number =  2331 Cross Entropy Loss= 75.42095265035049\n",
      "iteration number =  2332 Cross Entropy Loss= 75.41189584231232\n",
      "iteration number =  2333 Cross Entropy Loss= 75.40284580255374\n",
      "iteration number =  2334 Cross Entropy Loss= 75.3938025237885\n",
      "iteration number =  2335 Cross Entropy Loss= 75.38476599874133\n",
      "iteration number =  2336 Cross Entropy Loss= 75.37573622014801\n",
      "iteration number =  2337 Cross Entropy Loss= 75.36671318075523\n",
      "iteration number =  2338 Cross Entropy Loss= 75.35769687332052\n",
      "iteration number =  2339 Cross Entropy Loss= 75.34868729061247\n",
      "iteration number =  2340 Cross Entropy Loss= 75.33968442541044\n",
      "iteration number =  2341 Cross Entropy Loss= 75.3306882705047\n",
      "iteration number =  2342 Cross Entropy Loss= 75.32169881869635\n",
      "iteration number =  2343 Cross Entropy Loss= 75.31271606279732\n",
      "iteration number =  2344 Cross Entropy Loss= 75.30373999563037\n",
      "iteration number =  2345 Cross Entropy Loss= 75.29477061002896\n",
      "iteration number =  2346 Cross Entropy Loss= 75.2858078988374\n",
      "iteration number =  2347 Cross Entropy Loss= 75.2768518549107\n",
      "iteration number =  2348 Cross Entropy Loss= 75.26790247111458\n",
      "iteration number =  2349 Cross Entropy Loss= 75.25895974032548\n",
      "iteration number =  2350 Cross Entropy Loss= 75.25002365543051\n",
      "iteration number =  2351 Cross Entropy Loss= 75.2410942093274\n",
      "iteration number =  2352 Cross Entropy Loss= 75.23217139492463\n",
      "iteration number =  2353 Cross Entropy Loss= 75.22325520514116\n",
      "iteration number =  2354 Cross Entropy Loss= 75.21434563290663\n",
      "iteration number =  2355 Cross Entropy Loss= 75.20544267116125\n",
      "iteration number =  2356 Cross Entropy Loss= 75.19654631285576\n",
      "iteration number =  2357 Cross Entropy Loss= 75.18765655095143\n",
      "iteration number =  2358 Cross Entropy Loss= 75.17877337842009\n",
      "iteration number =  2359 Cross Entropy Loss= 75.16989678824405\n",
      "iteration number =  2360 Cross Entropy Loss= 75.16102677341605\n",
      "iteration number =  2361 Cross Entropy Loss= 75.15216332693937\n",
      "iteration number =  2362 Cross Entropy Loss= 75.14330644182766\n",
      "iteration number =  2363 Cross Entropy Loss= 75.13445611110501\n",
      "iteration number =  2364 Cross Entropy Loss= 75.1256123278059\n",
      "iteration number =  2365 Cross Entropy Loss= 75.1167750849752\n",
      "iteration number =  2366 Cross Entropy Loss= 75.10794437566813\n",
      "iteration number =  2367 Cross Entropy Loss= 75.09912019295024\n",
      "iteration number =  2368 Cross Entropy Loss= 75.09030252989743\n",
      "iteration number =  2369 Cross Entropy Loss= 75.08149137959583\n",
      "iteration number =  2370 Cross Entropy Loss= 75.0726867351419\n",
      "iteration number =  2371 Cross Entropy Loss= 75.0638885896424\n",
      "iteration number =  2372 Cross Entropy Loss= 75.05509693621423\n",
      "iteration number =  2373 Cross Entropy Loss= 75.04631176798459\n",
      "iteration number =  2374 Cross Entropy Loss= 75.03753307809085\n",
      "iteration number =  2375 Cross Entropy Loss= 75.02876085968053\n",
      "iteration number =  2376 Cross Entropy Loss= 75.01999510591139\n",
      "iteration number =  2377 Cross Entropy Loss= 75.01123580995127\n",
      "iteration number =  2378 Cross Entropy Loss= 75.00248296497819\n",
      "iteration number =  2379 Cross Entropy Loss= 74.99373656418018\n",
      "iteration number =  2380 Cross Entropy Loss= 74.98499660075547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2381 Cross Entropy Loss= 74.97626306791226\n",
      "iteration number =  2382 Cross Entropy Loss= 74.96753595886887\n",
      "iteration number =  2383 Cross Entropy Loss= 74.95881526685362\n",
      "iteration number =  2384 Cross Entropy Loss= 74.95010098510481\n",
      "iteration number =  2385 Cross Entropy Loss= 74.9413931068708\n",
      "iteration number =  2386 Cross Entropy Loss= 74.93269162540984\n",
      "iteration number =  2387 Cross Entropy Loss= 74.92399653399022\n",
      "iteration number =  2388 Cross Entropy Loss= 74.91530782589008\n",
      "iteration number =  2389 Cross Entropy Loss= 74.90662549439753\n",
      "iteration number =  2390 Cross Entropy Loss= 74.89794953281059\n",
      "iteration number =  2391 Cross Entropy Loss= 74.8892799344371\n",
      "iteration number =  2392 Cross Entropy Loss= 74.8806166925948\n",
      "iteration number =  2393 Cross Entropy Loss= 74.87195980061128\n",
      "iteration number =  2394 Cross Entropy Loss= 74.86330925182392\n",
      "iteration number =  2395 Cross Entropy Loss= 74.85466503957991\n",
      "iteration number =  2396 Cross Entropy Loss= 74.84602715723628\n",
      "iteration number =  2397 Cross Entropy Loss= 74.83739559815973\n",
      "iteration number =  2398 Cross Entropy Loss= 74.8287703557268\n",
      "iteration number =  2399 Cross Entropy Loss= 74.82015142332372\n",
      "iteration number =  2400 Cross Entropy Loss= 74.81153879434645\n",
      "iteration number =  2401 Cross Entropy Loss= 74.8029324622006\n",
      "iteration number =  2402 Cross Entropy Loss= 74.79433242030152\n",
      "iteration number =  2403 Cross Entropy Loss= 74.78573866207418\n",
      "iteration number =  2404 Cross Entropy Loss= 74.7771511809532\n",
      "iteration number =  2405 Cross Entropy Loss= 74.76856997038281\n",
      "iteration number =  2406 Cross Entropy Loss= 74.75999502381686\n",
      "iteration number =  2407 Cross Entropy Loss= 74.7514263347188\n",
      "iteration number =  2408 Cross Entropy Loss= 74.7428638965616\n",
      "iteration number =  2409 Cross Entropy Loss= 74.73430770282783\n",
      "iteration number =  2410 Cross Entropy Loss= 74.72575774700957\n",
      "iteration number =  2411 Cross Entropy Loss= 74.7172140226084\n",
      "iteration number =  2412 Cross Entropy Loss= 74.70867652313547\n",
      "iteration number =  2413 Cross Entropy Loss= 74.70014524211129\n",
      "iteration number =  2414 Cross Entropy Loss= 74.69162017306591\n",
      "iteration number =  2415 Cross Entropy Loss= 74.68310130953884\n",
      "iteration number =  2416 Cross Entropy Loss= 74.67458864507897\n",
      "iteration number =  2417 Cross Entropy Loss= 74.6660821732446\n",
      "iteration number =  2418 Cross Entropy Loss= 74.65758188760347\n",
      "iteration number =  2419 Cross Entropy Loss= 74.64908778173262\n",
      "iteration number =  2420 Cross Entropy Loss= 74.64059984921852\n",
      "iteration number =  2421 Cross Entropy Loss= 74.63211808365695\n",
      "iteration number =  2422 Cross Entropy Loss= 74.62364247865298\n",
      "iteration number =  2423 Cross Entropy Loss= 74.61517302782103\n",
      "iteration number =  2424 Cross Entropy Loss= 74.60670972478479\n",
      "iteration number =  2425 Cross Entropy Loss= 74.59825256317721\n",
      "iteration number =  2426 Cross Entropy Loss= 74.58980153664051\n",
      "iteration number =  2427 Cross Entropy Loss= 74.58135663882614\n",
      "iteration number =  2428 Cross Entropy Loss= 74.57291786339476\n",
      "iteration number =  2429 Cross Entropy Loss= 74.56448520401625\n",
      "iteration number =  2430 Cross Entropy Loss= 74.55605865436965\n",
      "iteration number =  2431 Cross Entropy Loss= 74.54763820814318\n",
      "iteration number =  2432 Cross Entropy Loss= 74.53922385903421\n",
      "iteration number =  2433 Cross Entropy Loss= 74.53081560074924\n",
      "iteration number =  2434 Cross Entropy Loss= 74.5224134270039\n",
      "iteration number =  2435 Cross Entropy Loss= 74.5140173315229\n",
      "iteration number =  2436 Cross Entropy Loss= 74.50562730804003\n",
      "iteration number =  2437 Cross Entropy Loss= 74.49724335029819\n",
      "iteration number =  2438 Cross Entropy Loss= 74.48886545204925\n",
      "iteration number =  2439 Cross Entropy Loss= 74.48049360705421\n",
      "iteration number =  2440 Cross Entropy Loss= 74.47212780908299\n",
      "iteration number =  2441 Cross Entropy Loss= 74.4637680519146\n",
      "iteration number =  2442 Cross Entropy Loss= 74.45541432933695\n",
      "iteration number =  2443 Cross Entropy Loss= 74.44706663514695\n",
      "iteration number =  2444 Cross Entropy Loss= 74.43872496315049\n",
      "iteration number =  2445 Cross Entropy Loss= 74.43038930716234\n",
      "iteration number =  2446 Cross Entropy Loss= 74.42205966100622\n",
      "iteration number =  2447 Cross Entropy Loss= 74.41373601851475\n",
      "iteration number =  2448 Cross Entropy Loss= 74.40541837352941\n",
      "iteration number =  2449 Cross Entropy Loss= 74.39710671990059\n",
      "iteration number =  2450 Cross Entropy Loss= 74.38880105148749\n",
      "iteration number =  2451 Cross Entropy Loss= 74.38050136215814\n",
      "iteration number =  2452 Cross Entropy Loss= 74.37220764578942\n",
      "iteration number =  2453 Cross Entropy Loss= 74.363919896267\n",
      "iteration number =  2454 Cross Entropy Loss= 74.35563810748536\n",
      "iteration number =  2455 Cross Entropy Loss= 74.34736227334768\n",
      "iteration number =  2456 Cross Entropy Loss= 74.33909238776599\n",
      "iteration number =  2457 Cross Entropy Loss= 74.33082844466097\n",
      "iteration number =  2458 Cross Entropy Loss= 74.32257043796209\n",
      "iteration number =  2459 Cross Entropy Loss= 74.31431836160746\n",
      "iteration number =  2460 Cross Entropy Loss= 74.30607220954394\n",
      "iteration number =  2461 Cross Entropy Loss= 74.29783197572706\n",
      "iteration number =  2462 Cross Entropy Loss= 74.28959765412095\n",
      "iteration number =  2463 Cross Entropy Loss= 74.28136923869845\n",
      "iteration number =  2464 Cross Entropy Loss= 74.27314672344099\n",
      "iteration number =  2465 Cross Entropy Loss= 74.2649301023386\n",
      "iteration number =  2466 Cross Entropy Loss= 74.25671936938994\n",
      "iteration number =  2467 Cross Entropy Loss= 74.24851451860226\n",
      "iteration number =  2468 Cross Entropy Loss= 74.2403155439913\n",
      "iteration number =  2469 Cross Entropy Loss= 74.23212243958142\n",
      "iteration number =  2470 Cross Entropy Loss= 74.2239351994055\n",
      "iteration number =  2471 Cross Entropy Loss= 74.2157538175049\n",
      "iteration number =  2472 Cross Entropy Loss= 74.20757828792952\n",
      "iteration number =  2473 Cross Entropy Loss= 74.19940860473773\n",
      "iteration number =  2474 Cross Entropy Loss= 74.19124476199636\n",
      "iteration number =  2475 Cross Entropy Loss= 74.18308675378073\n",
      "iteration number =  2476 Cross Entropy Loss= 74.17493457417459\n",
      "iteration number =  2477 Cross Entropy Loss= 74.16678821727004\n",
      "iteration number =  2478 Cross Entropy Loss= 74.15864767716772\n",
      "iteration number =  2479 Cross Entropy Loss= 74.15051294797655\n",
      "iteration number =  2480 Cross Entropy Loss= 74.1423840238139\n",
      "iteration number =  2481 Cross Entropy Loss= 74.13426089880546\n",
      "iteration number =  2482 Cross Entropy Loss= 74.12614356708531\n",
      "iteration number =  2483 Cross Entropy Loss= 74.11803202279579\n",
      "iteration number =  2484 Cross Entropy Loss= 74.10992626008766\n",
      "iteration number =  2485 Cross Entropy Loss= 74.10182627311988\n",
      "iteration number =  2486 Cross Entropy Loss= 74.09373205605978\n",
      "iteration number =  2487 Cross Entropy Loss= 74.08564360308294\n",
      "iteration number =  2488 Cross Entropy Loss= 74.07756090837313\n",
      "iteration number =  2489 Cross Entropy Loss= 74.06948396612245\n",
      "iteration number =  2490 Cross Entropy Loss= 74.06141277053122\n",
      "iteration number =  2491 Cross Entropy Loss= 74.05334731580791\n",
      "iteration number =  2492 Cross Entropy Loss= 74.04528759616926\n",
      "iteration number =  2493 Cross Entropy Loss= 74.03723360584014\n",
      "iteration number =  2494 Cross Entropy Loss= 74.02918533905361\n",
      "iteration number =  2495 Cross Entropy Loss= 74.02114279005089\n",
      "iteration number =  2496 Cross Entropy Loss= 74.01310595308134\n",
      "iteration number =  2497 Cross Entropy Loss= 74.0050748224024\n",
      "iteration number =  2498 Cross Entropy Loss= 73.99704939227969\n",
      "iteration number =  2499 Cross Entropy Loss= 73.9890296569869\n",
      "iteration number =  2500 Cross Entropy Loss= 73.98101561080576\n",
      "iteration number =  2501 Cross Entropy Loss= 73.97300724802611\n",
      "iteration number =  2502 Cross Entropy Loss= 73.96500456294581\n",
      "iteration number =  2503 Cross Entropy Loss= 73.95700754987081\n",
      "iteration number =  2504 Cross Entropy Loss= 73.94901620311502\n",
      "iteration number =  2505 Cross Entropy Loss= 73.9410305170004\n",
      "iteration number =  2506 Cross Entropy Loss= 73.93305048585687\n",
      "iteration number =  2507 Cross Entropy Loss= 73.92507610402235\n",
      "iteration number =  2508 Cross Entropy Loss= 73.91710736584274\n",
      "iteration number =  2509 Cross Entropy Loss= 73.90914426567186\n",
      "iteration number =  2510 Cross Entropy Loss= 73.90118679787147\n",
      "iteration number =  2511 Cross Entropy Loss= 73.89323495681128\n",
      "iteration number =  2512 Cross Entropy Loss= 73.88528873686886\n",
      "iteration number =  2513 Cross Entropy Loss= 73.87734813242974\n",
      "iteration number =  2514 Cross Entropy Loss= 73.86941313788725\n",
      "iteration number =  2515 Cross Entropy Loss= 73.86148374764267\n",
      "iteration number =  2516 Cross Entropy Loss= 73.85355995610503\n",
      "iteration number =  2517 Cross Entropy Loss= 73.8456417576913\n",
      "iteration number =  2518 Cross Entropy Loss= 73.8377291468262\n",
      "iteration number =  2519 Cross Entropy Loss= 73.8298221179423\n",
      "iteration number =  2520 Cross Entropy Loss= 73.82192066547994\n",
      "iteration number =  2521 Cross Entropy Loss= 73.81402478388728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2522 Cross Entropy Loss= 73.80613446762017\n",
      "iteration number =  2523 Cross Entropy Loss= 73.79824971114229\n",
      "iteration number =  2524 Cross Entropy Loss= 73.79037050892504\n",
      "iteration number =  2525 Cross Entropy Loss= 73.7824968554475\n",
      "iteration number =  2526 Cross Entropy Loss= 73.77462874519657\n",
      "iteration number =  2527 Cross Entropy Loss= 73.76676617266669\n",
      "iteration number =  2528 Cross Entropy Loss= 73.75890913236013\n",
      "iteration number =  2529 Cross Entropy Loss= 73.75105761878675\n",
      "iteration number =  2530 Cross Entropy Loss= 73.74321162646407\n",
      "iteration number =  2531 Cross Entropy Loss= 73.73537114991733\n",
      "iteration number =  2532 Cross Entropy Loss= 73.72753618367928\n",
      "iteration number =  2533 Cross Entropy Loss= 73.71970672229037\n",
      "iteration number =  2534 Cross Entropy Loss= 73.71188276029864\n",
      "iteration number =  2535 Cross Entropy Loss= 73.70406429225967\n",
      "iteration number =  2536 Cross Entropy Loss= 73.6962513127367\n",
      "iteration number =  2537 Cross Entropy Loss= 73.68844381630046\n",
      "iteration number =  2538 Cross Entropy Loss= 73.68064179752928\n",
      "iteration number =  2539 Cross Entropy Loss= 73.67284525100897\n",
      "iteration number =  2540 Cross Entropy Loss= 73.6650541713329\n",
      "iteration number =  2541 Cross Entropy Loss= 73.65726855310194\n",
      "iteration number =  2542 Cross Entropy Loss= 73.64948839092446\n",
      "iteration number =  2543 Cross Entropy Loss= 73.6417136794163\n",
      "iteration number =  2544 Cross Entropy Loss= 73.63394441320077\n",
      "iteration number =  2545 Cross Entropy Loss= 73.62618058690866\n",
      "iteration number =  2546 Cross Entropy Loss= 73.61842219517817\n",
      "iteration number =  2547 Cross Entropy Loss= 73.61066923265491\n",
      "iteration number =  2548 Cross Entropy Loss= 73.602921693992\n",
      "iteration number =  2549 Cross Entropy Loss= 73.59517957384986\n",
      "iteration number =  2550 Cross Entropy Loss= 73.58744286689634\n",
      "iteration number =  2551 Cross Entropy Loss= 73.57971156780667\n",
      "iteration number =  2552 Cross Entropy Loss= 73.57198567126343\n",
      "iteration number =  2553 Cross Entropy Loss= 73.56426517195658\n",
      "iteration number =  2554 Cross Entropy Loss= 73.55655006458338\n",
      "iteration number =  2555 Cross Entropy Loss= 73.54884034384844\n",
      "iteration number =  2556 Cross Entropy Loss= 73.54113600446364\n",
      "iteration number =  2557 Cross Entropy Loss= 73.53343704114823\n",
      "iteration number =  2558 Cross Entropy Loss= 73.5257434486287\n",
      "iteration number =  2559 Cross Entropy Loss= 73.5180552216388\n",
      "iteration number =  2560 Cross Entropy Loss= 73.51037235491958\n",
      "iteration number =  2561 Cross Entropy Loss= 73.5026948432193\n",
      "iteration number =  2562 Cross Entropy Loss= 73.49502268129349\n",
      "iteration number =  2563 Cross Entropy Loss= 73.48735586390487\n",
      "iteration number =  2564 Cross Entropy Loss= 73.47969438582338\n",
      "iteration number =  2565 Cross Entropy Loss= 73.47203824182616\n",
      "iteration number =  2566 Cross Entropy Loss= 73.46438742669753\n",
      "iteration number =  2567 Cross Entropy Loss= 73.45674193522899\n",
      "iteration number =  2568 Cross Entropy Loss= 73.44910176221919\n",
      "iteration number =  2569 Cross Entropy Loss= 73.4414669024739\n",
      "iteration number =  2570 Cross Entropy Loss= 73.4338373508061\n",
      "iteration number =  2571 Cross Entropy Loss= 73.42621310203582\n",
      "iteration number =  2572 Cross Entropy Loss= 73.41859415099019\n",
      "iteration number =  2573 Cross Entropy Loss= 73.4109804925035\n",
      "iteration number =  2574 Cross Entropy Loss= 73.40337212141708\n",
      "iteration number =  2575 Cross Entropy Loss= 73.39576903257934\n",
      "iteration number =  2576 Cross Entropy Loss= 73.38817122084573\n",
      "iteration number =  2577 Cross Entropy Loss= 73.3805786810788\n",
      "iteration number =  2578 Cross Entropy Loss= 73.37299140814808\n",
      "iteration number =  2579 Cross Entropy Loss= 73.36540939693015\n",
      "iteration number =  2580 Cross Entropy Loss= 73.35783264230855\n",
      "iteration number =  2581 Cross Entropy Loss= 73.3502611391739\n",
      "iteration number =  2582 Cross Entropy Loss= 73.34269488242374\n",
      "iteration number =  2583 Cross Entropy Loss= 73.33513386696262\n",
      "iteration number =  2584 Cross Entropy Loss= 73.32757808770202\n",
      "iteration number =  2585 Cross Entropy Loss= 73.3200275395604\n",
      "iteration number =  2586 Cross Entropy Loss= 73.31248221746311\n",
      "iteration number =  2587 Cross Entropy Loss= 73.30494211634249\n",
      "iteration number =  2588 Cross Entropy Loss= 73.2974072311377\n",
      "iteration number =  2589 Cross Entropy Loss= 73.2898775567949\n",
      "iteration number =  2590 Cross Entropy Loss= 73.28235308826709\n",
      "iteration number =  2591 Cross Entropy Loss= 73.27483382051415\n",
      "iteration number =  2592 Cross Entropy Loss= 73.26731974850279\n",
      "iteration number =  2593 Cross Entropy Loss= 73.25981086720662\n",
      "iteration number =  2594 Cross Entropy Loss= 73.25230717160609\n",
      "iteration number =  2595 Cross Entropy Loss= 73.24480865668845\n",
      "iteration number =  2596 Cross Entropy Loss= 73.23731531744775\n",
      "iteration number =  2597 Cross Entropy Loss= 73.22982714888494\n",
      "iteration number =  2598 Cross Entropy Loss= 73.22234414600763\n",
      "iteration number =  2599 Cross Entropy Loss= 73.21486630383032\n",
      "iteration number =  2600 Cross Entropy Loss= 73.20739361737422\n",
      "iteration number =  2601 Cross Entropy Loss= 73.19992608166731\n",
      "iteration number =  2602 Cross Entropy Loss= 73.19246369174431\n",
      "iteration number =  2603 Cross Entropy Loss= 73.18500644264671\n",
      "iteration number =  2604 Cross Entropy Loss= 73.17755432942263\n",
      "iteration number =  2605 Cross Entropy Loss= 73.17010734712707\n",
      "iteration number =  2606 Cross Entropy Loss= 73.16266549082152\n",
      "iteration number =  2607 Cross Entropy Loss= 73.15522875557433\n",
      "iteration number =  2608 Cross Entropy Loss= 73.14779713646041\n",
      "iteration number =  2609 Cross Entropy Loss= 73.14037062856141\n",
      "iteration number =  2610 Cross Entropy Loss= 73.1329492269656\n",
      "iteration number =  2611 Cross Entropy Loss= 73.12553292676786\n",
      "iteration number =  2612 Cross Entropy Loss= 73.11812172306978\n",
      "iteration number =  2613 Cross Entropy Loss= 73.1107156109795\n",
      "iteration number =  2614 Cross Entropy Loss= 73.10331458561177\n",
      "iteration number =  2615 Cross Entropy Loss= 73.095918642088\n",
      "iteration number =  2616 Cross Entropy Loss= 73.08852777553611\n",
      "iteration number =  2617 Cross Entropy Loss= 73.08114198109061\n",
      "iteration number =  2618 Cross Entropy Loss= 73.0737612538926\n",
      "iteration number =  2619 Cross Entropy Loss= 73.0663855890897\n",
      "iteration number =  2620 Cross Entropy Loss= 73.0590149818361\n",
      "iteration number =  2621 Cross Entropy Loss= 73.05164942729247\n",
      "iteration number =  2622 Cross Entropy Loss= 73.04428892062604\n",
      "iteration number =  2623 Cross Entropy Loss= 73.03693345701053\n",
      "iteration number =  2624 Cross Entropy Loss= 73.02958303162615\n",
      "iteration number =  2625 Cross Entropy Loss= 73.0222376396596\n",
      "iteration number =  2626 Cross Entropy Loss= 73.01489727630403\n",
      "iteration number =  2627 Cross Entropy Loss= 73.00756193675906\n",
      "iteration number =  2628 Cross Entropy Loss= 73.00023161623079\n",
      "iteration number =  2629 Cross Entropy Loss= 72.99290630993173\n",
      "iteration number =  2630 Cross Entropy Loss= 72.98558601308079\n",
      "iteration number =  2631 Cross Entropy Loss= 72.97827072090338\n",
      "iteration number =  2632 Cross Entropy Loss= 72.97096042863122\n",
      "iteration number =  2633 Cross Entropy Loss= 72.96365513150249\n",
      "iteration number =  2634 Cross Entropy Loss= 72.9563548247617\n",
      "iteration number =  2635 Cross Entropy Loss= 72.94905950365978\n",
      "iteration number =  2636 Cross Entropy Loss= 72.94176916345401\n",
      "iteration number =  2637 Cross Entropy Loss= 72.93448379940799\n",
      "iteration number =  2638 Cross Entropy Loss= 72.92720340679169\n",
      "iteration number =  2639 Cross Entropy Loss= 72.91992798088141\n",
      "iteration number =  2640 Cross Entropy Loss= 72.91265751695974\n",
      "iteration number =  2641 Cross Entropy Loss= 72.90539201031561\n",
      "iteration number =  2642 Cross Entropy Loss= 72.89813145624423\n",
      "iteration number =  2643 Cross Entropy Loss= 72.89087585004708\n",
      "iteration number =  2644 Cross Entropy Loss= 72.88362518703195\n",
      "iteration number =  2645 Cross Entropy Loss= 72.87637946251287\n",
      "iteration number =  2646 Cross Entropy Loss= 72.86913867181012\n",
      "iteration number =  2647 Cross Entropy Loss= 72.86190281025023\n",
      "iteration number =  2648 Cross Entropy Loss= 72.85467187316596\n",
      "iteration number =  2649 Cross Entropy Loss= 72.8474458558963\n",
      "iteration number =  2650 Cross Entropy Loss= 72.84022475378644\n",
      "iteration number =  2651 Cross Entropy Loss= 72.83300856218779\n",
      "iteration number =  2652 Cross Entropy Loss= 72.82579727645792\n",
      "iteration number =  2653 Cross Entropy Loss= 72.81859089196058\n",
      "iteration number =  2654 Cross Entropy Loss= 72.81138940406572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2655 Cross Entropy Loss= 72.80419280814942\n",
      "iteration number =  2656 Cross Entropy Loss= 72.79700109959393\n",
      "iteration number =  2657 Cross Entropy Loss= 72.7898142737876\n",
      "iteration number =  2658 Cross Entropy Loss= 72.78263232612495\n",
      "iteration number =  2659 Cross Entropy Loss= 72.7754552520066\n",
      "iteration number =  2660 Cross Entropy Loss= 72.76828304683924\n",
      "iteration number =  2661 Cross Entropy Loss= 72.76111570603571\n",
      "iteration number =  2662 Cross Entropy Loss= 72.75395322501491\n",
      "iteration number =  2663 Cross Entropy Loss= 72.74679559920182\n",
      "iteration number =  2664 Cross Entropy Loss= 72.73964282402747\n",
      "iteration number =  2665 Cross Entropy Loss= 72.73249489492899\n",
      "iteration number =  2666 Cross Entropy Loss= 72.72535180734948\n",
      "iteration number =  2667 Cross Entropy Loss= 72.71821355673812\n",
      "iteration number =  2668 Cross Entropy Loss= 72.71108013855013\n",
      "iteration number =  2669 Cross Entropy Loss= 72.70395154824669\n",
      "iteration number =  2670 Cross Entropy Loss= 72.69682778129501\n",
      "iteration number =  2671 Cross Entropy Loss= 72.68970883316834\n",
      "iteration number =  2672 Cross Entropy Loss= 72.68259469934584\n",
      "iteration number =  2673 Cross Entropy Loss= 72.67548537531266\n",
      "iteration number =  2674 Cross Entropy Loss= 72.66838085655995\n",
      "iteration number =  2675 Cross Entropy Loss= 72.66128113858477\n",
      "iteration number =  2676 Cross Entropy Loss= 72.65418621689014\n",
      "iteration number =  2677 Cross Entropy Loss= 72.647096086985\n",
      "iteration number =  2678 Cross Entropy Loss= 72.64001074438426\n",
      "iteration number =  2679 Cross Entropy Loss= 72.63293018460867\n",
      "iteration number =  2680 Cross Entropy Loss= 72.62585440318492\n",
      "iteration number =  2681 Cross Entropy Loss= 72.61878339564561\n",
      "iteration number =  2682 Cross Entropy Loss= 72.6117171575292\n",
      "iteration number =  2683 Cross Entropy Loss= 72.60465568438002\n",
      "iteration number =  2684 Cross Entropy Loss= 72.59759897174825\n",
      "iteration number =  2685 Cross Entropy Loss= 72.59054701518998\n",
      "iteration number =  2686 Cross Entropy Loss= 72.5834998102671\n",
      "iteration number =  2687 Cross Entropy Loss= 72.5764573525473\n",
      "iteration number =  2688 Cross Entropy Loss= 72.56941963760417\n",
      "iteration number =  2689 Cross Entropy Loss= 72.56238666101706\n",
      "iteration number =  2690 Cross Entropy Loss= 72.55535841837114\n",
      "iteration number =  2691 Cross Entropy Loss= 72.54833490525738\n",
      "iteration number =  2692 Cross Entropy Loss= 72.5413161172725\n",
      "iteration number =  2693 Cross Entropy Loss= 72.53430205001905\n",
      "iteration number =  2694 Cross Entropy Loss= 72.52729269910532\n",
      "iteration number =  2695 Cross Entropy Loss= 72.52028806014533\n",
      "iteration number =  2696 Cross Entropy Loss= 72.51328812875887\n",
      "iteration number =  2697 Cross Entropy Loss= 72.5062929005715\n",
      "iteration number =  2698 Cross Entropy Loss= 72.49930237121441\n",
      "iteration number =  2699 Cross Entropy Loss= 72.49231653632462\n",
      "iteration number =  2700 Cross Entropy Loss= 72.4853353915448\n",
      "iteration number =  2701 Cross Entropy Loss= 72.47835893252326\n",
      "iteration number =  2702 Cross Entropy Loss= 72.47138715491415\n",
      "iteration number =  2703 Cross Entropy Loss= 72.46442005437716\n",
      "iteration number =  2704 Cross Entropy Loss= 72.45745762657769\n",
      "iteration number =  2705 Cross Entropy Loss= 72.45049986718683\n",
      "iteration number =  2706 Cross Entropy Loss= 72.4435467718813\n",
      "iteration number =  2707 Cross Entropy Loss= 72.43659833634342\n",
      "iteration number =  2708 Cross Entropy Loss= 72.42965455626123\n",
      "iteration number =  2709 Cross Entropy Loss= 72.42271542732831\n",
      "iteration number =  2710 Cross Entropy Loss= 72.41578094524391\n",
      "iteration number =  2711 Cross Entropy Loss= 72.40885110571284\n",
      "iteration number =  2712 Cross Entropy Loss= 72.40192590444552\n",
      "iteration number =  2713 Cross Entropy Loss= 72.39500533715797\n",
      "iteration number =  2714 Cross Entropy Loss= 72.38808939957175\n",
      "iteration number =  2715 Cross Entropy Loss= 72.38117808741404\n",
      "iteration number =  2716 Cross Entropy Loss= 72.37427139641751\n",
      "iteration number =  2717 Cross Entropy Loss= 72.36736932232043\n",
      "iteration number =  2718 Cross Entropy Loss= 72.3604718608666\n",
      "iteration number =  2719 Cross Entropy Loss= 72.35357900780531\n",
      "iteration number =  2720 Cross Entropy Loss= 72.34669075889143\n",
      "iteration number =  2721 Cross Entropy Loss= 72.33980710988529\n",
      "iteration number =  2722 Cross Entropy Loss= 72.33292805655275\n",
      "iteration number =  2723 Cross Entropy Loss= 72.32605359466515\n",
      "iteration number =  2724 Cross Entropy Loss= 72.31918371999932\n",
      "iteration number =  2725 Cross Entropy Loss= 72.31231842833756\n",
      "iteration number =  2726 Cross Entropy Loss= 72.30545771546764\n",
      "iteration number =  2727 Cross Entropy Loss= 72.29860157718278\n",
      "iteration number =  2728 Cross Entropy Loss= 72.29175000928163\n",
      "iteration number =  2729 Cross Entropy Loss= 72.28490300756833\n",
      "iteration number =  2730 Cross Entropy Loss= 72.2780605678524\n",
      "iteration number =  2731 Cross Entropy Loss= 72.27122268594879\n",
      "iteration number =  2732 Cross Entropy Loss= 72.26438935767789\n",
      "iteration number =  2733 Cross Entropy Loss= 72.25756057886545\n",
      "iteration number =  2734 Cross Entropy Loss= 72.25073634534263\n",
      "iteration number =  2735 Cross Entropy Loss= 72.24391665294598\n",
      "iteration number =  2736 Cross Entropy Loss= 72.23710149751744\n",
      "iteration number =  2737 Cross Entropy Loss= 72.23029087490428\n",
      "iteration number =  2738 Cross Entropy Loss= 72.22348478095914\n",
      "iteration number =  2739 Cross Entropy Loss= 72.21668321154003\n",
      "iteration number =  2740 Cross Entropy Loss= 72.20988616251027\n",
      "iteration number =  2741 Cross Entropy Loss= 72.20309362973856\n",
      "iteration number =  2742 Cross Entropy Loss= 72.19630560909883\n",
      "iteration number =  2743 Cross Entropy Loss= 72.18952209647044\n",
      "iteration number =  2744 Cross Entropy Loss= 72.18274308773795\n",
      "iteration number =  2745 Cross Entropy Loss= 72.1759685787913\n",
      "iteration number =  2746 Cross Entropy Loss= 72.16919856552565\n",
      "iteration number =  2747 Cross Entropy Loss= 72.16243304384147\n",
      "iteration number =  2748 Cross Entropy Loss= 72.15567200964452\n",
      "iteration number =  2749 Cross Entropy Loss= 72.14891545884579\n",
      "iteration number =  2750 Cross Entropy Loss= 72.14216338736153\n",
      "iteration number =  2751 Cross Entropy Loss= 72.13541579111322\n",
      "iteration number =  2752 Cross Entropy Loss= 72.12867266602761\n",
      "iteration number =  2753 Cross Entropy Loss= 72.12193400803667\n",
      "iteration number =  2754 Cross Entropy Loss= 72.11519981307754\n",
      "iteration number =  2755 Cross Entropy Loss= 72.10847007709263\n",
      "iteration number =  2756 Cross Entropy Loss= 72.10174479602952\n",
      "iteration number =  2757 Cross Entropy Loss= 72.095023965841\n",
      "iteration number =  2758 Cross Entropy Loss= 72.08830758248502\n",
      "iteration number =  2759 Cross Entropy Loss= 72.08159564192472\n",
      "iteration number =  2760 Cross Entropy Loss= 72.0748881401284\n",
      "iteration number =  2761 Cross Entropy Loss= 72.06818507306953\n",
      "iteration number =  2762 Cross Entropy Loss= 72.06148643672674\n",
      "iteration number =  2763 Cross Entropy Loss= 72.05479222708375\n",
      "iteration number =  2764 Cross Entropy Loss= 72.04810244012948\n",
      "iteration number =  2765 Cross Entropy Loss= 72.04141707185794\n",
      "iteration number =  2766 Cross Entropy Loss= 72.03473611826826\n",
      "iteration number =  2767 Cross Entropy Loss= 72.02805957536467\n",
      "iteration number =  2768 Cross Entropy Loss= 72.02138743915648\n",
      "iteration number =  2769 Cross Entropy Loss= 72.01471970565817\n",
      "iteration number =  2770 Cross Entropy Loss= 72.00805637088925\n",
      "iteration number =  2771 Cross Entropy Loss= 72.00139743087428\n",
      "iteration number =  2772 Cross Entropy Loss= 71.9947428816429\n",
      "iteration number =  2773 Cross Entropy Loss= 71.98809271922988\n",
      "iteration number =  2774 Cross Entropy Loss= 71.98144693967494\n",
      "iteration number =  2775 Cross Entropy Loss= 71.97480553902288\n",
      "iteration number =  2776 Cross Entropy Loss= 71.96816851332358\n",
      "iteration number =  2777 Cross Entropy Loss= 71.96153585863185\n",
      "iteration number =  2778 Cross Entropy Loss= 71.95490757100761\n",
      "iteration number =  2779 Cross Entropy Loss= 71.94828364651572\n",
      "iteration number =  2780 Cross Entropy Loss= 71.9416640812261\n",
      "iteration number =  2781 Cross Entropy Loss= 71.93504887121361\n",
      "iteration number =  2782 Cross Entropy Loss= 71.9284380125581\n",
      "iteration number =  2783 Cross Entropy Loss= 71.92183150134446\n",
      "iteration number =  2784 Cross Entropy Loss= 71.9152293336625\n",
      "iteration number =  2785 Cross Entropy Loss= 71.90863150560693\n",
      "iteration number =  2786 Cross Entropy Loss= 71.90203801327756\n",
      "iteration number =  2787 Cross Entropy Loss= 71.895448852779\n",
      "iteration number =  2788 Cross Entropy Loss= 71.8888640202209\n",
      "iteration number =  2789 Cross Entropy Loss= 71.88228351171772\n",
      "iteration number =  2790 Cross Entropy Loss= 71.87570732338898\n",
      "iteration number =  2791 Cross Entropy Loss= 71.86913545135903\n",
      "iteration number =  2792 Cross Entropy Loss= 71.86256789175715\n",
      "iteration number =  2793 Cross Entropy Loss= 71.8560046407175\n",
      "iteration number =  2794 Cross Entropy Loss= 71.84944569437911\n",
      "iteration number =  2795 Cross Entropy Loss= 71.84289104888596\n",
      "iteration number =  2796 Cross Entropy Loss= 71.83634070038683\n",
      "iteration number =  2797 Cross Entropy Loss= 71.8297946450354\n",
      "iteration number =  2798 Cross Entropy Loss= 71.82325287899019\n",
      "iteration number =  2799 Cross Entropy Loss= 71.8167153984146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2800 Cross Entropy Loss= 71.81018219947683\n",
      "iteration number =  2801 Cross Entropy Loss= 71.80365327834994\n",
      "iteration number =  2802 Cross Entropy Loss= 71.79712863121183\n",
      "iteration number =  2803 Cross Entropy Loss= 71.79060825424517\n",
      "iteration number =  2804 Cross Entropy Loss= 71.7840921436375\n",
      "iteration number =  2805 Cross Entropy Loss= 71.77758029558109\n",
      "iteration number =  2806 Cross Entropy Loss= 71.77107270627305\n",
      "iteration number =  2807 Cross Entropy Loss= 71.76456937191529\n",
      "iteration number =  2808 Cross Entropy Loss= 71.75807028871446\n",
      "iteration number =  2809 Cross Entropy Loss= 71.75157545288204\n",
      "iteration number =  2810 Cross Entropy Loss= 71.7450848606342\n",
      "iteration number =  2811 Cross Entropy Loss= 71.7385985081919\n",
      "iteration number =  2812 Cross Entropy Loss= 71.73211639178086\n",
      "iteration number =  2813 Cross Entropy Loss= 71.72563850763153\n",
      "iteration number =  2814 Cross Entropy Loss= 71.71916485197909\n",
      "iteration number =  2815 Cross Entropy Loss= 71.71269542106344\n",
      "iteration number =  2816 Cross Entropy Loss= 71.70623021112922\n",
      "iteration number =  2817 Cross Entropy Loss= 71.69976921842576\n",
      "iteration number =  2818 Cross Entropy Loss= 71.69331243920712\n",
      "iteration number =  2819 Cross Entropy Loss= 71.686859869732\n",
      "iteration number =  2820 Cross Entropy Loss= 71.68041150626385\n",
      "iteration number =  2821 Cross Entropy Loss= 71.67396734507076\n",
      "iteration number =  2822 Cross Entropy Loss= 71.66752738242549\n",
      "iteration number =  2823 Cross Entropy Loss= 71.66109161460551\n",
      "iteration number =  2824 Cross Entropy Loss= 71.65466003789291\n",
      "iteration number =  2825 Cross Entropy Loss= 71.64823264857442\n",
      "iteration number =  2826 Cross Entropy Loss= 71.64180944294147\n",
      "iteration number =  2827 Cross Entropy Loss= 71.63539041729003\n",
      "iteration number =  2828 Cross Entropy Loss= 71.6289755679208\n",
      "iteration number =  2829 Cross Entropy Loss= 71.62256489113904\n",
      "iteration number =  2830 Cross Entropy Loss= 71.61615838325463\n",
      "iteration number =  2831 Cross Entropy Loss= 71.60975604058208\n",
      "iteration number =  2832 Cross Entropy Loss= 71.60335785944048\n",
      "iteration number =  2833 Cross Entropy Loss= 71.59696383615349\n",
      "iteration number =  2834 Cross Entropy Loss= 71.5905739670494\n",
      "iteration number =  2835 Cross Entropy Loss= 71.58418824846105\n",
      "iteration number =  2836 Cross Entropy Loss= 71.57780667672584\n",
      "iteration number =  2837 Cross Entropy Loss= 71.57142924818575\n",
      "iteration number =  2838 Cross Entropy Loss= 71.56505595918733\n",
      "iteration number =  2839 Cross Entropy Loss= 71.55868680608162\n",
      "iteration number =  2840 Cross Entropy Loss= 71.55232178522428\n",
      "iteration number =  2841 Cross Entropy Loss= 71.54596089297542\n",
      "iteration number =  2842 Cross Entropy Loss= 71.53960412569973\n",
      "iteration number =  2843 Cross Entropy Loss= 71.53325147976636\n",
      "iteration number =  2844 Cross Entropy Loss= 71.52690295154909\n",
      "iteration number =  2845 Cross Entropy Loss= 71.5205585374261\n",
      "iteration number =  2846 Cross Entropy Loss= 71.51421823378007\n",
      "iteration number =  2847 Cross Entropy Loss= 71.50788203699821\n",
      "iteration number =  2848 Cross Entropy Loss= 71.50154994347221\n",
      "iteration number =  2849 Cross Entropy Loss= 71.4952219495982\n",
      "iteration number =  2850 Cross Entropy Loss= 71.48889805177681\n",
      "iteration number =  2851 Cross Entropy Loss= 71.48257824641314\n",
      "iteration number =  2852 Cross Entropy Loss= 71.47626252991671\n",
      "iteration number =  2853 Cross Entropy Loss= 71.46995089870148\n",
      "iteration number =  2854 Cross Entropy Loss= 71.46364334918592\n",
      "iteration number =  2855 Cross Entropy Loss= 71.45733987779286\n",
      "iteration number =  2856 Cross Entropy Loss= 71.45104048094956\n",
      "iteration number =  2857 Cross Entropy Loss= 71.44474515508777\n",
      "iteration number =  2858 Cross Entropy Loss= 71.43845389664357\n",
      "iteration number =  2859 Cross Entropy Loss= 71.4321667020575\n",
      "iteration number =  2860 Cross Entropy Loss= 71.42588356777445\n",
      "iteration number =  2861 Cross Entropy Loss= 71.41960449024376\n",
      "iteration number =  2862 Cross Entropy Loss= 71.41332946591909\n",
      "iteration number =  2863 Cross Entropy Loss= 71.40705849125852\n",
      "iteration number =  2864 Cross Entropy Loss= 71.40079156272449\n",
      "iteration number =  2865 Cross Entropy Loss= 71.39452867678382\n",
      "iteration number =  2866 Cross Entropy Loss= 71.38826982990763\n",
      "iteration number =  2867 Cross Entropy Loss= 71.38201501857145\n",
      "iteration number =  2868 Cross Entropy Loss= 71.37576423925515\n",
      "iteration number =  2869 Cross Entropy Loss= 71.36951748844288\n",
      "iteration number =  2870 Cross Entropy Loss= 71.36327476262319\n",
      "iteration number =  2871 Cross Entropy Loss= 71.3570360582889\n",
      "iteration number =  2872 Cross Entropy Loss= 71.35080137193715\n",
      "iteration number =  2873 Cross Entropy Loss= 71.34457070006943\n",
      "iteration number =  2874 Cross Entropy Loss= 71.33834403919147\n",
      "iteration number =  2875 Cross Entropy Loss= 71.33212138581338\n",
      "iteration number =  2876 Cross Entropy Loss= 71.32590273644946\n",
      "iteration number =  2877 Cross Entropy Loss= 71.31968808761837\n",
      "iteration number =  2878 Cross Entropy Loss= 71.31347743584298\n",
      "iteration number =  2879 Cross Entropy Loss= 71.30727077765049\n",
      "iteration number =  2880 Cross Entropy Loss= 71.30106810957233\n",
      "iteration number =  2881 Cross Entropy Loss= 71.29486942814418\n",
      "iteration number =  2882 Cross Entropy Loss= 71.28867472990596\n",
      "iteration number =  2883 Cross Entropy Loss= 71.28248401140188\n",
      "iteration number =  2884 Cross Entropy Loss= 71.27629726918033\n",
      "iteration number =  2885 Cross Entropy Loss= 71.27011449979395\n",
      "iteration number =  2886 Cross Entropy Loss= 71.26393569979962\n",
      "iteration number =  2887 Cross Entropy Loss= 71.25776086575837\n",
      "iteration number =  2888 Cross Entropy Loss= 71.25158999423556\n",
      "iteration number =  2889 Cross Entropy Loss= 71.2454230818006\n",
      "iteration number =  2890 Cross Entropy Loss= 71.2392601250272\n",
      "iteration number =  2891 Cross Entropy Loss= 71.23310112049323\n",
      "iteration number =  2892 Cross Entropy Loss= 71.22694606478075\n",
      "iteration number =  2893 Cross Entropy Loss= 71.22079495447598\n",
      "iteration number =  2894 Cross Entropy Loss= 71.2146477861693\n",
      "iteration number =  2895 Cross Entropy Loss= 71.20850455645527\n",
      "iteration number =  2896 Cross Entropy Loss= 71.20236526193261\n",
      "iteration number =  2897 Cross Entropy Loss= 71.1962298992042\n",
      "iteration number =  2898 Cross Entropy Loss= 71.19009846487702\n",
      "iteration number =  2899 Cross Entropy Loss= 71.18397095556222\n",
      "iteration number =  2900 Cross Entropy Loss= 71.17784736787505\n",
      "iteration number =  2901 Cross Entropy Loss= 71.17172769843492\n",
      "iteration number =  2902 Cross Entropy Loss= 71.16561194386537\n",
      "iteration number =  2903 Cross Entropy Loss= 71.15950010079398\n",
      "iteration number =  2904 Cross Entropy Loss= 71.15339216585248\n",
      "iteration number =  2905 Cross Entropy Loss= 71.14728813567672\n",
      "iteration number =  2906 Cross Entropy Loss= 71.14118800690659\n",
      "iteration number =  2907 Cross Entropy Loss= 71.1350917761861\n",
      "iteration number =  2908 Cross Entropy Loss= 71.12899944016333\n",
      "iteration number =  2909 Cross Entropy Loss= 71.12291099549041\n",
      "iteration number =  2910 Cross Entropy Loss= 71.1168264388236\n",
      "iteration number =  2911 Cross Entropy Loss= 71.11074576682313\n",
      "iteration number =  2912 Cross Entropy Loss= 71.10466897615336\n",
      "iteration number =  2913 Cross Entropy Loss= 71.09859606348266\n",
      "iteration number =  2914 Cross Entropy Loss= 71.09252702548343\n",
      "iteration number =  2915 Cross Entropy Loss= 71.08646185883215\n",
      "iteration number =  2916 Cross Entropy Loss= 71.08040056020928\n",
      "iteration number =  2917 Cross Entropy Loss= 71.07434312629933\n",
      "iteration number =  2918 Cross Entropy Loss= 71.0682895537908\n",
      "iteration number =  2919 Cross Entropy Loss= 71.06223983937625\n",
      "iteration number =  2920 Cross Entropy Loss= 71.05619397975218\n",
      "iteration number =  2921 Cross Entropy Loss= 71.05015197161913\n",
      "iteration number =  2922 Cross Entropy Loss= 71.04411381168161\n",
      "iteration number =  2923 Cross Entropy Loss= 71.03807949664814\n",
      "iteration number =  2924 Cross Entropy Loss= 71.0320490232312\n",
      "iteration number =  2925 Cross Entropy Loss= 71.02602238814721\n",
      "iteration number =  2926 Cross Entropy Loss= 71.01999958811662\n",
      "iteration number =  2927 Cross Entropy Loss= 71.0139806198638\n",
      "iteration number =  2928 Cross Entropy Loss= 71.0079654801171\n",
      "iteration number =  2929 Cross Entropy Loss= 71.00195416560878\n",
      "iteration number =  2930 Cross Entropy Loss= 70.99594667307507\n",
      "iteration number =  2931 Cross Entropy Loss= 70.98994299925612\n",
      "iteration number =  2932 Cross Entropy Loss= 70.98394314089603\n",
      "iteration number =  2933 Cross Entropy Loss= 70.97794709474282\n",
      "iteration number =  2934 Cross Entropy Loss= 70.97195485754838\n",
      "iteration number =  2935 Cross Entropy Loss= 70.9659664260686\n",
      "iteration number =  2936 Cross Entropy Loss= 70.9599817970632\n",
      "iteration number =  2937 Cross Entropy Loss= 70.95400096729583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  2938 Cross Entropy Loss= 70.94802393353402\n",
      "iteration number =  2939 Cross Entropy Loss= 70.9420506925492\n",
      "iteration number =  2940 Cross Entropy Loss= 70.93608124111668\n",
      "iteration number =  2941 Cross Entropy Loss= 70.93011557601565\n",
      "iteration number =  2942 Cross Entropy Loss= 70.92415369402914\n",
      "iteration number =  2943 Cross Entropy Loss= 70.91819559194408\n",
      "iteration number =  2944 Cross Entropy Loss= 70.91224126655126\n",
      "iteration number =  2945 Cross Entropy Loss= 70.90629071464527\n",
      "iteration number =  2946 Cross Entropy Loss= 70.90034393302459\n",
      "iteration number =  2947 Cross Entropy Loss= 70.89440091849153\n",
      "iteration number =  2948 Cross Entropy Loss= 70.88846166785223\n",
      "iteration number =  2949 Cross Entropy Loss= 70.88252617791667\n",
      "iteration number =  2950 Cross Entropy Loss= 70.87659444549865\n",
      "iteration number =  2951 Cross Entropy Loss= 70.87066646741576\n",
      "iteration number =  2952 Cross Entropy Loss= 70.86474224048942\n",
      "iteration number =  2953 Cross Entropy Loss= 70.85882176154487\n",
      "iteration number =  2954 Cross Entropy Loss= 70.85290502741114\n",
      "iteration number =  2955 Cross Entropy Loss= 70.84699203492102\n",
      "iteration number =  2956 Cross Entropy Loss= 70.84108278091112\n",
      "iteration number =  2957 Cross Entropy Loss= 70.83517726222183\n",
      "iteration number =  2958 Cross Entropy Loss= 70.8292754756973\n",
      "iteration number =  2959 Cross Entropy Loss= 70.82337741818549\n",
      "iteration number =  2960 Cross Entropy Loss= 70.81748308653803\n",
      "iteration number =  2961 Cross Entropy Loss= 70.81159247761043\n",
      "iteration number =  2962 Cross Entropy Loss= 70.80570558826187\n",
      "iteration number =  2963 Cross Entropy Loss= 70.7998224153553\n",
      "iteration number =  2964 Cross Entropy Loss= 70.79394295575742\n",
      "iteration number =  2965 Cross Entropy Loss= 70.78806720633862\n",
      "iteration number =  2966 Cross Entropy Loss= 70.78219516397311\n",
      "iteration number =  2967 Cross Entropy Loss= 70.77632682553875\n",
      "iteration number =  2968 Cross Entropy Loss= 70.7704621879171\n",
      "iteration number =  2969 Cross Entropy Loss= 70.76460124799353\n",
      "iteration number =  2970 Cross Entropy Loss= 70.75874400265701\n",
      "iteration number =  2971 Cross Entropy Loss= 70.75289044880029\n",
      "iteration number =  2972 Cross Entropy Loss= 70.74704058331974\n",
      "iteration number =  2973 Cross Entropy Loss= 70.74119440311551\n",
      "iteration number =  2974 Cross Entropy Loss= 70.73535190509138\n",
      "iteration number =  2975 Cross Entropy Loss= 70.72951308615478\n",
      "iteration number =  2976 Cross Entropy Loss= 70.72367794321688\n",
      "iteration number =  2977 Cross Entropy Loss= 70.71784647319251\n",
      "iteration number =  2978 Cross Entropy Loss= 70.71201867300007\n",
      "iteration number =  2979 Cross Entropy Loss= 70.70619453956174\n",
      "iteration number =  2980 Cross Entropy Loss= 70.70037406980327\n",
      "iteration number =  2981 Cross Entropy Loss= 70.69455726065411\n",
      "iteration number =  2982 Cross Entropy Loss= 70.68874410904729\n",
      "iteration number =  2983 Cross Entropy Loss= 70.6829346119195\n",
      "iteration number =  2984 Cross Entropy Loss= 70.6771287662111\n",
      "iteration number =  2985 Cross Entropy Loss= 70.67132656886601\n",
      "iteration number =  2986 Cross Entropy Loss= 70.66552801683179\n",
      "iteration number =  2987 Cross Entropy Loss= 70.65973310705962\n",
      "iteration number =  2988 Cross Entropy Loss= 70.6539418365043\n",
      "iteration number =  2989 Cross Entropy Loss= 70.64815420212419\n",
      "iteration number =  2990 Cross Entropy Loss= 70.64237020088129\n",
      "iteration number =  2991 Cross Entropy Loss= 70.63658982974115\n",
      "iteration number =  2992 Cross Entropy Loss= 70.63081308567294\n",
      "iteration number =  2993 Cross Entropy Loss= 70.6250399656494\n",
      "iteration number =  2994 Cross Entropy Loss= 70.61927046664681\n",
      "iteration number =  2995 Cross Entropy Loss= 70.61350458564507\n",
      "iteration number =  2996 Cross Entropy Loss= 70.60774231962762\n",
      "iteration number =  2997 Cross Entropy Loss= 70.60198366558146\n",
      "iteration number =  2998 Cross Entropy Loss= 70.59622862049713\n",
      "iteration number =  2999 Cross Entropy Loss= 70.59047718136873\n",
      "iteration number =  3000 Cross Entropy Loss= 70.58472934519389\n",
      "iteration number =  3001 Cross Entropy Loss= 70.5789851089738\n",
      "iteration number =  3002 Cross Entropy Loss= 70.57324446971316\n",
      "iteration number =  3003 Cross Entropy Loss= 70.5675074244202\n",
      "iteration number =  3004 Cross Entropy Loss= 70.56177397010669\n",
      "iteration number =  3005 Cross Entropy Loss= 70.55604410378787\n",
      "iteration number =  3006 Cross Entropy Loss= 70.55031782248254\n",
      "iteration number =  3007 Cross Entropy Loss= 70.54459512321297\n",
      "iteration number =  3008 Cross Entropy Loss= 70.53887600300496\n",
      "iteration number =  3009 Cross Entropy Loss= 70.53316045888778\n",
      "iteration number =  3010 Cross Entropy Loss= 70.52744848789416\n",
      "iteration number =  3011 Cross Entropy Loss= 70.5217400870604\n",
      "iteration number =  3012 Cross Entropy Loss= 70.51603525342618\n",
      "iteration number =  3013 Cross Entropy Loss= 70.51033398403473\n",
      "iteration number =  3014 Cross Entropy Loss= 70.5046362759327\n",
      "iteration number =  3015 Cross Entropy Loss= 70.49894212617022\n",
      "iteration number =  3016 Cross Entropy Loss= 70.4932515318009\n",
      "iteration number =  3017 Cross Entropy Loss= 70.48756448988175\n",
      "iteration number =  3018 Cross Entropy Loss= 70.48188099747327\n",
      "iteration number =  3019 Cross Entropy Loss= 70.47620105163938\n",
      "iteration number =  3020 Cross Entropy Loss= 70.47052464944744\n",
      "iteration number =  3021 Cross Entropy Loss= 70.46485178796827\n",
      "iteration number =  3022 Cross Entropy Loss= 70.45918246427607\n",
      "iteration number =  3023 Cross Entropy Loss= 70.4535166754485\n",
      "iteration number =  3024 Cross Entropy Loss= 70.4478544185666\n",
      "iteration number =  3025 Cross Entropy Loss= 70.44219569071485\n",
      "iteration number =  3026 Cross Entropy Loss= 70.43654048898114\n",
      "iteration number =  3027 Cross Entropy Loss= 70.43088881045671\n",
      "iteration number =  3028 Cross Entropy Loss= 70.42524065223627\n",
      "iteration number =  3029 Cross Entropy Loss= 70.41959601141788\n",
      "iteration number =  3030 Cross Entropy Loss= 70.41395488510297\n",
      "iteration number =  3031 Cross Entropy Loss= 70.40831727039638\n",
      "iteration number =  3032 Cross Entropy Loss= 70.40268316440631\n",
      "iteration number =  3033 Cross Entropy Loss= 70.39705256424435\n",
      "iteration number =  3034 Cross Entropy Loss= 70.39142546702543\n",
      "iteration number =  3035 Cross Entropy Loss= 70.38580186986785\n",
      "iteration number =  3036 Cross Entropy Loss= 70.38018176989326\n",
      "iteration number =  3037 Cross Entropy Loss= 70.37456516422668\n",
      "iteration number =  3038 Cross Entropy Loss= 70.36895204999647\n",
      "iteration number =  3039 Cross Entropy Loss= 70.36334242433429\n",
      "iteration number =  3040 Cross Entropy Loss= 70.35773628437519\n",
      "iteration number =  3041 Cross Entropy Loss= 70.35213362725753\n",
      "iteration number =  3042 Cross Entropy Loss= 70.346534450123\n",
      "iteration number =  3043 Cross Entropy Loss= 70.34093875011658\n",
      "iteration number =  3044 Cross Entropy Loss= 70.33534652438662\n",
      "iteration number =  3045 Cross Entropy Loss= 70.32975777008471\n",
      "iteration number =  3046 Cross Entropy Loss= 70.32417248436585\n",
      "iteration number =  3047 Cross Entropy Loss= 70.31859066438818\n",
      "iteration number =  3048 Cross Entropy Loss= 70.31301230731333\n",
      "iteration number =  3049 Cross Entropy Loss= 70.30743741030608\n",
      "iteration number =  3050 Cross Entropy Loss= 70.30186597053452\n",
      "iteration number =  3051 Cross Entropy Loss= 70.2962979851701\n",
      "iteration number =  3052 Cross Entropy Loss= 70.29073345138742\n",
      "iteration number =  3053 Cross Entropy Loss= 70.28517236636446\n",
      "iteration number =  3054 Cross Entropy Loss= 70.27961472728242\n",
      "iteration number =  3055 Cross Entropy Loss= 70.27406053132577\n",
      "iteration number =  3056 Cross Entropy Loss= 70.26850977568222\n",
      "iteration number =  3057 Cross Entropy Loss= 70.26296245754274\n",
      "iteration number =  3058 Cross Entropy Loss= 70.25741857410158\n",
      "iteration number =  3059 Cross Entropy Loss= 70.25187812255618\n",
      "iteration number =  3060 Cross Entropy Loss= 70.24634110010726\n",
      "iteration number =  3061 Cross Entropy Loss= 70.24080750395876\n",
      "iteration number =  3062 Cross Entropy Loss= 70.23527733131783\n",
      "iteration number =  3063 Cross Entropy Loss= 70.22975057939487\n",
      "iteration number =  3064 Cross Entropy Loss= 70.22422724540348\n",
      "iteration number =  3065 Cross Entropy Loss= 70.21870732656048\n",
      "iteration number =  3066 Cross Entropy Loss= 70.2131908200859\n",
      "iteration number =  3067 Cross Entropy Loss= 70.20767772320298\n",
      "iteration number =  3068 Cross Entropy Loss= 70.20216803313814\n",
      "iteration number =  3069 Cross Entropy Loss= 70.19666174712103\n",
      "iteration number =  3070 Cross Entropy Loss= 70.19115886238444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3071 Cross Entropy Loss= 70.18565937616438\n",
      "iteration number =  3072 Cross Entropy Loss= 70.18016328570005\n",
      "iteration number =  3073 Cross Entropy Loss= 70.17467058823382\n",
      "iteration number =  3074 Cross Entropy Loss= 70.16918128101119\n",
      "iteration number =  3075 Cross Entropy Loss= 70.16369536128087\n",
      "iteration number =  3076 Cross Entropy Loss= 70.15821282629474\n",
      "iteration number =  3077 Cross Entropy Loss= 70.15273367330778\n",
      "iteration number =  3078 Cross Entropy Loss= 70.14725789957821\n",
      "iteration number =  3079 Cross Entropy Loss= 70.1417855023673\n",
      "iteration number =  3080 Cross Entropy Loss= 70.13631647893955\n",
      "iteration number =  3081 Cross Entropy Loss= 70.13085082656252\n",
      "iteration number =  3082 Cross Entropy Loss= 70.12538854250701\n",
      "iteration number =  3083 Cross Entropy Loss= 70.11992962404682\n",
      "iteration number =  3084 Cross Entropy Loss= 70.11447406845895\n",
      "iteration number =  3085 Cross Entropy Loss= 70.10902187302355\n",
      "iteration number =  3086 Cross Entropy Loss= 70.10357303502383\n",
      "iteration number =  3087 Cross Entropy Loss= 70.0981275517461\n",
      "iteration number =  3088 Cross Entropy Loss= 70.09268542047984\n",
      "iteration number =  3089 Cross Entropy Loss= 70.08724663851757\n",
      "iteration number =  3090 Cross Entropy Loss= 70.08181120315494\n",
      "iteration number =  3091 Cross Entropy Loss= 70.0763791116907\n",
      "iteration number =  3092 Cross Entropy Loss= 70.07095036142667\n",
      "iteration number =  3093 Cross Entropy Loss= 70.06552494966776\n",
      "iteration number =  3094 Cross Entropy Loss= 70.06010287372195\n",
      "iteration number =  3095 Cross Entropy Loss= 70.0546841309003\n",
      "iteration number =  3096 Cross Entropy Loss= 70.04926871851696\n",
      "iteration number =  3097 Cross Entropy Loss= 70.04385663388913\n",
      "iteration number =  3098 Cross Entropy Loss= 70.03844787433708\n",
      "iteration number =  3099 Cross Entropy Loss= 70.0330424371841\n",
      "iteration number =  3100 Cross Entropy Loss= 70.02764031975657\n",
      "iteration number =  3101 Cross Entropy Loss= 70.02224151938394\n",
      "iteration number =  3102 Cross Entropy Loss= 70.01684603339864\n",
      "iteration number =  3103 Cross Entropy Loss= 70.01145385913618\n",
      "iteration number =  3104 Cross Entropy Loss= 70.00606499393511\n",
      "iteration number =  3105 Cross Entropy Loss= 70.000679435137\n",
      "iteration number =  3106 Cross Entropy Loss= 69.99529718008641\n",
      "iteration number =  3107 Cross Entropy Loss= 69.98991822613098\n",
      "iteration number =  3108 Cross Entropy Loss= 69.98454257062136\n",
      "iteration number =  3109 Cross Entropy Loss= 69.97917021091118\n",
      "iteration number =  3110 Cross Entropy Loss= 69.97380114435705\n",
      "iteration number =  3111 Cross Entropy Loss= 69.9684353683187\n",
      "iteration number =  3112 Cross Entropy Loss= 69.96307288015873\n",
      "iteration number =  3113 Cross Entropy Loss= 69.95771367724284\n",
      "iteration number =  3114 Cross Entropy Loss= 69.95235775693962\n",
      "iteration number =  3115 Cross Entropy Loss= 69.94700511662074\n",
      "iteration number =  3116 Cross Entropy Loss= 69.9416557536608\n",
      "iteration number =  3117 Cross Entropy Loss= 69.93630966543739\n",
      "iteration number =  3118 Cross Entropy Loss= 69.93096684933107\n",
      "iteration number =  3119 Cross Entropy Loss= 69.92562730272537\n",
      "iteration number =  3120 Cross Entropy Loss= 69.9202910230068\n",
      "iteration number =  3121 Cross Entropy Loss= 69.91495800756479\n",
      "iteration number =  3122 Cross Entropy Loss= 69.90962825379178\n",
      "iteration number =  3123 Cross Entropy Loss= 69.90430175908311\n",
      "iteration number =  3124 Cross Entropy Loss= 69.89897852083713\n",
      "iteration number =  3125 Cross Entropy Loss= 69.89365853645506\n",
      "iteration number =  3126 Cross Entropy Loss= 69.88834180334112\n",
      "iteration number =  3127 Cross Entropy Loss= 69.88302831890243\n",
      "iteration number =  3128 Cross Entropy Loss= 69.87771808054904\n",
      "iteration number =  3129 Cross Entropy Loss= 69.87241108569397\n",
      "iteration number =  3130 Cross Entropy Loss= 69.8671073317531\n",
      "iteration number =  3131 Cross Entropy Loss= 69.86180681614528\n",
      "iteration number =  3132 Cross Entropy Loss= 69.85650953629222\n",
      "iteration number =  3133 Cross Entropy Loss= 69.8512154896186\n",
      "iteration number =  3134 Cross Entropy Loss= 69.84592467355196\n",
      "iteration number =  3135 Cross Entropy Loss= 69.84063708552279\n",
      "iteration number =  3136 Cross Entropy Loss= 69.83535272296439\n",
      "iteration number =  3137 Cross Entropy Loss= 69.83007158331303\n",
      "iteration number =  3138 Cross Entropy Loss= 69.82479366400786\n",
      "iteration number =  3139 Cross Entropy Loss= 69.81951896249089\n",
      "iteration number =  3140 Cross Entropy Loss= 69.81424747620702\n",
      "iteration number =  3141 Cross Entropy Loss= 69.80897920260402\n",
      "iteration number =  3142 Cross Entropy Loss= 69.80371413913254\n",
      "iteration number =  3143 Cross Entropy Loss= 69.7984522832461\n",
      "iteration number =  3144 Cross Entropy Loss= 69.79319363240108\n",
      "iteration number =  3145 Cross Entropy Loss= 69.78793818405673\n",
      "iteration number =  3146 Cross Entropy Loss= 69.78268593567512\n",
      "iteration number =  3147 Cross Entropy Loss= 69.7774368847212\n",
      "iteration number =  3148 Cross Entropy Loss= 69.77219102866275\n",
      "iteration number =  3149 Cross Entropy Loss= 69.76694836497046\n",
      "iteration number =  3150 Cross Entropy Loss= 69.76170889111773\n",
      "iteration number =  3151 Cross Entropy Loss= 69.75647260458095\n",
      "iteration number =  3152 Cross Entropy Loss= 69.75123950283918\n",
      "iteration number =  3153 Cross Entropy Loss= 69.74600958337444\n",
      "iteration number =  3154 Cross Entropy Loss= 69.7407828436715\n",
      "iteration number =  3155 Cross Entropy Loss= 69.73555928121797\n",
      "iteration number =  3156 Cross Entropy Loss= 69.73033889350427\n",
      "iteration number =  3157 Cross Entropy Loss= 69.72512167802364\n",
      "iteration number =  3158 Cross Entropy Loss= 69.71990763227213\n",
      "iteration number =  3159 Cross Entropy Loss= 69.71469675374856\n",
      "iteration number =  3160 Cross Entropy Loss= 69.70948903995456\n",
      "iteration number =  3161 Cross Entropy Loss= 69.7042844883946\n",
      "iteration number =  3162 Cross Entropy Loss= 69.69908309657589\n",
      "iteration number =  3163 Cross Entropy Loss= 69.69388486200845\n",
      "iteration number =  3164 Cross Entropy Loss= 69.68868978220506\n",
      "iteration number =  3165 Cross Entropy Loss= 69.68349785468129\n",
      "iteration number =  3166 Cross Entropy Loss= 69.67830907695551\n",
      "iteration number =  3167 Cross Entropy Loss= 69.67312344654883\n",
      "iteration number =  3168 Cross Entropy Loss= 69.66794096098512\n",
      "iteration number =  3169 Cross Entropy Loss= 69.66276161779103\n",
      "iteration number =  3170 Cross Entropy Loss= 69.657585414496\n",
      "iteration number =  3171 Cross Entropy Loss= 69.65241234863214\n",
      "iteration number =  3172 Cross Entropy Loss= 69.64724241773439\n",
      "iteration number =  3173 Cross Entropy Loss= 69.6420756193404\n",
      "iteration number =  3174 Cross Entropy Loss= 69.63691195099057\n",
      "iteration number =  3175 Cross Entropy Loss= 69.63175141022806\n",
      "iteration number =  3176 Cross Entropy Loss= 69.62659399459872\n",
      "iteration number =  3177 Cross Entropy Loss= 69.62143970165116\n",
      "iteration number =  3178 Cross Entropy Loss= 69.61628852893672\n",
      "iteration number =  3179 Cross Entropy Loss= 69.61114047400946\n",
      "iteration number =  3180 Cross Entropy Loss= 69.60599553442614\n",
      "iteration number =  3181 Cross Entropy Loss= 69.60085370774627\n",
      "iteration number =  3182 Cross Entropy Loss= 69.59571499153208\n",
      "iteration number =  3183 Cross Entropy Loss= 69.59057938334841\n",
      "iteration number =  3184 Cross Entropy Loss= 69.58544688076293\n",
      "iteration number =  3185 Cross Entropy Loss= 69.58031748134594\n",
      "iteration number =  3186 Cross Entropy Loss= 69.57519118267047\n",
      "iteration number =  3187 Cross Entropy Loss= 69.5700679823122\n",
      "iteration number =  3188 Cross Entropy Loss= 69.56494787784955\n",
      "iteration number =  3189 Cross Entropy Loss= 69.55983086686356\n",
      "iteration number =  3190 Cross Entropy Loss= 69.55471694693804\n",
      "iteration number =  3191 Cross Entropy Loss= 69.54960611565939\n",
      "iteration number =  3192 Cross Entropy Loss= 69.54449837061672\n",
      "iteration number =  3193 Cross Entropy Loss= 69.53939370940185\n",
      "iteration number =  3194 Cross Entropy Loss= 69.53429212960918\n",
      "iteration number =  3195 Cross Entropy Loss= 69.52919362883584\n",
      "iteration number =  3196 Cross Entropy Loss= 69.52409820468161\n",
      "iteration number =  3197 Cross Entropy Loss= 69.51900585474888\n",
      "iteration number =  3198 Cross Entropy Loss= 69.51391657664276\n",
      "iteration number =  3199 Cross Entropy Loss= 69.50883036797093\n",
      "iteration number =  3200 Cross Entropy Loss= 69.50374722634376\n",
      "iteration number =  3201 Cross Entropy Loss= 69.49866714937428\n",
      "iteration number =  3202 Cross Entropy Loss= 69.49359013467813\n",
      "iteration number =  3203 Cross Entropy Loss= 69.48851617987356\n",
      "iteration number =  3204 Cross Entropy Loss= 69.48344528258144\n",
      "iteration number =  3205 Cross Entropy Loss= 69.47837744042535\n",
      "iteration number =  3206 Cross Entropy Loss= 69.47331265103139\n",
      "iteration number =  3207 Cross Entropy Loss= 69.46825091202835\n",
      "iteration number =  3208 Cross Entropy Loss= 69.4631922210476\n",
      "iteration number =  3209 Cross Entropy Loss= 69.45813657572312\n",
      "iteration number =  3210 Cross Entropy Loss= 69.45308397369149\n",
      "iteration number =  3211 Cross Entropy Loss= 69.44803441259192\n",
      "iteration number =  3212 Cross Entropy Loss= 69.4429878900662\n",
      "iteration number =  3213 Cross Entropy Loss= 69.43794440375872\n",
      "iteration number =  3214 Cross Entropy Loss= 69.43290395131645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3215 Cross Entropy Loss= 69.42786653038895\n",
      "iteration number =  3216 Cross Entropy Loss= 69.4228321386284\n",
      "iteration number =  3217 Cross Entropy Loss= 69.41780077368952\n",
      "iteration number =  3218 Cross Entropy Loss= 69.4127724332296\n",
      "iteration number =  3219 Cross Entropy Loss= 69.40774711490855\n",
      "iteration number =  3220 Cross Entropy Loss= 69.40272481638878\n",
      "iteration number =  3221 Cross Entropy Loss= 69.39770553533536\n",
      "iteration number =  3222 Cross Entropy Loss= 69.39268926941584\n",
      "iteration number =  3223 Cross Entropy Loss= 69.38767601630036\n",
      "iteration number =  3224 Cross Entropy Loss= 69.38266577366163\n",
      "iteration number =  3225 Cross Entropy Loss= 69.37765853917489\n",
      "iteration number =  3226 Cross Entropy Loss= 69.37265431051792\n",
      "iteration number =  3227 Cross Entropy Loss= 69.36765308537107\n",
      "iteration number =  3228 Cross Entropy Loss= 69.36265486141721\n",
      "iteration number =  3229 Cross Entropy Loss= 69.35765963634178\n",
      "iteration number =  3230 Cross Entropy Loss= 69.35266740783271\n",
      "iteration number =  3231 Cross Entropy Loss= 69.34767817358049\n",
      "iteration number =  3232 Cross Entropy Loss= 69.34269193127813\n",
      "iteration number =  3233 Cross Entropy Loss= 69.33770867862114\n",
      "iteration number =  3234 Cross Entropy Loss= 69.3327284133076\n",
      "iteration number =  3235 Cross Entropy Loss= 69.32775113303808\n",
      "iteration number =  3236 Cross Entropy Loss= 69.32277683551561\n",
      "iteration number =  3237 Cross Entropy Loss= 69.31780551844584\n",
      "iteration number =  3238 Cross Entropy Loss= 69.31283717953683\n",
      "iteration number =  3239 Cross Entropy Loss= 69.30787181649919\n",
      "iteration number =  3240 Cross Entropy Loss= 69.30290942704599\n",
      "iteration number =  3241 Cross Entropy Loss= 69.29795000889284\n",
      "iteration number =  3242 Cross Entropy Loss= 69.29299355975782\n",
      "iteration number =  3243 Cross Entropy Loss= 69.2880400773615\n",
      "iteration number =  3244 Cross Entropy Loss= 69.28308955942691\n",
      "iteration number =  3245 Cross Entropy Loss= 69.27814200367965\n",
      "iteration number =  3246 Cross Entropy Loss= 69.27319740784765\n",
      "iteration number =  3247 Cross Entropy Loss= 69.26825576966145\n",
      "iteration number =  3248 Cross Entropy Loss= 69.263317086854\n",
      "iteration number =  3249 Cross Entropy Loss= 69.25838135716072\n",
      "iteration number =  3250 Cross Entropy Loss= 69.25344857831948\n",
      "iteration number =  3251 Cross Entropy Loss= 69.24851874807067\n",
      "iteration number =  3252 Cross Entropy Loss= 69.24359186415705\n",
      "iteration number =  3253 Cross Entropy Loss= 69.23866792432392\n",
      "iteration number =  3254 Cross Entropy Loss= 69.23374692631897\n",
      "iteration number =  3255 Cross Entropy Loss= 69.22882886789236\n",
      "iteration number =  3256 Cross Entropy Loss= 69.22391374679668\n",
      "iteration number =  3257 Cross Entropy Loss= 69.21900156078699\n",
      "iteration number =  3258 Cross Entropy Loss= 69.21409230762076\n",
      "iteration number =  3259 Cross Entropy Loss= 69.2091859850579\n",
      "iteration number =  3260 Cross Entropy Loss= 69.20428259086074\n",
      "iteration number =  3261 Cross Entropy Loss= 69.19938212279406\n",
      "iteration number =  3262 Cross Entropy Loss= 69.19448457862504\n",
      "iteration number =  3263 Cross Entropy Loss= 69.18958995612329\n",
      "iteration number =  3264 Cross Entropy Loss= 69.18469825306083\n",
      "iteration number =  3265 Cross Entropy Loss= 69.17980946721212\n",
      "iteration number =  3266 Cross Entropy Loss= 69.17492359635398\n",
      "iteration number =  3267 Cross Entropy Loss= 69.1700406382657\n",
      "iteration number =  3268 Cross Entropy Loss= 69.1651605907289\n",
      "iteration number =  3269 Cross Entropy Loss= 69.16028345152765\n",
      "iteration number =  3270 Cross Entropy Loss= 69.15540921844841\n",
      "iteration number =  3271 Cross Entropy Loss= 69.15053788928002\n",
      "iteration number =  3272 Cross Entropy Loss= 69.14566946181371\n",
      "iteration number =  3273 Cross Entropy Loss= 69.14080393384312\n",
      "iteration number =  3274 Cross Entropy Loss= 69.13594130316423\n",
      "iteration number =  3275 Cross Entropy Loss= 69.13108156757544\n",
      "iteration number =  3276 Cross Entropy Loss= 69.12622472487752\n",
      "iteration number =  3277 Cross Entropy Loss= 69.12137077287358\n",
      "iteration number =  3278 Cross Entropy Loss= 69.11651970936913\n",
      "iteration number =  3279 Cross Entropy Loss= 69.11167153217207\n",
      "iteration number =  3280 Cross Entropy Loss= 69.1068262390926\n",
      "iteration number =  3281 Cross Entropy Loss= 69.10198382794333\n",
      "iteration number =  3282 Cross Entropy Loss= 69.0971442965392\n",
      "iteration number =  3283 Cross Entropy Loss= 69.09230764269753\n",
      "iteration number =  3284 Cross Entropy Loss= 69.08747386423798\n",
      "iteration number =  3285 Cross Entropy Loss= 69.08264295898252\n",
      "iteration number =  3286 Cross Entropy Loss= 69.07781492475551\n",
      "iteration number =  3287 Cross Entropy Loss= 69.07298975938366\n",
      "iteration number =  3288 Cross Entropy Loss= 69.06816746069597\n",
      "iteration number =  3289 Cross Entropy Loss= 69.06334802652381\n",
      "iteration number =  3290 Cross Entropy Loss= 69.05853145470086\n",
      "iteration number =  3291 Cross Entropy Loss= 69.05371774306313\n",
      "iteration number =  3292 Cross Entropy Loss= 69.04890688944897\n",
      "iteration number =  3293 Cross Entropy Loss= 69.04409889169906\n",
      "iteration number =  3294 Cross Entropy Loss= 69.03929374765634\n",
      "iteration number =  3295 Cross Entropy Loss= 69.03449145516615\n",
      "iteration number =  3296 Cross Entropy Loss= 69.02969201207607\n",
      "iteration number =  3297 Cross Entropy Loss= 69.02489541623603\n",
      "iteration number =  3298 Cross Entropy Loss= 69.02010166549823\n",
      "iteration number =  3299 Cross Entropy Loss= 69.01531075771722\n",
      "iteration number =  3300 Cross Entropy Loss= 69.0105226907498\n",
      "iteration number =  3301 Cross Entropy Loss= 69.0057374624551\n",
      "iteration number =  3302 Cross Entropy Loss= 69.00095507069454\n",
      "iteration number =  3303 Cross Entropy Loss= 68.9961755133318\n",
      "iteration number =  3304 Cross Entropy Loss= 68.9913987882329\n",
      "iteration number =  3305 Cross Entropy Loss= 68.98662489326608\n",
      "iteration number =  3306 Cross Entropy Loss= 68.9818538263019\n",
      "iteration number =  3307 Cross Entropy Loss= 68.97708558521319\n",
      "iteration number =  3308 Cross Entropy Loss= 68.97232016787505\n",
      "iteration number =  3309 Cross Entropy Loss= 68.96755757216485\n",
      "iteration number =  3310 Cross Entropy Loss= 68.96279779596225\n",
      "iteration number =  3311 Cross Entropy Loss= 68.95804083714914\n",
      "iteration number =  3312 Cross Entropy Loss= 68.95328669360967\n",
      "iteration number =  3313 Cross Entropy Loss= 68.9485353632303\n",
      "iteration number =  3314 Cross Entropy Loss= 68.94378684389969\n",
      "iteration number =  3315 Cross Entropy Loss= 68.9390411335088\n",
      "iteration number =  3316 Cross Entropy Loss= 68.93429822995077\n",
      "iteration number =  3317 Cross Entropy Loss= 68.92955813112107\n",
      "iteration number =  3318 Cross Entropy Loss= 68.92482083491731\n",
      "iteration number =  3319 Cross Entropy Loss= 68.92008633923948\n",
      "iteration number =  3320 Cross Entropy Loss= 68.91535464198968\n",
      "iteration number =  3321 Cross Entropy Loss= 68.91062574107231\n",
      "iteration number =  3322 Cross Entropy Loss= 68.90589963439398\n",
      "iteration number =  3323 Cross Entropy Loss= 68.90117631986351\n",
      "iteration number =  3324 Cross Entropy Loss= 68.896455795392\n",
      "iteration number =  3325 Cross Entropy Loss= 68.89173805889274\n",
      "iteration number =  3326 Cross Entropy Loss= 68.8870231082812\n",
      "iteration number =  3327 Cross Entropy Loss= 68.88231094147511\n",
      "iteration number =  3328 Cross Entropy Loss= 68.87760155639444\n",
      "iteration number =  3329 Cross Entropy Loss= 68.8728949509613\n",
      "iteration number =  3330 Cross Entropy Loss= 68.86819112310006\n",
      "iteration number =  3331 Cross Entropy Loss= 68.86349007073726\n",
      "iteration number =  3332 Cross Entropy Loss= 68.85879179180168\n",
      "iteration number =  3333 Cross Entropy Loss= 68.85409628422423\n",
      "iteration number =  3334 Cross Entropy Loss= 68.84940354593809\n",
      "iteration number =  3335 Cross Entropy Loss= 68.84471357487861\n",
      "iteration number =  3336 Cross Entropy Loss= 68.84002636898327\n",
      "iteration number =  3337 Cross Entropy Loss= 68.83534192619186\n",
      "iteration number =  3338 Cross Entropy Loss= 68.83066024444621\n",
      "iteration number =  3339 Cross Entropy Loss= 68.82598132169042\n",
      "iteration number =  3340 Cross Entropy Loss= 68.82130515587073\n",
      "iteration number =  3341 Cross Entropy Loss= 68.81663174493559\n",
      "iteration number =  3342 Cross Entropy Loss= 68.8119610868356\n",
      "iteration number =  3343 Cross Entropy Loss= 68.80729317952351\n",
      "iteration number =  3344 Cross Entropy Loss= 68.80262802095424\n",
      "iteration number =  3345 Cross Entropy Loss= 68.79796560908493\n",
      "iteration number =  3346 Cross Entropy Loss= 68.79330594187479\n",
      "iteration number =  3347 Cross Entropy Loss= 68.78864901728524\n",
      "iteration number =  3348 Cross Entropy Loss= 68.78399483327985\n",
      "iteration number =  3349 Cross Entropy Loss= 68.77934338782433\n",
      "iteration number =  3350 Cross Entropy Loss= 68.77469467888652\n",
      "iteration number =  3351 Cross Entropy Loss= 68.77004870443645\n",
      "iteration number =  3352 Cross Entropy Loss= 68.76540546244627\n",
      "iteration number =  3353 Cross Entropy Loss= 68.76076495089025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3354 Cross Entropy Loss= 68.75612716774484\n",
      "iteration number =  3355 Cross Entropy Loss= 68.75149211098855\n",
      "iteration number =  3356 Cross Entropy Loss= 68.7468597786021\n",
      "iteration number =  3357 Cross Entropy Loss= 68.74223016856828\n",
      "iteration number =  3358 Cross Entropy Loss= 68.73760327887204\n",
      "iteration number =  3359 Cross Entropy Loss= 68.73297910750046\n",
      "iteration number =  3360 Cross Entropy Loss= 68.72835765244267\n",
      "iteration number =  3361 Cross Entropy Loss= 68.72373891168998\n",
      "iteration number =  3362 Cross Entropy Loss= 68.71912288323581\n",
      "iteration number =  3363 Cross Entropy Loss= 68.71450956507569\n",
      "iteration number =  3364 Cross Entropy Loss= 68.7098989552072\n",
      "iteration number =  3365 Cross Entropy Loss= 68.7052910516301\n",
      "iteration number =  3366 Cross Entropy Loss= 68.7006858523462\n",
      "iteration number =  3367 Cross Entropy Loss= 68.69608335535946\n",
      "iteration number =  3368 Cross Entropy Loss= 68.69148355867586\n",
      "iteration number =  3369 Cross Entropy Loss= 68.68688646030355\n",
      "iteration number =  3370 Cross Entropy Loss= 68.68229205825274\n",
      "iteration number =  3371 Cross Entropy Loss= 68.67770035053572\n",
      "iteration number =  3372 Cross Entropy Loss= 68.67311133516685\n",
      "iteration number =  3373 Cross Entropy Loss= 68.66852501016263\n",
      "iteration number =  3374 Cross Entropy Loss= 68.66394137354155\n",
      "iteration number =  3375 Cross Entropy Loss= 68.65936042332427\n",
      "iteration number =  3376 Cross Entropy Loss= 68.65478215753348\n",
      "iteration number =  3377 Cross Entropy Loss= 68.65020657419393\n",
      "iteration number =  3378 Cross Entropy Loss= 68.64563367133245\n",
      "iteration number =  3379 Cross Entropy Loss= 68.64106344697794\n",
      "iteration number =  3380 Cross Entropy Loss= 68.63649589916136\n",
      "iteration number =  3381 Cross Entropy Loss= 68.63193102591572\n",
      "iteration number =  3382 Cross Entropy Loss= 68.6273688252761\n",
      "iteration number =  3383 Cross Entropy Loss= 68.62280929527964\n",
      "iteration number =  3384 Cross Entropy Loss= 68.61825243396548\n",
      "iteration number =  3385 Cross Entropy Loss= 68.6136982393749\n",
      "iteration number =  3386 Cross Entropy Loss= 68.60914670955115\n",
      "iteration number =  3387 Cross Entropy Loss= 68.60459784253956\n",
      "iteration number =  3388 Cross Entropy Loss= 68.60005163638748\n",
      "iteration number =  3389 Cross Entropy Loss= 68.59550808914429\n",
      "iteration number =  3390 Cross Entropy Loss= 68.59096719886146\n",
      "iteration number =  3391 Cross Entropy Loss= 68.58642896359243\n",
      "iteration number =  3392 Cross Entropy Loss= 68.58189338139269\n",
      "iteration number =  3393 Cross Entropy Loss= 68.57736045031976\n",
      "iteration number =  3394 Cross Entropy Loss= 68.57283016843323\n",
      "iteration number =  3395 Cross Entropy Loss= 68.5683025337946\n",
      "iteration number =  3396 Cross Entropy Loss= 68.56377754446747\n",
      "iteration number =  3397 Cross Entropy Loss= 68.55925519851748\n",
      "iteration number =  3398 Cross Entropy Loss= 68.5547354940122\n",
      "iteration number =  3399 Cross Entropy Loss= 68.5502184290213\n",
      "iteration number =  3400 Cross Entropy Loss= 68.54570400161637\n",
      "iteration number =  3401 Cross Entropy Loss= 68.54119220987108\n",
      "iteration number =  3402 Cross Entropy Loss= 68.53668305186105\n",
      "iteration number =  3403 Cross Entropy Loss= 68.53217652566393\n",
      "iteration number =  3404 Cross Entropy Loss= 68.52767262935936\n",
      "iteration number =  3405 Cross Entropy Loss= 68.52317136102897\n",
      "iteration number =  3406 Cross Entropy Loss= 68.51867271875639\n",
      "iteration number =  3407 Cross Entropy Loss= 68.51417670062723\n",
      "iteration number =  3408 Cross Entropy Loss= 68.5096833047291\n",
      "iteration number =  3409 Cross Entropy Loss= 68.50519252915159\n",
      "iteration number =  3410 Cross Entropy Loss= 68.50070437198625\n",
      "iteration number =  3411 Cross Entropy Loss= 68.49621883132662\n",
      "iteration number =  3412 Cross Entropy Loss= 68.49173590526824\n",
      "iteration number =  3413 Cross Entropy Loss= 68.48725559190862\n",
      "iteration number =  3414 Cross Entropy Loss= 68.4827778893472\n",
      "iteration number =  3415 Cross Entropy Loss= 68.47830279568542\n",
      "iteration number =  3416 Cross Entropy Loss= 68.47383030902671\n",
      "iteration number =  3417 Cross Entropy Loss= 68.46936042747639\n",
      "iteration number =  3418 Cross Entropy Loss= 68.46489314914182\n",
      "iteration number =  3419 Cross Entropy Loss= 68.46042847213226\n",
      "iteration number =  3420 Cross Entropy Loss= 68.45596639455896\n",
      "iteration number =  3421 Cross Entropy Loss= 68.45150691453512\n",
      "iteration number =  3422 Cross Entropy Loss= 68.44705003017584\n",
      "iteration number =  3423 Cross Entropy Loss= 68.44259573959826\n",
      "iteration number =  3424 Cross Entropy Loss= 68.4381440409214\n",
      "iteration number =  3425 Cross Entropy Loss= 68.4336949322662\n",
      "iteration number =  3426 Cross Entropy Loss= 68.42924841175562\n",
      "iteration number =  3427 Cross Entropy Loss= 68.42480447751451\n",
      "iteration number =  3428 Cross Entropy Loss= 68.42036312766965\n",
      "iteration number =  3429 Cross Entropy Loss= 68.41592436034973\n",
      "iteration number =  3430 Cross Entropy Loss= 68.41148817368546\n",
      "iteration number =  3431 Cross Entropy Loss= 68.40705456580936\n",
      "iteration number =  3432 Cross Entropy Loss= 68.40262353485595\n",
      "iteration number =  3433 Cross Entropy Loss= 68.39819507896168\n",
      "iteration number =  3434 Cross Entropy Loss= 68.39376919626487\n",
      "iteration number =  3435 Cross Entropy Loss= 68.38934588490578\n",
      "iteration number =  3436 Cross Entropy Loss= 68.38492514302659\n",
      "iteration number =  3437 Cross Entropy Loss= 68.38050696877139\n",
      "iteration number =  3438 Cross Entropy Loss= 68.37609136028615\n",
      "iteration number =  3439 Cross Entropy Loss= 68.37167831571882\n",
      "iteration number =  3440 Cross Entropy Loss= 68.36726783321916\n",
      "iteration number =  3441 Cross Entropy Loss= 68.36285991093892\n",
      "iteration number =  3442 Cross Entropy Loss= 68.35845454703167\n",
      "iteration number =  3443 Cross Entropy Loss= 68.35405173965296\n",
      "iteration number =  3444 Cross Entropy Loss= 68.34965148696014\n",
      "iteration number =  3445 Cross Entropy Loss= 68.34525378711254\n",
      "iteration number =  3446 Cross Entropy Loss= 68.34085863827131\n",
      "iteration number =  3447 Cross Entropy Loss= 68.33646603859953\n",
      "iteration number =  3448 Cross Entropy Loss= 68.33207598626217\n",
      "iteration number =  3449 Cross Entropy Loss= 68.32768847942603\n",
      "iteration number =  3450 Cross Entropy Loss= 68.32330351625987\n",
      "iteration number =  3451 Cross Entropy Loss= 68.31892109493424\n",
      "iteration number =  3452 Cross Entropy Loss= 68.31454121362161\n",
      "iteration number =  3453 Cross Entropy Loss= 68.31016387049632\n",
      "iteration number =  3454 Cross Entropy Loss= 68.30578906373461\n",
      "iteration number =  3455 Cross Entropy Loss= 68.3014167915145\n",
      "iteration number =  3456 Cross Entropy Loss= 68.29704705201596\n",
      "iteration number =  3457 Cross Entropy Loss= 68.29267984342079\n",
      "iteration number =  3458 Cross Entropy Loss= 68.28831516391261\n",
      "iteration number =  3459 Cross Entropy Loss= 68.28395301167703\n",
      "iteration number =  3460 Cross Entropy Loss= 68.27959338490132\n",
      "iteration number =  3461 Cross Entropy Loss= 68.27523628177477\n",
      "iteration number =  3462 Cross Entropy Loss= 68.27088170048843\n",
      "iteration number =  3463 Cross Entropy Loss= 68.26652963923524\n",
      "iteration number =  3464 Cross Entropy Loss= 68.26218009620996\n",
      "iteration number =  3465 Cross Entropy Loss= 68.2578330696092\n",
      "iteration number =  3466 Cross Entropy Loss= 68.2534885576314\n",
      "iteration number =  3467 Cross Entropy Loss= 68.24914655847687\n",
      "iteration number =  3468 Cross Entropy Loss= 68.24480707034772\n",
      "iteration number =  3469 Cross Entropy Loss= 68.24047009144792\n",
      "iteration number =  3470 Cross Entropy Loss= 68.23613561998326\n",
      "iteration number =  3471 Cross Entropy Loss= 68.23180365416135\n",
      "iteration number =  3472 Cross Entropy Loss= 68.22747419219164\n",
      "iteration number =  3473 Cross Entropy Loss= 68.2231472322854\n",
      "iteration number =  3474 Cross Entropy Loss= 68.21882277265571\n",
      "iteration number =  3475 Cross Entropy Loss= 68.21450081151748\n",
      "iteration number =  3476 Cross Entropy Loss= 68.21018134708746\n",
      "iteration number =  3477 Cross Entropy Loss= 68.20586437758413\n",
      "iteration number =  3478 Cross Entropy Loss= 68.2015499012279\n",
      "iteration number =  3479 Cross Entropy Loss= 68.19723791624091\n",
      "iteration number =  3480 Cross Entropy Loss= 68.19292842084712\n",
      "iteration number =  3481 Cross Entropy Loss= 68.1886214132723\n",
      "iteration number =  3482 Cross Entropy Loss= 68.18431689174405\n",
      "iteration number =  3483 Cross Entropy Loss= 68.1800148544917\n",
      "iteration number =  3484 Cross Entropy Loss= 68.17571529974647\n",
      "iteration number =  3485 Cross Entropy Loss= 68.17141822574129\n",
      "iteration number =  3486 Cross Entropy Loss= 68.16712363071095\n",
      "iteration number =  3487 Cross Entropy Loss= 68.16283151289198\n",
      "iteration number =  3488 Cross Entropy Loss= 68.15854187052274\n",
      "iteration number =  3489 Cross Entropy Loss= 68.15425470184334\n",
      "iteration number =  3490 Cross Entropy Loss= 68.1499700050957\n",
      "iteration number =  3491 Cross Entropy Loss= 68.14568777852348\n",
      "iteration number =  3492 Cross Entropy Loss= 68.14140802037218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3493 Cross Entropy Loss= 68.13713072888908\n",
      "iteration number =  3494 Cross Entropy Loss= 68.13285590232314\n",
      "iteration number =  3495 Cross Entropy Loss= 68.12858353892517\n",
      "iteration number =  3496 Cross Entropy Loss= 68.12431363694778\n",
      "iteration number =  3497 Cross Entropy Loss= 68.12004619464523\n",
      "iteration number =  3498 Cross Entropy Loss= 68.11578121027367\n",
      "iteration number =  3499 Cross Entropy Loss= 68.11151868209092\n",
      "iteration number =  3500 Cross Entropy Loss= 68.10725860835664\n",
      "iteration number =  3501 Cross Entropy Loss= 68.1030009873322\n",
      "iteration number =  3502 Cross Entropy Loss= 68.09874581728072\n",
      "iteration number =  3503 Cross Entropy Loss= 68.09449309646709\n",
      "iteration number =  3504 Cross Entropy Loss= 68.09024282315795\n",
      "iteration number =  3505 Cross Entropy Loss= 68.08599499562172\n",
      "iteration number =  3506 Cross Entropy Loss= 68.08174961212852\n",
      "iteration number =  3507 Cross Entropy Loss= 68.07750667095021\n",
      "iteration number =  3508 Cross Entropy Loss= 68.07326617036045\n",
      "iteration number =  3509 Cross Entropy Loss= 68.06902810863458\n",
      "iteration number =  3510 Cross Entropy Loss= 68.06479248404972\n",
      "iteration number =  3511 Cross Entropy Loss= 68.06055929488471\n",
      "iteration number =  3512 Cross Entropy Loss= 68.05632853942011\n",
      "iteration number =  3513 Cross Entropy Loss= 68.05210021593824\n",
      "iteration number =  3514 Cross Entropy Loss= 68.04787432272313\n",
      "iteration number =  3515 Cross Entropy Loss= 68.04365085806053\n",
      "iteration number =  3516 Cross Entropy Loss= 68.03942982023796\n",
      "iteration number =  3517 Cross Entropy Loss= 68.03521120754456\n",
      "iteration number =  3518 Cross Entropy Loss= 68.03099501827134\n",
      "iteration number =  3519 Cross Entropy Loss= 68.0267812507109\n",
      "iteration number =  3520 Cross Entropy Loss= 68.0225699031576\n",
      "iteration number =  3521 Cross Entropy Loss= 68.01836097390753\n",
      "iteration number =  3522 Cross Entropy Loss= 68.0141544612585\n",
      "iteration number =  3523 Cross Entropy Loss= 68.00995036350996\n",
      "iteration number =  3524 Cross Entropy Loss= 68.00574867896314\n",
      "iteration number =  3525 Cross Entropy Loss= 68.00154940592097\n",
      "iteration number =  3526 Cross Entropy Loss= 67.99735254268802\n",
      "iteration number =  3527 Cross Entropy Loss= 67.99315808757063\n",
      "iteration number =  3528 Cross Entropy Loss= 67.9889660388768\n",
      "iteration number =  3529 Cross Entropy Loss= 67.98477639491625\n",
      "iteration number =  3530 Cross Entropy Loss= 67.98058915400034\n",
      "iteration number =  3531 Cross Entropy Loss= 67.97640431444223\n",
      "iteration number =  3532 Cross Entropy Loss= 67.97222187455665\n",
      "iteration number =  3533 Cross Entropy Loss= 67.9680418326601\n",
      "iteration number =  3534 Cross Entropy Loss= 67.96386418707073\n",
      "iteration number =  3535 Cross Entropy Loss= 67.95968893610834\n",
      "iteration number =  3536 Cross Entropy Loss= 67.95551607809449\n",
      "iteration number =  3537 Cross Entropy Loss= 67.95134561135238\n",
      "iteration number =  3538 Cross Entropy Loss= 67.94717753420686\n",
      "iteration number =  3539 Cross Entropy Loss= 67.9430118449845\n",
      "iteration number =  3540 Cross Entropy Loss= 67.9388485420135\n",
      "iteration number =  3541 Cross Entropy Loss= 67.93468762362379\n",
      "iteration number =  3542 Cross Entropy Loss= 67.93052908814688\n",
      "iteration number =  3543 Cross Entropy Loss= 67.92637293391604\n",
      "iteration number =  3544 Cross Entropy Loss= 67.92221915926612\n",
      "iteration number =  3545 Cross Entropy Loss= 67.91806776253372\n",
      "iteration number =  3546 Cross Entropy Loss= 67.91391874205702\n",
      "iteration number =  3547 Cross Entropy Loss= 67.90977209617589\n",
      "iteration number =  3548 Cross Entropy Loss= 67.90562782323185\n",
      "iteration number =  3549 Cross Entropy Loss= 67.9014859215681\n",
      "iteration number =  3550 Cross Entropy Loss= 67.89734638952945\n",
      "iteration number =  3551 Cross Entropy Loss= 67.89320922546237\n",
      "iteration number =  3552 Cross Entropy Loss= 67.889074427715\n",
      "iteration number =  3553 Cross Entropy Loss= 67.88494199463712\n",
      "iteration number =  3554 Cross Entropy Loss= 67.88081192458012\n",
      "iteration number =  3555 Cross Entropy Loss= 67.87668421589706\n",
      "iteration number =  3556 Cross Entropy Loss= 67.87255886694263\n",
      "iteration number =  3557 Cross Entropy Loss= 67.86843587607316\n",
      "iteration number =  3558 Cross Entropy Loss= 67.86431524164662\n",
      "iteration number =  3559 Cross Entropy Loss= 67.86019696202258\n",
      "iteration number =  3560 Cross Entropy Loss= 67.85608103556231\n",
      "iteration number =  3561 Cross Entropy Loss= 67.85196746062863\n",
      "iteration number =  3562 Cross Entropy Loss= 67.84785623558602\n",
      "iteration number =  3563 Cross Entropy Loss= 67.84374735880058\n",
      "iteration number =  3564 Cross Entropy Loss= 67.83964082864004\n",
      "iteration number =  3565 Cross Entropy Loss= 67.83553664347376\n",
      "iteration number =  3566 Cross Entropy Loss= 67.83143480167269\n",
      "iteration number =  3567 Cross Entropy Loss= 67.8273353016094\n",
      "iteration number =  3568 Cross Entropy Loss= 67.82323814165808\n",
      "iteration number =  3569 Cross Entropy Loss= 67.81914332019457\n",
      "iteration number =  3570 Cross Entropy Loss= 67.81505083559625\n",
      "iteration number =  3571 Cross Entropy Loss= 67.81096068624214\n",
      "iteration number =  3572 Cross Entropy Loss= 67.80687287051289\n",
      "iteration number =  3573 Cross Entropy Loss= 67.8027873867907\n",
      "iteration number =  3574 Cross Entropy Loss= 67.79870423345943\n",
      "iteration number =  3575 Cross Entropy Loss= 67.79462340890451\n",
      "iteration number =  3576 Cross Entropy Loss= 67.79054491151295\n",
      "iteration number =  3577 Cross Entropy Loss= 67.7864687396734\n",
      "iteration number =  3578 Cross Entropy Loss= 67.78239489177605\n",
      "iteration number =  3579 Cross Entropy Loss= 67.7783233662127\n",
      "iteration number =  3580 Cross Entropy Loss= 67.7742541613768\n",
      "iteration number =  3581 Cross Entropy Loss= 67.77018727566329\n",
      "iteration number =  3582 Cross Entropy Loss= 67.76612270746875\n",
      "iteration number =  3583 Cross Entropy Loss= 67.76206045519135\n",
      "iteration number =  3584 Cross Entropy Loss= 67.75800051723081\n",
      "iteration number =  3585 Cross Entropy Loss= 67.75394289198846\n",
      "iteration number =  3586 Cross Entropy Loss= 67.74988757786718\n",
      "iteration number =  3587 Cross Entropy Loss= 67.74583457327142\n",
      "iteration number =  3588 Cross Entropy Loss= 67.74178387660727\n",
      "iteration number =  3589 Cross Entropy Loss= 67.7377354862823\n",
      "iteration number =  3590 Cross Entropy Loss= 67.73368940070571\n",
      "iteration number =  3591 Cross Entropy Loss= 67.72964561828826\n",
      "iteration number =  3592 Cross Entropy Loss= 67.72560413744223\n",
      "iteration number =  3593 Cross Entropy Loss= 67.72156495658153\n",
      "iteration number =  3594 Cross Entropy Loss= 67.71752807412159\n",
      "iteration number =  3595 Cross Entropy Loss= 67.71349348847941\n",
      "iteration number =  3596 Cross Entropy Loss= 67.70946119807354\n",
      "iteration number =  3597 Cross Entropy Loss= 67.70543120132412\n",
      "iteration number =  3598 Cross Entropy Loss= 67.70140349665277\n",
      "iteration number =  3599 Cross Entropy Loss= 67.69737808248274\n",
      "iteration number =  3600 Cross Entropy Loss= 67.69335495723878\n",
      "iteration number =  3601 Cross Entropy Loss= 67.68933411934724\n",
      "iteration number =  3602 Cross Entropy Loss= 67.68531556723593\n",
      "iteration number =  3603 Cross Entropy Loss= 67.68129929933431\n",
      "iteration number =  3604 Cross Entropy Loss= 67.67728531407329\n",
      "iteration number =  3605 Cross Entropy Loss= 67.67327360988536\n",
      "iteration number =  3606 Cross Entropy Loss= 67.66926418520455\n",
      "iteration number =  3607 Cross Entropy Loss= 67.66525703846642\n",
      "iteration number =  3608 Cross Entropy Loss= 67.66125216810809\n",
      "iteration number =  3609 Cross Entropy Loss= 67.65724957256816\n",
      "iteration number =  3610 Cross Entropy Loss= 67.65324925028679\n",
      "iteration number =  3611 Cross Entropy Loss= 67.64925119970567\n",
      "iteration number =  3612 Cross Entropy Loss= 67.64525541926803\n",
      "iteration number =  3613 Cross Entropy Loss= 67.6412619074186\n",
      "iteration number =  3614 Cross Entropy Loss= 67.63727066260361\n",
      "iteration number =  3615 Cross Entropy Loss= 67.6332816832709\n",
      "iteration number =  3616 Cross Entropy Loss= 67.62929496786971\n",
      "iteration number =  3617 Cross Entropy Loss= 67.6253105148509\n",
      "iteration number =  3618 Cross Entropy Loss= 67.62132832266678\n",
      "iteration number =  3619 Cross Entropy Loss= 67.61734838977121\n",
      "iteration number =  3620 Cross Entropy Loss= 67.61337071461955\n",
      "iteration number =  3621 Cross Entropy Loss= 67.60939529566863\n",
      "iteration number =  3622 Cross Entropy Loss= 67.60542213137688\n",
      "iteration number =  3623 Cross Entropy Loss= 67.60145122020415\n",
      "iteration number =  3624 Cross Entropy Loss= 67.59748256061184\n",
      "iteration number =  3625 Cross Entropy Loss= 67.5935161510628\n",
      "iteration number =  3626 Cross Entropy Loss= 67.58955199002143\n",
      "iteration number =  3627 Cross Entropy Loss= 67.58559007595365\n",
      "iteration number =  3628 Cross Entropy Loss= 67.5816304073268\n",
      "iteration number =  3629 Cross Entropy Loss= 67.57767298260976\n",
      "iteration number =  3630 Cross Entropy Loss= 67.5737178002729\n",
      "iteration number =  3631 Cross Entropy Loss= 67.56976485878809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3632 Cross Entropy Loss= 67.56581415662868\n",
      "iteration number =  3633 Cross Entropy Loss= 67.56186569226946\n",
      "iteration number =  3634 Cross Entropy Loss= 67.55791946418682\n",
      "iteration number =  3635 Cross Entropy Loss= 67.55397547085849\n",
      "iteration number =  3636 Cross Entropy Loss= 67.55003371076381\n",
      "iteration number =  3637 Cross Entropy Loss= 67.54609418238351\n",
      "iteration number =  3638 Cross Entropy Loss= 67.54215688419984\n",
      "iteration number =  3639 Cross Entropy Loss= 67.53822181469653\n",
      "iteration number =  3640 Cross Entropy Loss= 67.53428897235872\n",
      "iteration number =  3641 Cross Entropy Loss= 67.53035835567314\n",
      "iteration number =  3642 Cross Entropy Loss= 67.5264299631279\n",
      "iteration number =  3643 Cross Entropy Loss= 67.52250379321256\n",
      "iteration number =  3644 Cross Entropy Loss= 67.51857984441826\n",
      "iteration number =  3645 Cross Entropy Loss= 67.5146581152375\n",
      "iteration number =  3646 Cross Entropy Loss= 67.51073860416426\n",
      "iteration number =  3647 Cross Entropy Loss= 67.50682130969402\n",
      "iteration number =  3648 Cross Entropy Loss= 67.50290623032367\n",
      "iteration number =  3649 Cross Entropy Loss= 67.49899336455164\n",
      "iteration number =  3650 Cross Entropy Loss= 67.49508271087771\n",
      "iteration number =  3651 Cross Entropy Loss= 67.49117426780319\n",
      "iteration number =  3652 Cross Entropy Loss= 67.48726803383079\n",
      "iteration number =  3653 Cross Entropy Loss= 67.48336400746473\n",
      "iteration number =  3654 Cross Entropy Loss= 67.47946218721064\n",
      "iteration number =  3655 Cross Entropy Loss= 67.4755625715756\n",
      "iteration number =  3656 Cross Entropy Loss= 67.47166515906812\n",
      "iteration number =  3657 Cross Entropy Loss= 67.4677699481982\n",
      "iteration number =  3658 Cross Entropy Loss= 67.46387693747724\n",
      "iteration number =  3659 Cross Entropy Loss= 67.45998612541808\n",
      "iteration number =  3660 Cross Entropy Loss= 67.45609751053505\n",
      "iteration number =  3661 Cross Entropy Loss= 67.45221109134383\n",
      "iteration number =  3662 Cross Entropy Loss= 67.44832686636161\n",
      "iteration number =  3663 Cross Entropy Loss= 67.44444483410697\n",
      "iteration number =  3664 Cross Entropy Loss= 67.44056499309995\n",
      "iteration number =  3665 Cross Entropy Loss= 67.43668734186198\n",
      "iteration number =  3666 Cross Entropy Loss= 67.43281187891598\n",
      "iteration number =  3667 Cross Entropy Loss= 67.42893860278623\n",
      "iteration number =  3668 Cross Entropy Loss= 67.42506751199845\n",
      "iteration number =  3669 Cross Entropy Loss= 67.42119860507981\n",
      "iteration number =  3670 Cross Entropy Loss= 67.41733188055889\n",
      "iteration number =  3671 Cross Entropy Loss= 67.41346733696567\n",
      "iteration number =  3672 Cross Entropy Loss= 67.40960497283154\n",
      "iteration number =  3673 Cross Entropy Loss= 67.40574478668935\n",
      "iteration number =  3674 Cross Entropy Loss= 67.40188677707334\n",
      "iteration number =  3675 Cross Entropy Loss= 67.39803094251914\n",
      "iteration number =  3676 Cross Entropy Loss= 67.3941772815638\n",
      "iteration number =  3677 Cross Entropy Loss= 67.3903257927458\n",
      "iteration number =  3678 Cross Entropy Loss= 67.386476474605\n",
      "iteration number =  3679 Cross Entropy Loss= 67.38262932568271\n",
      "iteration number =  3680 Cross Entropy Loss= 67.37878434452158\n",
      "iteration number =  3681 Cross Entropy Loss= 67.3749415296657\n",
      "iteration number =  3682 Cross Entropy Loss= 67.37110087966053\n",
      "iteration number =  3683 Cross Entropy Loss= 67.36726239305298\n",
      "iteration number =  3684 Cross Entropy Loss= 67.3634260683913\n",
      "iteration number =  3685 Cross Entropy Loss= 67.35959190422517\n",
      "iteration number =  3686 Cross Entropy Loss= 67.35575989910565\n",
      "iteration number =  3687 Cross Entropy Loss= 67.35193005158519\n",
      "iteration number =  3688 Cross Entropy Loss= 67.3481023602176\n",
      "iteration number =  3689 Cross Entropy Loss= 67.34427682355816\n",
      "iteration number =  3690 Cross Entropy Loss= 67.34045344016346\n",
      "iteration number =  3691 Cross Entropy Loss= 67.3366322085915\n",
      "iteration number =  3692 Cross Entropy Loss= 67.33281312740164\n",
      "iteration number =  3693 Cross Entropy Loss= 67.32899619515466\n",
      "iteration number =  3694 Cross Entropy Loss= 67.32518141041271\n",
      "iteration number =  3695 Cross Entropy Loss= 67.32136877173927\n",
      "iteration number =  3696 Cross Entropy Loss= 67.31755827769928\n",
      "iteration number =  3697 Cross Entropy Loss= 67.31374992685897\n",
      "iteration number =  3698 Cross Entropy Loss= 67.309943717786\n",
      "iteration number =  3699 Cross Entropy Loss= 67.30613964904936\n",
      "iteration number =  3700 Cross Entropy Loss= 67.30233771921942\n",
      "iteration number =  3701 Cross Entropy Loss= 67.29853792686798\n",
      "iteration number =  3702 Cross Entropy Loss= 67.29474027056808\n",
      "iteration number =  3703 Cross Entropy Loss= 67.29094474889425\n",
      "iteration number =  3704 Cross Entropy Loss= 67.28715136042229\n",
      "iteration number =  3705 Cross Entropy Loss= 67.2833601037294\n",
      "iteration number =  3706 Cross Entropy Loss= 67.27957097739414\n",
      "iteration number =  3707 Cross Entropy Loss= 67.27578397999645\n",
      "iteration number =  3708 Cross Entropy Loss= 67.27199911011756\n",
      "iteration number =  3709 Cross Entropy Loss= 67.26821636634011\n",
      "iteration number =  3710 Cross Entropy Loss= 67.26443574724809\n",
      "iteration number =  3711 Cross Entropy Loss= 67.26065725142678\n",
      "iteration number =  3712 Cross Entropy Loss= 67.25688087746289\n",
      "iteration number =  3713 Cross Entropy Loss= 67.25310662394443\n",
      "iteration number =  3714 Cross Entropy Loss= 67.24933448946078\n",
      "iteration number =  3715 Cross Entropy Loss= 67.24556447260264\n",
      "iteration number =  3716 Cross Entropy Loss= 67.24179657196204\n",
      "iteration number =  3717 Cross Entropy Loss= 67.23803078613241\n",
      "iteration number =  3718 Cross Entropy Loss= 67.23426711370848\n",
      "iteration number =  3719 Cross Entropy Loss= 67.23050555328629\n",
      "iteration number =  3720 Cross Entropy Loss= 67.22674610346328\n",
      "iteration number =  3721 Cross Entropy Loss= 67.22298876283817\n",
      "iteration number =  3722 Cross Entropy Loss= 67.21923353001102\n",
      "iteration number =  3723 Cross Entropy Loss= 67.21548040358329\n",
      "iteration number =  3724 Cross Entropy Loss= 67.21172938215764\n",
      "iteration number =  3725 Cross Entropy Loss= 67.20798046433818\n",
      "iteration number =  3726 Cross Entropy Loss= 67.2042336487303\n",
      "iteration number =  3727 Cross Entropy Loss= 67.20048893394068\n",
      "iteration number =  3728 Cross Entropy Loss= 67.19674631857735\n",
      "iteration number =  3729 Cross Entropy Loss= 67.1930058012497\n",
      "iteration number =  3730 Cross Entropy Loss= 67.1892673805684\n",
      "iteration number =  3731 Cross Entropy Loss= 67.18553105514542\n",
      "iteration number =  3732 Cross Entropy Loss= 67.18179682359408\n",
      "iteration number =  3733 Cross Entropy Loss= 67.17806468452902\n",
      "iteration number =  3734 Cross Entropy Loss= 67.17433463656616\n",
      "iteration number =  3735 Cross Entropy Loss= 67.17060667832274\n",
      "iteration number =  3736 Cross Entropy Loss= 67.16688080841735\n",
      "iteration number =  3737 Cross Entropy Loss= 67.16315702546984\n",
      "iteration number =  3738 Cross Entropy Loss= 67.15943532810138\n",
      "iteration number =  3739 Cross Entropy Loss= 67.15571571493447\n",
      "iteration number =  3740 Cross Entropy Loss= 67.15199818459288\n",
      "iteration number =  3741 Cross Entropy Loss= 67.1482827357017\n",
      "iteration number =  3742 Cross Entropy Loss= 67.14456936688731\n",
      "iteration number =  3743 Cross Entropy Loss= 67.1408580767774\n",
      "iteration number =  3744 Cross Entropy Loss= 67.13714886400096\n",
      "iteration number =  3745 Cross Entropy Loss= 67.13344172718828\n",
      "iteration number =  3746 Cross Entropy Loss= 67.1297366649709\n",
      "iteration number =  3747 Cross Entropy Loss= 67.12603367598172\n",
      "iteration number =  3748 Cross Entropy Loss= 67.12233275885487\n",
      "iteration number =  3749 Cross Entropy Loss= 67.11863391222585\n",
      "iteration number =  3750 Cross Entropy Loss= 67.1149371347313\n",
      "iteration number =  3751 Cross Entropy Loss= 67.11124242500934\n",
      "iteration number =  3752 Cross Entropy Loss= 67.10754978169922\n",
      "iteration number =  3753 Cross Entropy Loss= 67.10385920344156\n",
      "iteration number =  3754 Cross Entropy Loss= 67.1001706888782\n",
      "iteration number =  3755 Cross Entropy Loss= 67.09648423665232\n",
      "iteration number =  3756 Cross Entropy Loss= 67.09279984540834\n",
      "iteration number =  3757 Cross Entropy Loss= 67.08911751379198\n",
      "iteration number =  3758 Cross Entropy Loss= 67.0854372404502\n",
      "iteration number =  3759 Cross Entropy Loss= 67.0817590240313\n",
      "iteration number =  3760 Cross Entropy Loss= 67.07808286318478\n",
      "iteration number =  3761 Cross Entropy Loss= 67.07440875656145\n",
      "iteration number =  3762 Cross Entropy Loss= 67.07073670281338\n",
      "iteration number =  3763 Cross Entropy Loss= 67.06706670059393\n",
      "iteration number =  3764 Cross Entropy Loss= 67.06339874855766\n",
      "iteration number =  3765 Cross Entropy Loss= 67.05973284536047\n",
      "iteration number =  3766 Cross Entropy Loss= 67.05606898965951\n",
      "iteration number =  3767 Cross Entropy Loss= 67.05240718011316\n",
      "iteration number =  3768 Cross Entropy Loss= 67.04874741538109\n",
      "iteration number =  3769 Cross Entropy Loss= 67.04508969412419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3770 Cross Entropy Loss= 67.04143401500465\n",
      "iteration number =  3771 Cross Entropy Loss= 67.03778037668593\n",
      "iteration number =  3772 Cross Entropy Loss= 67.03412877783268\n",
      "iteration number =  3773 Cross Entropy Loss= 67.03047921711081\n",
      "iteration number =  3774 Cross Entropy Loss= 67.02683169318757\n",
      "iteration number =  3775 Cross Entropy Loss= 67.02318620473136\n",
      "iteration number =  3776 Cross Entropy Loss= 67.01954275041189\n",
      "iteration number =  3777 Cross Entropy Loss= 67.01590132890007\n",
      "iteration number =  3778 Cross Entropy Loss= 67.01226193886811\n",
      "iteration number =  3779 Cross Entropy Loss= 67.00862457898938\n",
      "iteration number =  3780 Cross Entropy Loss= 67.00498924793862\n",
      "iteration number =  3781 Cross Entropy Loss= 67.00135594439166\n",
      "iteration number =  3782 Cross Entropy Loss= 66.99772466702572\n",
      "iteration number =  3783 Cross Entropy Loss= 66.99409541451911\n",
      "iteration number =  3784 Cross Entropy Loss= 66.99046818555152\n",
      "iteration number =  3785 Cross Entropy Loss= 66.98684297880375\n",
      "iteration number =  3786 Cross Entropy Loss= 66.98321979295793\n",
      "iteration number =  3787 Cross Entropy Loss= 66.97959862669737\n",
      "iteration number =  3788 Cross Entropy Loss= 66.97597947870659\n",
      "iteration number =  3789 Cross Entropy Loss= 66.9723623476714\n",
      "iteration number =  3790 Cross Entropy Loss= 66.96874723227884\n",
      "iteration number =  3791 Cross Entropy Loss= 66.9651341312171\n",
      "iteration number =  3792 Cross Entropy Loss= 66.96152304317565\n",
      "iteration number =  3793 Cross Entropy Loss= 66.95791396684515\n",
      "iteration number =  3794 Cross Entropy Loss= 66.95430690091754\n",
      "iteration number =  3795 Cross Entropy Loss= 66.95070184408596\n",
      "iteration number =  3796 Cross Entropy Loss= 66.94709879504471\n",
      "iteration number =  3797 Cross Entropy Loss= 66.94349775248936\n",
      "iteration number =  3798 Cross Entropy Loss= 66.93989871511671\n",
      "iteration number =  3799 Cross Entropy Loss= 66.93630168162474\n",
      "iteration number =  3800 Cross Entropy Loss= 66.93270665071265\n",
      "iteration number =  3801 Cross Entropy Loss= 66.92911362108086\n",
      "iteration number =  3802 Cross Entropy Loss= 66.92552259143102\n",
      "iteration number =  3803 Cross Entropy Loss= 66.92193356046592\n",
      "iteration number =  3804 Cross Entropy Loss= 66.91834652688962\n",
      "iteration number =  3805 Cross Entropy Loss= 66.9147614894074\n",
      "iteration number =  3806 Cross Entropy Loss= 66.91117844672569\n",
      "iteration number =  3807 Cross Entropy Loss= 66.90759739755214\n",
      "iteration number =  3808 Cross Entropy Loss= 66.9040183405956\n",
      "iteration number =  3809 Cross Entropy Loss= 66.90044127456616\n",
      "iteration number =  3810 Cross Entropy Loss= 66.89686619817506\n",
      "iteration number =  3811 Cross Entropy Loss= 66.89329311013475\n",
      "iteration number =  3812 Cross Entropy Loss= 66.8897220091589\n",
      "iteration number =  3813 Cross Entropy Loss= 66.88615289396233\n",
      "iteration number =  3814 Cross Entropy Loss= 66.88258576326109\n",
      "iteration number =  3815 Cross Entropy Loss= 66.8790206157724\n",
      "iteration number =  3816 Cross Entropy Loss= 66.87545745021471\n",
      "iteration number =  3817 Cross Entropy Loss= 66.87189626530758\n",
      "iteration number =  3818 Cross Entropy Loss= 66.86833705977185\n",
      "iteration number =  3819 Cross Entropy Loss= 66.86477983232948\n",
      "iteration number =  3820 Cross Entropy Loss= 66.86122458170364\n",
      "iteration number =  3821 Cross Entropy Loss= 66.85767130661868\n",
      "iteration number =  3822 Cross Entropy Loss= 66.85412000580014\n",
      "iteration number =  3823 Cross Entropy Loss= 66.85057067797472\n",
      "iteration number =  3824 Cross Entropy Loss= 66.84702332187032\n",
      "iteration number =  3825 Cross Entropy Loss= 66.843477936216\n",
      "iteration number =  3826 Cross Entropy Loss= 66.83993451974202\n",
      "iteration number =  3827 Cross Entropy Loss= 66.83639307117978\n",
      "iteration number =  3828 Cross Entropy Loss= 66.8328535892619\n",
      "iteration number =  3829 Cross Entropy Loss= 66.82931607272212\n",
      "iteration number =  3830 Cross Entropy Loss= 66.82578052029538\n",
      "iteration number =  3831 Cross Entropy Loss= 66.8222469307178\n",
      "iteration number =  3832 Cross Entropy Loss= 66.81871530272663\n",
      "iteration number =  3833 Cross Entropy Loss= 66.81518563506035\n",
      "iteration number =  3834 Cross Entropy Loss= 66.81165792645851\n",
      "iteration number =  3835 Cross Entropy Loss= 66.80813217566192\n",
      "iteration number =  3836 Cross Entropy Loss= 66.8046083814125\n",
      "iteration number =  3837 Cross Entropy Loss= 66.80108654245335\n",
      "iteration number =  3838 Cross Entropy Loss= 66.7975666575287\n",
      "iteration number =  3839 Cross Entropy Loss= 66.79404872538397\n",
      "iteration number =  3840 Cross Entropy Loss= 66.79053274476573\n",
      "iteration number =  3841 Cross Entropy Loss= 66.7870187144217\n",
      "iteration number =  3842 Cross Entropy Loss= 66.78350663310077\n",
      "iteration number =  3843 Cross Entropy Loss= 66.77999649955295\n",
      "iteration number =  3844 Cross Entropy Loss= 66.77648831252944\n",
      "iteration number =  3845 Cross Entropy Loss= 66.77298207078256\n",
      "iteration number =  3846 Cross Entropy Loss= 66.76947777306577\n",
      "iteration number =  3847 Cross Entropy Loss= 66.76597541813375\n",
      "iteration number =  3848 Cross Entropy Loss= 66.76247500474221\n",
      "iteration number =  3849 Cross Entropy Loss= 66.75897653164814\n",
      "iteration number =  3850 Cross Entropy Loss= 66.75547999760954\n",
      "iteration number =  3851 Cross Entropy Loss= 66.75198540138564\n",
      "iteration number =  3852 Cross Entropy Loss= 66.74849274173678\n",
      "iteration number =  3853 Cross Entropy Loss= 66.74500201742444\n",
      "iteration number =  3854 Cross Entropy Loss= 66.74151322721126\n",
      "iteration number =  3855 Cross Entropy Loss= 66.73802636986096\n",
      "iteration number =  3856 Cross Entropy Loss= 66.73454144413849\n",
      "iteration number =  3857 Cross Entropy Loss= 66.73105844880983\n",
      "iteration number =  3858 Cross Entropy Loss= 66.72757738264217\n",
      "iteration number =  3859 Cross Entropy Loss= 66.72409824440378\n",
      "iteration number =  3860 Cross Entropy Loss= 66.72062103286409\n",
      "iteration number =  3861 Cross Entropy Loss= 66.71714574679366\n",
      "iteration number =  3862 Cross Entropy Loss= 66.71367238496416\n",
      "iteration number =  3863 Cross Entropy Loss= 66.71020094614839\n",
      "iteration number =  3864 Cross Entropy Loss= 66.7067314291203\n",
      "iteration number =  3865 Cross Entropy Loss= 66.70326383265491\n",
      "iteration number =  3866 Cross Entropy Loss= 66.69979815552841\n",
      "iteration number =  3867 Cross Entropy Loss= 66.69633439651811\n",
      "iteration number =  3868 Cross Entropy Loss= 66.69287255440241\n",
      "iteration number =  3869 Cross Entropy Loss= 66.68941262796085\n",
      "iteration number =  3870 Cross Entropy Loss= 66.68595461597405\n",
      "iteration number =  3871 Cross Entropy Loss= 66.68249851722382\n",
      "iteration number =  3872 Cross Entropy Loss= 66.67904433049303\n",
      "iteration number =  3873 Cross Entropy Loss= 66.67559205456564\n",
      "iteration number =  3874 Cross Entropy Loss= 66.6721416882268\n",
      "iteration number =  3875 Cross Entropy Loss= 66.66869323026269\n",
      "iteration number =  3876 Cross Entropy Loss= 66.66524667946067\n",
      "iteration number =  3877 Cross Entropy Loss= 66.66180203460911\n",
      "iteration number =  3878 Cross Entropy Loss= 66.65835929449761\n",
      "iteration number =  3879 Cross Entropy Loss= 66.65491845791679\n",
      "iteration number =  3880 Cross Entropy Loss= 66.6514795236584\n",
      "iteration number =  3881 Cross Entropy Loss= 66.64804249051528\n",
      "iteration number =  3882 Cross Entropy Loss= 66.64460735728137\n",
      "iteration number =  3883 Cross Entropy Loss= 66.64117412275178\n",
      "iteration number =  3884 Cross Entropy Loss= 66.6377427857226\n",
      "iteration number =  3885 Cross Entropy Loss= 66.6343133449911\n",
      "iteration number =  3886 Cross Entropy Loss= 66.63088579935562\n",
      "iteration number =  3887 Cross Entropy Loss= 66.62746014761561\n",
      "iteration number =  3888 Cross Entropy Loss= 66.62403638857158\n",
      "iteration number =  3889 Cross Entropy Loss= 66.62061452102517\n",
      "iteration number =  3890 Cross Entropy Loss= 66.61719454377912\n",
      "iteration number =  3891 Cross Entropy Loss= 66.61377645563718\n",
      "iteration number =  3892 Cross Entropy Loss= 66.61036025540429\n",
      "iteration number =  3893 Cross Entropy Loss= 66.60694594188641\n",
      "iteration number =  3894 Cross Entropy Loss= 66.60353351389062\n",
      "iteration number =  3895 Cross Entropy Loss= 66.60012297022507\n",
      "iteration number =  3896 Cross Entropy Loss= 66.59671430969897\n",
      "iteration number =  3897 Cross Entropy Loss= 66.59330753112268\n",
      "iteration number =  3898 Cross Entropy Loss= 66.58990263330759\n",
      "iteration number =  3899 Cross Entropy Loss= 66.58649961506616\n",
      "iteration number =  3900 Cross Entropy Loss= 66.58309847521195\n",
      "iteration number =  3901 Cross Entropy Loss= 66.57969921255962\n",
      "iteration number =  3902 Cross Entropy Loss= 66.57630182592486\n",
      "iteration number =  3903 Cross Entropy Loss= 66.57290631412445\n",
      "iteration number =  3904 Cross Entropy Loss= 66.56951267597627\n",
      "iteration number =  3905 Cross Entropy Loss= 66.56612091029925\n",
      "iteration number =  3906 Cross Entropy Loss= 66.56273101591337\n",
      "iteration number =  3907 Cross Entropy Loss= 66.55934299163971\n",
      "iteration number =  3908 Cross Entropy Loss= 66.55595683630044\n",
      "iteration number =  3909 Cross Entropy Loss= 66.55257254871873\n",
      "iteration number =  3910 Cross Entropy Loss= 66.54919012771887\n",
      "iteration number =  3911 Cross Entropy Loss= 66.54580957212622\n",
      "iteration number =  3912 Cross Entropy Loss= 66.54243088076714\n",
      "iteration number =  3913 Cross Entropy Loss= 66.53905405246913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  3914 Cross Entropy Loss= 66.5356790860607\n",
      "iteration number =  3915 Cross Entropy Loss= 66.53230598037145\n",
      "iteration number =  3916 Cross Entropy Loss= 66.52893473423201\n",
      "iteration number =  3917 Cross Entropy Loss= 66.52556534647408\n",
      "iteration number =  3918 Cross Entropy Loss= 66.52219781593045\n",
      "iteration number =  3919 Cross Entropy Loss= 66.5188321414349\n",
      "iteration number =  3920 Cross Entropy Loss= 66.51546832182233\n",
      "iteration number =  3921 Cross Entropy Loss= 66.51210635592861\n",
      "iteration number =  3922 Cross Entropy Loss= 66.50874624259076\n",
      "iteration number =  3923 Cross Entropy Loss= 66.50538798064679\n",
      "iteration number =  3924 Cross Entropy Loss= 66.50203156893575\n",
      "iteration number =  3925 Cross Entropy Loss= 66.49867700629778\n",
      "iteration number =  3926 Cross Entropy Loss= 66.49532429157405\n",
      "iteration number =  3927 Cross Entropy Loss= 66.49197342360677\n",
      "iteration number =  3928 Cross Entropy Loss= 66.48862440123916\n",
      "iteration number =  3929 Cross Entropy Loss= 66.48527722331555\n",
      "iteration number =  3930 Cross Entropy Loss= 66.48193188868129\n",
      "iteration number =  3931 Cross Entropy Loss= 66.47858839618273\n",
      "iteration number =  3932 Cross Entropy Loss= 66.47524674466732\n",
      "iteration number =  3933 Cross Entropy Loss= 66.47190693298347\n",
      "iteration number =  3934 Cross Entropy Loss= 66.46856895998074\n",
      "iteration number =  3935 Cross Entropy Loss= 66.46523282450961\n",
      "iteration number =  3936 Cross Entropy Loss= 66.46189852542167\n",
      "iteration number =  3937 Cross Entropy Loss= 66.45856606156951\n",
      "iteration number =  3938 Cross Entropy Loss= 66.45523543180676\n",
      "iteration number =  3939 Cross Entropy Loss= 66.45190663498808\n",
      "iteration number =  3940 Cross Entropy Loss= 66.44857966996916\n",
      "iteration number =  3941 Cross Entropy Loss= 66.44525453560674\n",
      "iteration number =  3942 Cross Entropy Loss= 66.44193123075854\n",
      "iteration number =  3943 Cross Entropy Loss= 66.43860975428335\n",
      "iteration number =  3944 Cross Entropy Loss= 66.43529010504096\n",
      "iteration number =  3945 Cross Entropy Loss= 66.43197228189221\n",
      "iteration number =  3946 Cross Entropy Loss= 66.42865628369891\n",
      "iteration number =  3947 Cross Entropy Loss= 66.42534210932398\n",
      "iteration number =  3948 Cross Entropy Loss= 66.42202975763128\n",
      "iteration number =  3949 Cross Entropy Loss= 66.41871922748571\n",
      "iteration number =  3950 Cross Entropy Loss= 66.4154105177532\n",
      "iteration number =  3951 Cross Entropy Loss= 66.4121036273007\n",
      "iteration number =  3952 Cross Entropy Loss= 66.40879855499615\n",
      "iteration number =  3953 Cross Entropy Loss= 66.40549529970856\n",
      "iteration number =  3954 Cross Entropy Loss= 66.40219386030788\n",
      "iteration number =  3955 Cross Entropy Loss= 66.39889423566511\n",
      "iteration number =  3956 Cross Entropy Loss= 66.39559642465227\n",
      "iteration number =  3957 Cross Entropy Loss= 66.39230042614238\n",
      "iteration number =  3958 Cross Entropy Loss= 66.38900623900948\n",
      "iteration number =  3959 Cross Entropy Loss= 66.3857138621286\n",
      "iteration number =  3960 Cross Entropy Loss= 66.38242329437576\n",
      "iteration number =  3961 Cross Entropy Loss= 66.37913453462801\n",
      "iteration number =  3962 Cross Entropy Loss= 66.37584758176344\n",
      "iteration number =  3963 Cross Entropy Loss= 66.37256243466106\n",
      "iteration number =  3964 Cross Entropy Loss= 66.36927909220097\n",
      "iteration number =  3965 Cross Entropy Loss= 66.36599755326418\n",
      "iteration number =  3966 Cross Entropy Loss= 66.3627178167328\n",
      "iteration number =  3967 Cross Entropy Loss= 66.35943988148983\n",
      "iteration number =  3968 Cross Entropy Loss= 66.35616374641938\n",
      "iteration number =  3969 Cross Entropy Loss= 66.35288941040645\n",
      "iteration number =  3970 Cross Entropy Loss= 66.34961687233712\n",
      "iteration number =  3971 Cross Entropy Loss= 66.34634613109841\n",
      "iteration number =  3972 Cross Entropy Loss= 66.34307718557837\n",
      "iteration number =  3973 Cross Entropy Loss= 66.33981003466599\n",
      "iteration number =  3974 Cross Entropy Loss= 66.33654467725131\n",
      "iteration number =  3975 Cross Entropy Loss= 66.33328111222534\n",
      "iteration number =  3976 Cross Entropy Loss= 66.33001933848003\n",
      "iteration number =  3977 Cross Entropy Loss= 66.3267593549084\n",
      "iteration number =  3978 Cross Entropy Loss= 66.3235011604044\n",
      "iteration number =  3979 Cross Entropy Loss= 66.320244753863\n",
      "iteration number =  3980 Cross Entropy Loss= 66.31699013418009\n",
      "iteration number =  3981 Cross Entropy Loss= 66.31373730025263\n",
      "iteration number =  3982 Cross Entropy Loss= 66.31048625097849\n",
      "iteration number =  3983 Cross Entropy Loss= 66.30723698525655\n",
      "iteration number =  3984 Cross Entropy Loss= 66.30398950198669\n",
      "iteration number =  3985 Cross Entropy Loss= 66.30074380006971\n",
      "iteration number =  3986 Cross Entropy Loss= 66.29749987840746\n",
      "iteration number =  3987 Cross Entropy Loss= 66.29425773590272\n",
      "iteration number =  3988 Cross Entropy Loss= 66.29101737145923\n",
      "iteration number =  3989 Cross Entropy Loss= 66.28777878398175\n",
      "iteration number =  3990 Cross Entropy Loss= 66.28454197237599\n",
      "iteration number =  3991 Cross Entropy Loss= 66.28130693554863\n",
      "iteration number =  3992 Cross Entropy Loss= 66.27807367240732\n",
      "iteration number =  3993 Cross Entropy Loss= 66.27484218186066\n",
      "iteration number =  3994 Cross Entropy Loss= 66.27161246281828\n",
      "iteration number =  3995 Cross Entropy Loss= 66.26838451419073\n",
      "iteration number =  3996 Cross Entropy Loss= 66.26515833488952\n",
      "iteration number =  3997 Cross Entropy Loss= 66.26193392382714\n",
      "iteration number =  3998 Cross Entropy Loss= 66.25871127991704\n",
      "iteration number =  3999 Cross Entropy Loss= 66.25549040207366\n",
      "iteration number =  4000 Cross Entropy Loss= 66.25227128921235\n",
      "iteration number =  4001 Cross Entropy Loss= 66.24905394024947\n",
      "iteration number =  4002 Cross Entropy Loss= 66.24583835410228\n",
      "iteration number =  4003 Cross Entropy Loss= 66.24262452968908\n",
      "iteration number =  4004 Cross Entropy Loss= 66.23941246592906\n",
      "iteration number =  4005 Cross Entropy Loss= 66.23620216174241\n",
      "iteration number =  4006 Cross Entropy Loss= 66.23299361605022\n",
      "iteration number =  4007 Cross Entropy Loss= 66.2297868277746\n",
      "iteration number =  4008 Cross Entropy Loss= 66.22658179583857\n",
      "iteration number =  4009 Cross Entropy Loss= 66.22337851916613\n",
      "iteration number =  4010 Cross Entropy Loss= 66.2201769966822\n",
      "iteration number =  4011 Cross Entropy Loss= 66.21697722731267\n",
      "iteration number =  4012 Cross Entropy Loss= 66.21377920998437\n",
      "iteration number =  4013 Cross Entropy Loss= 66.21058294362508\n",
      "iteration number =  4014 Cross Entropy Loss= 66.20738842716355\n",
      "iteration number =  4015 Cross Entropy Loss= 66.20419565952943\n",
      "iteration number =  4016 Cross Entropy Loss= 66.20100463965333\n",
      "iteration number =  4017 Cross Entropy Loss= 66.19781536646687\n",
      "iteration number =  4018 Cross Entropy Loss= 66.19462783890249\n",
      "iteration number =  4019 Cross Entropy Loss= 66.19144205589367\n",
      "iteration number =  4020 Cross Entropy Loss= 66.18825801637479\n",
      "iteration number =  4021 Cross Entropy Loss= 66.18507571928116\n",
      "iteration number =  4022 Cross Entropy Loss= 66.18189516354907\n",
      "iteration number =  4023 Cross Entropy Loss= 66.17871634811571\n",
      "iteration number =  4024 Cross Entropy Loss= 66.1755392719192\n",
      "iteration number =  4025 Cross Entropy Loss= 66.17236393389865\n",
      "iteration number =  4026 Cross Entropy Loss= 66.16919033299405\n",
      "iteration number =  4027 Cross Entropy Loss= 66.16601846814632\n",
      "iteration number =  4028 Cross Entropy Loss= 66.16284833829735\n",
      "iteration number =  4029 Cross Entropy Loss= 66.15967994238994\n",
      "iteration number =  4030 Cross Entropy Loss= 66.15651327936781\n",
      "iteration number =  4031 Cross Entropy Loss= 66.15334834817565\n",
      "iteration number =  4032 Cross Entropy Loss= 66.150185147759\n",
      "iteration number =  4033 Cross Entropy Loss= 66.14702367706442\n",
      "iteration number =  4034 Cross Entropy Loss= 66.14386393503935\n",
      "iteration number =  4035 Cross Entropy Loss= 66.14070592063212\n",
      "iteration number =  4036 Cross Entropy Loss= 66.13754963279204\n",
      "iteration number =  4037 Cross Entropy Loss= 66.13439507046932\n",
      "iteration number =  4038 Cross Entropy Loss= 66.13124223261508\n",
      "iteration number =  4039 Cross Entropy Loss= 66.1280911181814\n",
      "iteration number =  4040 Cross Entropy Loss= 66.12494172612125\n",
      "iteration number =  4041 Cross Entropy Loss= 66.12179405538849\n",
      "iteration number =  4042 Cross Entropy Loss= 66.11864810493795\n",
      "iteration number =  4043 Cross Entropy Loss= 66.11550387372534\n",
      "iteration number =  4044 Cross Entropy Loss= 66.11236136070734\n",
      "iteration number =  4045 Cross Entropy Loss= 66.10922056484145\n",
      "iteration number =  4046 Cross Entropy Loss= 66.10608148508618\n",
      "iteration number =  4047 Cross Entropy Loss= 66.10294412040089\n",
      "iteration number =  4048 Cross Entropy Loss= 66.09980846974588\n",
      "iteration number =  4049 Cross Entropy Loss= 66.09667453208235\n",
      "iteration number =  4050 Cross Entropy Loss= 66.0935423063724\n",
      "iteration number =  4051 Cross Entropy Loss= 66.09041179157906\n",
      "iteration number =  4052 Cross Entropy Loss= 66.08728298666625\n",
      "iteration number =  4053 Cross Entropy Loss= 66.08415589059881\n",
      "iteration number =  4054 Cross Entropy Loss= 66.08103050234246\n",
      "iteration number =  4055 Cross Entropy Loss= 66.07790682086387\n",
      "iteration number =  4056 Cross Entropy Loss= 66.07478484513057\n",
      "iteration number =  4057 Cross Entropy Loss= 66.07166457411098\n",
      "iteration number =  4058 Cross Entropy Loss= 66.0685460067745\n",
      "iteration number =  4059 Cross Entropy Loss= 66.06542914209135\n",
      "iteration number =  4060 Cross Entropy Loss= 66.06231397903267\n",
      "iteration number =  4061 Cross Entropy Loss= 66.05920051657051\n",
      "iteration number =  4062 Cross Entropy Loss= 66.05608875367781\n",
      "iteration number =  4063 Cross Entropy Loss= 66.05297868932844\n",
      "iteration number =  4064 Cross Entropy Loss= 66.0498703224971\n",
      "iteration number =  4065 Cross Entropy Loss= 66.04676365215943\n",
      "iteration number =  4066 Cross Entropy Loss= 66.04365867729193\n",
      "iteration number =  4067 Cross Entropy Loss= 66.04055539687205\n",
      "iteration number =  4068 Cross Entropy Loss= 66.03745380987806\n",
      "iteration number =  4069 Cross Entropy Loss= 66.03435391528917\n",
      "iteration number =  4070 Cross Entropy Loss= 66.03125571208548\n",
      "iteration number =  4071 Cross Entropy Loss= 66.02815919924794\n",
      "iteration number =  4072 Cross Entropy Loss= 66.0250643757584\n",
      "iteration number =  4073 Cross Entropy Loss= 66.02197124059963\n",
      "iteration number =  4074 Cross Entropy Loss= 66.01887979275524\n",
      "iteration number =  4075 Cross Entropy Loss= 66.01579003120978\n",
      "iteration number =  4076 Cross Entropy Loss= 66.0127019549486\n",
      "iteration number =  4077 Cross Entropy Loss= 66.009615562958\n",
      "iteration number =  4078 Cross Entropy Loss= 66.00653085422516\n",
      "iteration number =  4079 Cross Entropy Loss= 66.0034478277381\n",
      "iteration number =  4080 Cross Entropy Loss= 66.00036648248575\n",
      "iteration number =  4081 Cross Entropy Loss= 65.99728681745789\n",
      "iteration number =  4082 Cross Entropy Loss= 65.99420883164524\n",
      "iteration number =  4083 Cross Entropy Loss= 65.99113252403932\n",
      "iteration number =  4084 Cross Entropy Loss= 65.98805789363254\n",
      "iteration number =  4085 Cross Entropy Loss= 65.98498493941827\n",
      "iteration number =  4086 Cross Entropy Loss= 65.98191366039063\n",
      "iteration number =  4087 Cross Entropy Loss= 65.97884405554468\n",
      "iteration number =  4088 Cross Entropy Loss= 65.97577612387636\n",
      "iteration number =  4089 Cross Entropy Loss= 65.97270986438245\n",
      "iteration number =  4090 Cross Entropy Loss= 65.9696452760606\n",
      "iteration number =  4091 Cross Entropy Loss= 65.96658235790937\n",
      "iteration number =  4092 Cross Entropy Loss= 65.96352110892812\n",
      "iteration number =  4093 Cross Entropy Loss= 65.96046152811716\n",
      "iteration number =  4094 Cross Entropy Loss= 65.95740361447758\n",
      "iteration number =  4095 Cross Entropy Loss= 65.95434736701141\n",
      "iteration number =  4096 Cross Entropy Loss= 65.95129278472146\n",
      "iteration number =  4097 Cross Entropy Loss= 65.9482398666115\n",
      "iteration number =  4098 Cross Entropy Loss= 65.9451886116861\n",
      "iteration number =  4099 Cross Entropy Loss= 65.9421390189507\n",
      "iteration number =  4100 Cross Entropy Loss= 65.93909108741161\n",
      "iteration number =  4101 Cross Entropy Loss= 65.93604481607599\n",
      "iteration number =  4102 Cross Entropy Loss= 65.93300020395186\n",
      "iteration number =  4103 Cross Entropy Loss= 65.92995725004812\n",
      "iteration number =  4104 Cross Entropy Loss= 65.92691595337449\n",
      "iteration number =  4105 Cross Entropy Loss= 65.92387631294157\n",
      "iteration number =  4106 Cross Entropy Loss= 65.9208383277608\n",
      "iteration number =  4107 Cross Entropy Loss= 65.91780199684449\n",
      "iteration number =  4108 Cross Entropy Loss= 65.91476731920577\n",
      "iteration number =  4109 Cross Entropy Loss= 65.91173429385869\n",
      "iteration number =  4110 Cross Entropy Loss= 65.90870291981804\n",
      "iteration number =  4111 Cross Entropy Loss= 65.90567319609958\n",
      "iteration number =  4112 Cross Entropy Loss= 65.90264512171983\n",
      "iteration number =  4113 Cross Entropy Loss= 65.89961869569622\n",
      "iteration number =  4114 Cross Entropy Loss= 65.89659391704697\n",
      "iteration number =  4115 Cross Entropy Loss= 65.89357078479121\n",
      "iteration number =  4116 Cross Entropy Loss= 65.89054929794884\n",
      "iteration number =  4117 Cross Entropy Loss= 65.88752945554066\n",
      "iteration number =  4118 Cross Entropy Loss= 65.88451125658831\n",
      "iteration number =  4119 Cross Entropy Loss= 65.88149470011425\n",
      "iteration number =  4120 Cross Entropy Loss= 65.87847978514178\n",
      "iteration number =  4121 Cross Entropy Loss= 65.87546651069506\n",
      "iteration number =  4122 Cross Entropy Loss= 65.8724548757991\n",
      "iteration number =  4123 Cross Entropy Loss= 65.86944487947969\n",
      "iteration number =  4124 Cross Entropy Loss= 65.86643652076351\n",
      "iteration number =  4125 Cross Entropy Loss= 65.8634297986781\n",
      "iteration number =  4126 Cross Entropy Loss= 65.86042471225176\n",
      "iteration number =  4127 Cross Entropy Loss= 65.85742126051368\n",
      "iteration number =  4128 Cross Entropy Loss= 65.85441944249385\n",
      "iteration number =  4129 Cross Entropy Loss= 65.85141925722314\n",
      "iteration number =  4130 Cross Entropy Loss= 65.84842070373323\n",
      "iteration number =  4131 Cross Entropy Loss= 65.84542378105658\n",
      "iteration number =  4132 Cross Entropy Loss= 65.84242848822657\n",
      "iteration number =  4133 Cross Entropy Loss= 65.83943482427735\n",
      "iteration number =  4134 Cross Entropy Loss= 65.8364427882439\n",
      "iteration number =  4135 Cross Entropy Loss= 65.83345237916207\n",
      "iteration number =  4136 Cross Entropy Loss= 65.83046359606848\n",
      "iteration number =  4137 Cross Entropy Loss= 65.82747643800064\n",
      "iteration number =  4138 Cross Entropy Loss= 65.82449090399682\n",
      "iteration number =  4139 Cross Entropy Loss= 65.82150699309615\n",
      "iteration number =  4140 Cross Entropy Loss= 65.81852470433857\n",
      "iteration number =  4141 Cross Entropy Loss= 65.81554403676486\n",
      "iteration number =  4142 Cross Entropy Loss= 65.81256498941663\n",
      "iteration number =  4143 Cross Entropy Loss= 65.80958756133626\n",
      "iteration number =  4144 Cross Entropy Loss= 65.80661175156698\n",
      "iteration number =  4145 Cross Entropy Loss= 65.80363755915285\n",
      "iteration number =  4146 Cross Entropy Loss= 65.80066498313874\n",
      "iteration number =  4147 Cross Entropy Loss= 65.79769402257033\n",
      "iteration number =  4148 Cross Entropy Loss= 65.79472467649411\n",
      "iteration number =  4149 Cross Entropy Loss= 65.79175694395742\n",
      "iteration number =  4150 Cross Entropy Loss= 65.78879082400837\n",
      "iteration number =  4151 Cross Entropy Loss= 65.78582631569593\n",
      "iteration number =  4152 Cross Entropy Loss= 65.78286341806981\n",
      "iteration number =  4153 Cross Entropy Loss= 65.7799021301806\n",
      "iteration number =  4154 Cross Entropy Loss= 65.77694245107968\n",
      "iteration number =  4155 Cross Entropy Loss= 65.77398437981924\n",
      "iteration number =  4156 Cross Entropy Loss= 65.77102791545225\n",
      "iteration number =  4157 Cross Entropy Loss= 65.76807305703255\n",
      "iteration number =  4158 Cross Entropy Loss= 65.76511980361474\n",
      "iteration number =  4159 Cross Entropy Loss= 65.76216815425423\n",
      "iteration number =  4160 Cross Entropy Loss= 65.75921810800725\n",
      "iteration number =  4161 Cross Entropy Loss= 65.75626966393081\n",
      "iteration number =  4162 Cross Entropy Loss= 65.75332282108278\n",
      "iteration number =  4163 Cross Entropy Loss= 65.75037757852176\n",
      "iteration number =  4164 Cross Entropy Loss= 65.7474339353072\n",
      "iteration number =  4165 Cross Entropy Loss= 65.74449189049933\n",
      "iteration number =  4166 Cross Entropy Loss= 65.74155144315921\n",
      "iteration number =  4167 Cross Entropy Loss= 65.73861259234863\n",
      "iteration number =  4168 Cross Entropy Loss= 65.73567533713029\n",
      "iteration number =  4169 Cross Entropy Loss= 65.73273967656756\n",
      "iteration number =  4170 Cross Entropy Loss= 65.7298056097247\n",
      "iteration number =  4171 Cross Entropy Loss= 65.72687313566675\n",
      "iteration number =  4172 Cross Entropy Loss= 65.72394225345951\n",
      "iteration number =  4173 Cross Entropy Loss= 65.7210129621696\n",
      "iteration number =  4174 Cross Entropy Loss= 65.71808526086443\n",
      "iteration number =  4175 Cross Entropy Loss= 65.71515914861219\n",
      "iteration number =  4176 Cross Entropy Loss= 65.71223462448188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4177 Cross Entropy Loss= 65.70931168754328\n",
      "iteration number =  4178 Cross Entropy Loss= 65.70639033686696\n",
      "iteration number =  4179 Cross Entropy Loss= 65.7034705715243\n",
      "iteration number =  4180 Cross Entropy Loss= 65.70055239058745\n",
      "iteration number =  4181 Cross Entropy Loss= 65.69763579312931\n",
      "iteration number =  4182 Cross Entropy Loss= 65.69472077822363\n",
      "iteration number =  4183 Cross Entropy Loss= 65.69180734494492\n",
      "iteration number =  4184 Cross Entropy Loss= 65.68889549236847\n",
      "iteration number =  4185 Cross Entropy Loss= 65.68598521957037\n",
      "iteration number =  4186 Cross Entropy Loss= 65.68307652562746\n",
      "iteration number =  4187 Cross Entropy Loss= 65.68016940961739\n",
      "iteration number =  4188 Cross Entropy Loss= 65.67726387061859\n",
      "iteration number =  4189 Cross Entropy Loss= 65.67435990771027\n",
      "iteration number =  4190 Cross Entropy Loss= 65.6714575199724\n",
      "iteration number =  4191 Cross Entropy Loss= 65.66855670648576\n",
      "iteration number =  4192 Cross Entropy Loss= 65.66565746633188\n",
      "iteration number =  4193 Cross Entropy Loss= 65.66275979859307\n",
      "iteration number =  4194 Cross Entropy Loss= 65.65986370235245\n",
      "iteration number =  4195 Cross Entropy Loss= 65.65696917669388\n",
      "iteration number =  4196 Cross Entropy Loss= 65.654076220702\n",
      "iteration number =  4197 Cross Entropy Loss= 65.65118483346222\n",
      "iteration number =  4198 Cross Entropy Loss= 65.64829501406075\n",
      "iteration number =  4199 Cross Entropy Loss= 65.64540676158455\n",
      "iteration number =  4200 Cross Entropy Loss= 65.64252007512135\n",
      "iteration number =  4201 Cross Entropy Loss= 65.63963495375967\n",
      "iteration number =  4202 Cross Entropy Loss= 65.63675139658878\n",
      "iteration number =  4203 Cross Entropy Loss= 65.6338694026987\n",
      "iteration number =  4204 Cross Entropy Loss= 65.63098897118029\n",
      "iteration number =  4205 Cross Entropy Loss= 65.6281101011251\n",
      "iteration number =  4206 Cross Entropy Loss= 65.62523279162546\n",
      "iteration number =  4207 Cross Entropy Loss= 65.62235704177452\n",
      "iteration number =  4208 Cross Entropy Loss= 65.61948285066612\n",
      "iteration number =  4209 Cross Entropy Loss= 65.61661021739494\n",
      "iteration number =  4210 Cross Entropy Loss= 65.61373914105633\n",
      "iteration number =  4211 Cross Entropy Loss= 65.61086962074653\n",
      "iteration number =  4212 Cross Entropy Loss= 65.6080016555624\n",
      "iteration number =  4213 Cross Entropy Loss= 65.60513524460166\n",
      "iteration number =  4214 Cross Entropy Loss= 65.60227038696274\n",
      "iteration number =  4215 Cross Entropy Loss= 65.59940708174486\n",
      "iteration number =  4216 Cross Entropy Loss= 65.596545328048\n",
      "iteration number =  4217 Cross Entropy Loss= 65.59368512497285\n",
      "iteration number =  4218 Cross Entropy Loss= 65.5908264716209\n",
      "iteration number =  4219 Cross Entropy Loss= 65.5879693670944\n",
      "iteration number =  4220 Cross Entropy Loss= 65.58511381049632\n",
      "iteration number =  4221 Cross Entropy Loss= 65.58225980093042\n",
      "iteration number =  4222 Cross Entropy Loss= 65.57940733750118\n",
      "iteration number =  4223 Cross Entropy Loss= 65.57655641931387\n",
      "iteration number =  4224 Cross Entropy Loss= 65.57370704547446\n",
      "iteration number =  4225 Cross Entropy Loss= 65.57085921508974\n",
      "iteration number =  4226 Cross Entropy Loss= 65.56801292726718\n",
      "iteration number =  4227 Cross Entropy Loss= 65.56516818111506\n",
      "iteration number =  4228 Cross Entropy Loss= 65.56232497574234\n",
      "iteration number =  4229 Cross Entropy Loss= 65.55948331025883\n",
      "iteration number =  4230 Cross Entropy Loss= 65.55664318377495\n",
      "iteration number =  4231 Cross Entropy Loss= 65.553804595402\n",
      "iteration number =  4232 Cross Entropy Loss= 65.55096754425195\n",
      "iteration number =  4233 Cross Entropy Loss= 65.54813202943751\n",
      "iteration number =  4234 Cross Entropy Loss= 65.54529805007218\n",
      "iteration number =  4235 Cross Entropy Loss= 65.54246560527015\n",
      "iteration number =  4236 Cross Entropy Loss= 65.53963469414643\n",
      "iteration number =  4237 Cross Entropy Loss= 65.53680531581666\n",
      "iteration number =  4238 Cross Entropy Loss= 65.53397746939733\n",
      "iteration number =  4239 Cross Entropy Loss= 65.53115115400557\n",
      "iteration number =  4240 Cross Entropy Loss= 65.52832636875935\n",
      "iteration number =  4241 Cross Entropy Loss= 65.52550311277732\n",
      "iteration number =  4242 Cross Entropy Loss= 65.52268138517884\n",
      "iteration number =  4243 Cross Entropy Loss= 65.5198611850841\n",
      "iteration number =  4244 Cross Entropy Loss= 65.5170425116139\n",
      "iteration number =  4245 Cross Entropy Loss= 65.51422536388989\n",
      "iteration number =  4246 Cross Entropy Loss= 65.51140974103441\n",
      "iteration number =  4247 Cross Entropy Loss= 65.5085956421705\n",
      "iteration number =  4248 Cross Entropy Loss= 65.505783066422\n",
      "iteration number =  4249 Cross Entropy Loss= 65.5029720129134\n",
      "iteration number =  4250 Cross Entropy Loss= 65.50016248077002\n",
      "iteration number =  4251 Cross Entropy Loss= 65.49735446911781\n",
      "iteration number =  4252 Cross Entropy Loss= 65.49454797708353\n",
      "iteration number =  4253 Cross Entropy Loss= 65.49174300379461\n",
      "iteration number =  4254 Cross Entropy Loss= 65.48893954837926\n",
      "iteration number =  4255 Cross Entropy Loss= 65.48613760996636\n",
      "iteration number =  4256 Cross Entropy Loss= 65.48333718768559\n",
      "iteration number =  4257 Cross Entropy Loss= 65.48053828066728\n",
      "iteration number =  4258 Cross Entropy Loss= 65.47774088804252\n",
      "iteration number =  4259 Cross Entropy Loss= 65.47494500894314\n",
      "iteration number =  4260 Cross Entropy Loss= 65.47215064250165\n",
      "iteration number =  4261 Cross Entropy Loss= 65.46935778785134\n",
      "iteration number =  4262 Cross Entropy Loss= 65.46656644412616\n",
      "iteration number =  4263 Cross Entropy Loss= 65.46377661046084\n",
      "iteration number =  4264 Cross Entropy Loss= 65.46098828599077\n",
      "iteration number =  4265 Cross Entropy Loss= 65.45820146985213\n",
      "iteration number =  4266 Cross Entropy Loss= 65.45541616118173\n",
      "iteration number =  4267 Cross Entropy Loss= 65.4526323591172\n",
      "iteration number =  4268 Cross Entropy Loss= 65.44985006279681\n",
      "iteration number =  4269 Cross Entropy Loss= 65.44706927135957\n",
      "iteration number =  4270 Cross Entropy Loss= 65.44428998394521\n",
      "iteration number =  4271 Cross Entropy Loss= 65.44151219969419\n",
      "iteration number =  4272 Cross Entropy Loss= 65.43873591774766\n",
      "iteration number =  4273 Cross Entropy Loss= 65.43596113724746\n",
      "iteration number =  4274 Cross Entropy Loss= 65.43318785733622\n",
      "iteration number =  4275 Cross Entropy Loss= 65.4304160771572\n",
      "iteration number =  4276 Cross Entropy Loss= 65.42764579585445\n",
      "iteration number =  4277 Cross Entropy Loss= 65.42487701257264\n",
      "iteration number =  4278 Cross Entropy Loss= 65.42210972645722\n",
      "iteration number =  4279 Cross Entropy Loss= 65.41934393665431\n",
      "iteration number =  4280 Cross Entropy Loss= 65.41657964231078\n",
      "iteration number =  4281 Cross Entropy Loss= 65.41381684257418\n",
      "iteration number =  4282 Cross Entropy Loss= 65.41105553659274\n",
      "iteration number =  4283 Cross Entropy Loss= 65.40829572351545\n",
      "iteration number =  4284 Cross Entropy Loss= 65.40553740249199\n",
      "iteration number =  4285 Cross Entropy Loss= 65.4027805726727\n",
      "iteration number =  4286 Cross Entropy Loss= 65.40002523320868\n",
      "iteration number =  4287 Cross Entropy Loss= 65.39727138325172\n",
      "iteration number =  4288 Cross Entropy Loss= 65.3945190219543\n",
      "iteration number =  4289 Cross Entropy Loss= 65.3917681484696\n",
      "iteration number =  4290 Cross Entropy Loss= 65.38901876195149\n",
      "iteration number =  4291 Cross Entropy Loss= 65.38627086155458\n",
      "iteration number =  4292 Cross Entropy Loss= 65.38352444643414\n",
      "iteration number =  4293 Cross Entropy Loss= 65.38077951574617\n",
      "iteration number =  4294 Cross Entropy Loss= 65.37803606864735\n",
      "iteration number =  4295 Cross Entropy Loss= 65.37529410429505\n",
      "iteration number =  4296 Cross Entropy Loss= 65.37255362184737\n",
      "iteration number =  4297 Cross Entropy Loss= 65.36981462046305\n",
      "iteration number =  4298 Cross Entropy Loss= 65.36707709930157\n",
      "iteration number =  4299 Cross Entropy Loss= 65.36434105752309\n",
      "iteration number =  4300 Cross Entropy Loss= 65.36160649428845\n",
      "iteration number =  4301 Cross Entropy Loss= 65.35887340875925\n",
      "iteration number =  4302 Cross Entropy Loss= 65.35614180009767\n",
      "iteration number =  4303 Cross Entropy Loss= 65.35341166746667\n",
      "iteration number =  4304 Cross Entropy Loss= 65.35068301002985\n",
      "iteration number =  4305 Cross Entropy Loss= 65.34795582695156\n",
      "iteration number =  4306 Cross Entropy Loss= 65.34523011739678\n",
      "iteration number =  4307 Cross Entropy Loss= 65.34250588053118\n",
      "iteration number =  4308 Cross Entropy Loss= 65.33978311552116\n",
      "iteration number =  4309 Cross Entropy Loss= 65.33706182153375\n",
      "iteration number =  4310 Cross Entropy Loss= 65.33434199773676\n",
      "iteration number =  4311 Cross Entropy Loss= 65.33162364329856\n",
      "iteration number =  4312 Cross Entropy Loss= 65.32890675738832\n",
      "iteration number =  4313 Cross Entropy Loss= 65.32619133917584\n",
      "iteration number =  4314 Cross Entropy Loss= 65.32347738783156\n",
      "iteration number =  4315 Cross Entropy Loss= 65.32076490252669\n",
      "iteration number =  4316 Cross Entropy Loss= 65.31805388243308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4317 Cross Entropy Loss= 65.31534432672326\n",
      "iteration number =  4318 Cross Entropy Loss= 65.31263623457045\n",
      "iteration number =  4319 Cross Entropy Loss= 65.30992960514853\n",
      "iteration number =  4320 Cross Entropy Loss= 65.30722443763207\n",
      "iteration number =  4321 Cross Entropy Loss= 65.30452073119635\n",
      "iteration number =  4322 Cross Entropy Loss= 65.30181848501728\n",
      "iteration number =  4323 Cross Entropy Loss= 65.29911769827144\n",
      "iteration number =  4324 Cross Entropy Loss= 65.29641837013618\n",
      "iteration number =  4325 Cross Entropy Loss= 65.29372049978939\n",
      "iteration number =  4326 Cross Entropy Loss= 65.29102408640973\n",
      "iteration number =  4327 Cross Entropy Loss= 65.28832912917652\n",
      "iteration number =  4328 Cross Entropy Loss= 65.28563562726971\n",
      "iteration number =  4329 Cross Entropy Loss= 65.28294357987\n",
      "iteration number =  4330 Cross Entropy Loss= 65.28025298615866\n",
      "iteration number =  4331 Cross Entropy Loss= 65.27756384531772\n",
      "iteration number =  4332 Cross Entropy Loss= 65.27487615652984\n",
      "iteration number =  4333 Cross Entropy Loss= 65.27218991897837\n",
      "iteration number =  4334 Cross Entropy Loss= 65.26950513184728\n",
      "iteration number =  4335 Cross Entropy Loss= 65.26682179432129\n",
      "iteration number =  4336 Cross Entropy Loss= 65.26413990558572\n",
      "iteration number =  4337 Cross Entropy Loss= 65.26145946482659\n",
      "iteration number =  4338 Cross Entropy Loss= 65.25878047123055\n",
      "iteration number =  4339 Cross Entropy Loss= 65.25610292398497\n",
      "iteration number =  4340 Cross Entropy Loss= 65.25342682227786\n",
      "iteration number =  4341 Cross Entropy Loss= 65.25075216529787\n",
      "iteration number =  4342 Cross Entropy Loss= 65.24807895223435\n",
      "iteration number =  4343 Cross Entropy Loss= 65.2454071822773\n",
      "iteration number =  4344 Cross Entropy Loss= 65.24273685461739\n",
      "iteration number =  4345 Cross Entropy Loss= 65.24006796844591\n",
      "iteration number =  4346 Cross Entropy Loss= 65.23740052295487\n",
      "iteration number =  4347 Cross Entropy Loss= 65.2347345173369\n",
      "iteration number =  4348 Cross Entropy Loss= 65.23206995078534\n",
      "iteration number =  4349 Cross Entropy Loss= 65.2294068224941\n",
      "iteration number =  4350 Cross Entropy Loss= 65.22674513165782\n",
      "iteration number =  4351 Cross Entropy Loss= 65.2240848774718\n",
      "iteration number =  4352 Cross Entropy Loss= 65.22142605913194\n",
      "iteration number =  4353 Cross Entropy Loss= 65.21876867583487\n",
      "iteration number =  4354 Cross Entropy Loss= 65.21611272677778\n",
      "iteration number =  4355 Cross Entropy Loss= 65.21345821115864\n",
      "iteration number =  4356 Cross Entropy Loss= 65.21080512817596\n",
      "iteration number =  4357 Cross Entropy Loss= 65.20815347702896\n",
      "iteration number =  4358 Cross Entropy Loss= 65.20550325691751\n",
      "iteration number =  4359 Cross Entropy Loss= 65.2028544670421\n",
      "iteration number =  4360 Cross Entropy Loss= 65.20020710660394\n",
      "iteration number =  4361 Cross Entropy Loss= 65.1975611748048\n",
      "iteration number =  4362 Cross Entropy Loss= 65.19491667084718\n",
      "iteration number =  4363 Cross Entropy Loss= 65.19227359393417\n",
      "iteration number =  4364 Cross Entropy Loss= 65.18963194326955\n",
      "iteration number =  4365 Cross Entropy Loss= 65.1869917180577\n",
      "iteration number =  4366 Cross Entropy Loss= 65.18435291750373\n",
      "iteration number =  4367 Cross Entropy Loss= 65.18171554081331\n",
      "iteration number =  4368 Cross Entropy Loss= 65.17907958719283\n",
      "iteration number =  4369 Cross Entropy Loss= 65.17644505584924\n",
      "iteration number =  4370 Cross Entropy Loss= 65.17381194599022\n",
      "iteration number =  4371 Cross Entropy Loss= 65.17118025682404\n",
      "iteration number =  4372 Cross Entropy Loss= 65.16854998755963\n",
      "iteration number =  4373 Cross Entropy Loss= 65.16592113740656\n",
      "iteration number =  4374 Cross Entropy Loss= 65.16329370557506\n",
      "iteration number =  4375 Cross Entropy Loss= 65.16066769127598\n",
      "iteration number =  4376 Cross Entropy Loss= 65.1580430937208\n",
      "iteration number =  4377 Cross Entropy Loss= 65.15541991212167\n",
      "iteration number =  4378 Cross Entropy Loss= 65.15279814569136\n",
      "iteration number =  4379 Cross Entropy Loss= 65.15017779364331\n",
      "iteration number =  4380 Cross Entropy Loss= 65.14755885519153\n",
      "iteration number =  4381 Cross Entropy Loss= 65.14494132955078\n",
      "iteration number =  4382 Cross Entropy Loss= 65.14232521593631\n",
      "iteration number =  4383 Cross Entropy Loss= 65.13971051356411\n",
      "iteration number =  4384 Cross Entropy Loss= 65.1370972216508\n",
      "iteration number =  4385 Cross Entropy Loss= 65.13448533941359\n",
      "iteration number =  4386 Cross Entropy Loss= 65.13187486607036\n",
      "iteration number =  4387 Cross Entropy Loss= 65.1292658008396\n",
      "iteration number =  4388 Cross Entropy Loss= 65.12665814294047\n",
      "iteration number =  4389 Cross Entropy Loss= 65.12405189159271\n",
      "iteration number =  4390 Cross Entropy Loss= 65.12144704601673\n",
      "iteration number =  4391 Cross Entropy Loss= 65.11884360543357\n",
      "iteration number =  4392 Cross Entropy Loss= 65.11624156906484\n",
      "iteration number =  4393 Cross Entropy Loss= 65.11364093613288\n",
      "iteration number =  4394 Cross Entropy Loss= 65.1110417058606\n",
      "iteration number =  4395 Cross Entropy Loss= 65.10844387747153\n",
      "iteration number =  4396 Cross Entropy Loss= 65.10584745018986\n",
      "iteration number =  4397 Cross Entropy Loss= 65.10325242324036\n",
      "iteration number =  4398 Cross Entropy Loss= 65.1006587958485\n",
      "iteration number =  4399 Cross Entropy Loss= 65.09806656724032\n",
      "iteration number =  4400 Cross Entropy Loss= 65.09547573664247\n",
      "iteration number =  4401 Cross Entropy Loss= 65.09288630328227\n",
      "iteration number =  4402 Cross Entropy Loss= 65.09029826638766\n",
      "iteration number =  4403 Cross Entropy Loss= 65.08771162518718\n",
      "iteration number =  4404 Cross Entropy Loss= 65.08512637891\n",
      "iteration number =  4405 Cross Entropy Loss= 65.08254252678591\n",
      "iteration number =  4406 Cross Entropy Loss= 65.07996006804534\n",
      "iteration number =  4407 Cross Entropy Loss= 65.07737900191931\n",
      "iteration number =  4408 Cross Entropy Loss= 65.07479932763948\n",
      "iteration number =  4409 Cross Entropy Loss= 65.07222104443811\n",
      "iteration number =  4410 Cross Entropy Loss= 65.06964415154812\n",
      "iteration number =  4411 Cross Entropy Loss= 65.06706864820302\n",
      "iteration number =  4412 Cross Entropy Loss= 65.0644945336369\n",
      "iteration number =  4413 Cross Entropy Loss= 65.06192180708457\n",
      "iteration number =  4414 Cross Entropy Loss= 65.05935046778134\n",
      "iteration number =  4415 Cross Entropy Loss= 65.05678051496321\n",
      "iteration number =  4416 Cross Entropy Loss= 65.05421194786678\n",
      "iteration number =  4417 Cross Entropy Loss= 65.05164476572922\n",
      "iteration number =  4418 Cross Entropy Loss= 65.0490789677884\n",
      "iteration number =  4419 Cross Entropy Loss= 65.04651455328272\n",
      "iteration number =  4420 Cross Entropy Loss= 65.04395152145125\n",
      "iteration number =  4421 Cross Entropy Loss= 65.04138987153362\n",
      "iteration number =  4422 Cross Entropy Loss= 65.03882960277012\n",
      "iteration number =  4423 Cross Entropy Loss= 65.03627071440161\n",
      "iteration number =  4424 Cross Entropy Loss= 65.03371320566963\n",
      "iteration number =  4425 Cross Entropy Loss= 65.0311570758162\n",
      "iteration number =  4426 Cross Entropy Loss= 65.02860232408408\n",
      "iteration number =  4427 Cross Entropy Loss= 65.02604894971658\n",
      "iteration number =  4428 Cross Entropy Loss= 65.02349695195761\n",
      "iteration number =  4429 Cross Entropy Loss= 65.02094633005171\n",
      "iteration number =  4430 Cross Entropy Loss= 65.01839708324403\n",
      "iteration number =  4431 Cross Entropy Loss= 65.01584921078027\n",
      "iteration number =  4432 Cross Entropy Loss= 65.01330271190682\n",
      "iteration number =  4433 Cross Entropy Loss= 65.0107575858706\n",
      "iteration number =  4434 Cross Entropy Loss= 65.00821383191919\n",
      "iteration number =  4435 Cross Entropy Loss= 65.00567144930072\n",
      "iteration number =  4436 Cross Entropy Loss= 65.00313043726399\n",
      "iteration number =  4437 Cross Entropy Loss= 65.0005907950583\n",
      "iteration number =  4438 Cross Entropy Loss= 64.99805252193369\n",
      "iteration number =  4439 Cross Entropy Loss= 64.99551561714065\n",
      "iteration number =  4440 Cross Entropy Loss= 64.99298007993039\n",
      "iteration number =  4441 Cross Entropy Loss= 64.99044590955467\n",
      "iteration number =  4442 Cross Entropy Loss= 64.98791310526585\n",
      "iteration number =  4443 Cross Entropy Loss= 64.98538166631687\n",
      "iteration number =  4444 Cross Entropy Loss= 64.98285159196132\n",
      "iteration number =  4445 Cross Entropy Loss= 64.98032288145333\n",
      "iteration number =  4446 Cross Entropy Loss= 64.97779553404766\n",
      "iteration number =  4447 Cross Entropy Loss= 64.97526954899965\n",
      "iteration number =  4448 Cross Entropy Loss= 64.97274492556527\n",
      "iteration number =  4449 Cross Entropy Loss= 64.97022166300103\n",
      "iteration number =  4450 Cross Entropy Loss= 64.96769976056406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4451 Cross Entropy Loss= 64.9651792175121\n",
      "iteration number =  4452 Cross Entropy Loss= 64.96266003310346\n",
      "iteration number =  4453 Cross Entropy Loss= 64.96014220659706\n",
      "iteration number =  4454 Cross Entropy Loss= 64.95762573725239\n",
      "iteration number =  4455 Cross Entropy Loss= 64.95511062432954\n",
      "iteration number =  4456 Cross Entropy Loss= 64.9525968670892\n",
      "iteration number =  4457 Cross Entropy Loss= 64.95008446479262\n",
      "iteration number =  4458 Cross Entropy Loss= 64.9475734167017\n",
      "iteration number =  4459 Cross Entropy Loss= 64.94506372207887\n",
      "iteration number =  4460 Cross Entropy Loss= 64.94255538018717\n",
      "iteration number =  4461 Cross Entropy Loss= 64.94004839029022\n",
      "iteration number =  4462 Cross Entropy Loss= 64.93754275165223\n",
      "iteration number =  4463 Cross Entropy Loss= 64.935038463538\n",
      "iteration number =  4464 Cross Entropy Loss= 64.93253552521294\n",
      "iteration number =  4465 Cross Entropy Loss= 64.930033935943\n",
      "iteration number =  4466 Cross Entropy Loss= 64.92753369499471\n",
      "iteration number =  4467 Cross Entropy Loss= 64.92503480163523\n",
      "iteration number =  4468 Cross Entropy Loss= 64.92253725513231\n",
      "iteration number =  4469 Cross Entropy Loss= 64.9200410547542\n",
      "iteration number =  4470 Cross Entropy Loss= 64.9175461997698\n",
      "iteration number =  4471 Cross Entropy Loss= 64.9150526894486\n",
      "iteration number =  4472 Cross Entropy Loss= 64.91256052306063\n",
      "iteration number =  4473 Cross Entropy Loss= 64.91006969987653\n",
      "iteration number =  4474 Cross Entropy Loss= 64.90758021916749\n",
      "iteration number =  4475 Cross Entropy Loss= 64.9050920802053\n",
      "iteration number =  4476 Cross Entropy Loss= 64.90260528226236\n",
      "iteration number =  4477 Cross Entropy Loss= 64.90011982461155\n",
      "iteration number =  4478 Cross Entropy Loss= 64.89763570652644\n",
      "iteration number =  4479 Cross Entropy Loss= 64.89515292728112\n",
      "iteration number =  4480 Cross Entropy Loss= 64.89267148615023\n",
      "iteration number =  4481 Cross Entropy Loss= 64.89019138240909\n",
      "iteration number =  4482 Cross Entropy Loss= 64.88771261533347\n",
      "iteration number =  4483 Cross Entropy Loss= 64.88523518419977\n",
      "iteration number =  4484 Cross Entropy Loss= 64.88275908828498\n",
      "iteration number =  4485 Cross Entropy Loss= 64.88028432686664\n",
      "iteration number =  4486 Cross Entropy Loss= 64.87781089922288\n",
      "iteration number =  4487 Cross Entropy Loss= 64.87533880463236\n",
      "iteration number =  4488 Cross Entropy Loss= 64.87286804237439\n",
      "iteration number =  4489 Cross Entropy Loss= 64.87039861172877\n",
      "iteration number =  4490 Cross Entropy Loss= 64.86793051197593\n",
      "iteration number =  4491 Cross Entropy Loss= 64.8654637423968\n",
      "iteration number =  4492 Cross Entropy Loss= 64.86299830227296\n",
      "iteration number =  4493 Cross Entropy Loss= 64.86053419088653\n",
      "iteration number =  4494 Cross Entropy Loss= 64.85807140752014\n",
      "iteration number =  4495 Cross Entropy Loss= 64.85560995145711\n",
      "iteration number =  4496 Cross Entropy Loss= 64.85314982198119\n",
      "iteration number =  4497 Cross Entropy Loss= 64.8506910183768\n",
      "iteration number =  4498 Cross Entropy Loss= 64.84823353992888\n",
      "iteration number =  4499 Cross Entropy Loss= 64.84577738592294\n",
      "iteration number =  4500 Cross Entropy Loss= 64.84332255564506\n",
      "iteration number =  4501 Cross Entropy Loss= 64.84086904838189\n",
      "iteration number =  4502 Cross Entropy Loss= 64.8384168634206\n",
      "iteration number =  4503 Cross Entropy Loss= 64.83596600004901\n",
      "iteration number =  4504 Cross Entropy Loss= 64.8335164575554\n",
      "iteration number =  4505 Cross Entropy Loss= 64.83106823522871\n",
      "iteration number =  4506 Cross Entropy Loss= 64.82862133235837\n",
      "iteration number =  4507 Cross Entropy Loss= 64.82617574823438\n",
      "iteration number =  4508 Cross Entropy Loss= 64.82373148214734\n",
      "iteration number =  4509 Cross Entropy Loss= 64.82128853338838\n",
      "iteration number =  4510 Cross Entropy Loss= 64.8188469012492\n",
      "iteration number =  4511 Cross Entropy Loss= 64.81640658502204\n",
      "iteration number =  4512 Cross Entropy Loss= 64.8139675839997\n",
      "iteration number =  4513 Cross Entropy Loss= 64.81152989747558\n",
      "iteration number =  4514 Cross Entropy Loss= 64.80909352474359\n",
      "iteration number =  4515 Cross Entropy Loss= 64.8066584650982\n",
      "iteration number =  4516 Cross Entropy Loss= 64.80422471783446\n",
      "iteration number =  4517 Cross Entropy Loss= 64.80179228224797\n",
      "iteration number =  4518 Cross Entropy Loss= 64.79936115763486\n",
      "iteration number =  4519 Cross Entropy Loss= 64.79693134329183\n",
      "iteration number =  4520 Cross Entropy Loss= 64.79450283851615\n",
      "iteration number =  4521 Cross Entropy Loss= 64.79207564260562\n",
      "iteration number =  4522 Cross Entropy Loss= 64.78964975485862\n",
      "iteration number =  4523 Cross Entropy Loss= 64.78722517457402\n",
      "iteration number =  4524 Cross Entropy Loss= 64.78480190105131\n",
      "iteration number =  4525 Cross Entropy Loss= 64.7823799335905\n",
      "iteration number =  4526 Cross Entropy Loss= 64.77995927149216\n",
      "iteration number =  4527 Cross Entropy Loss= 64.7775399140574\n",
      "iteration number =  4528 Cross Entropy Loss= 64.77512186058789\n",
      "iteration number =  4529 Cross Entropy Loss= 64.77270511038583\n",
      "iteration number =  4530 Cross Entropy Loss= 64.77028966275398\n",
      "iteration number =  4531 Cross Entropy Loss= 64.76787551699564\n",
      "iteration number =  4532 Cross Entropy Loss= 64.76546267241471\n",
      "iteration number =  4533 Cross Entropy Loss= 64.76305112831554\n",
      "iteration number =  4534 Cross Entropy Loss= 64.7606408840031\n",
      "iteration number =  4535 Cross Entropy Loss= 64.75823193878287\n",
      "iteration number =  4536 Cross Entropy Loss= 64.75582429196089\n",
      "iteration number =  4537 Cross Entropy Loss= 64.75341794284377\n",
      "iteration number =  4538 Cross Entropy Loss= 64.7510128907386\n",
      "iteration number =  4539 Cross Entropy Loss= 64.74860913495304\n",
      "iteration number =  4540 Cross Entropy Loss= 64.74620667479533\n",
      "iteration number =  4541 Cross Entropy Loss= 64.74380550957423\n",
      "iteration number =  4542 Cross Entropy Loss= 64.741405638599\n",
      "iteration number =  4543 Cross Entropy Loss= 64.73900706117949\n",
      "iteration number =  4544 Cross Entropy Loss= 64.7366097766261\n",
      "iteration number =  4545 Cross Entropy Loss= 64.7342137842497\n",
      "iteration number =  4546 Cross Entropy Loss= 64.73181908336177\n",
      "iteration number =  4547 Cross Entropy Loss= 64.72942567327432\n",
      "iteration number =  4548 Cross Entropy Loss= 64.72703355329985\n",
      "iteration number =  4549 Cross Entropy Loss= 64.72464272275147\n",
      "iteration number =  4550 Cross Entropy Loss= 64.72225318094274\n",
      "iteration number =  4551 Cross Entropy Loss= 64.71986492718783\n",
      "iteration number =  4552 Cross Entropy Loss= 64.71747796080143\n",
      "iteration number =  4553 Cross Entropy Loss= 64.71509228109875\n",
      "iteration number =  4554 Cross Entropy Loss= 64.71270788739551\n",
      "iteration number =  4555 Cross Entropy Loss= 64.71032477900805\n",
      "iteration number =  4556 Cross Entropy Loss= 64.70794295525316\n",
      "iteration number =  4557 Cross Entropy Loss= 64.70556241544817\n",
      "iteration number =  4558 Cross Entropy Loss= 64.70318315891102\n",
      "iteration number =  4559 Cross Entropy Loss= 64.70080518496009\n",
      "iteration number =  4560 Cross Entropy Loss= 64.69842849291433\n",
      "iteration number =  4561 Cross Entropy Loss= 64.69605308209326\n",
      "iteration number =  4562 Cross Entropy Loss= 64.69367895181685\n",
      "iteration number =  4563 Cross Entropy Loss= 64.69130610140566\n",
      "iteration number =  4564 Cross Entropy Loss= 64.68893453018077\n",
      "iteration number =  4565 Cross Entropy Loss= 64.6865642374638\n",
      "iteration number =  4566 Cross Entropy Loss= 64.68419522257683\n",
      "iteration number =  4567 Cross Entropy Loss= 64.68182748484256\n",
      "iteration number =  4568 Cross Entropy Loss= 64.67946102358417\n",
      "iteration number =  4569 Cross Entropy Loss= 64.67709583812538\n",
      "iteration number =  4570 Cross Entropy Loss= 64.67473192779042\n",
      "iteration number =  4571 Cross Entropy Loss= 64.67236929190406\n",
      "iteration number =  4572 Cross Entropy Loss= 64.6700079297916\n",
      "iteration number =  4573 Cross Entropy Loss= 64.66764784077886\n",
      "iteration number =  4574 Cross Entropy Loss= 64.66528902419218\n",
      "iteration number =  4575 Cross Entropy Loss= 64.6629314793584\n",
      "iteration number =  4576 Cross Entropy Loss= 64.660575205605\n",
      "iteration number =  4577 Cross Entropy Loss= 64.65822020225978\n",
      "iteration number =  4578 Cross Entropy Loss= 64.65586646865125\n",
      "iteration number =  4579 Cross Entropy Loss= 64.65351400410835\n",
      "iteration number =  4580 Cross Entropy Loss= 64.65116280796056\n",
      "iteration number =  4581 Cross Entropy Loss= 64.64881287953787\n",
      "iteration number =  4582 Cross Entropy Loss= 64.64646421817083\n",
      "iteration number =  4583 Cross Entropy Loss= 64.64411682319046\n",
      "iteration number =  4584 Cross Entropy Loss= 64.64177069392832\n",
      "iteration number =  4585 Cross Entropy Loss= 64.63942582971652\n",
      "iteration number =  4586 Cross Entropy Loss= 64.63708222988764\n",
      "iteration number =  4587 Cross Entropy Loss= 64.63473989377478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4588 Cross Entropy Loss= 64.63239882071159\n",
      "iteration number =  4589 Cross Entropy Loss= 64.63005901003223\n",
      "iteration number =  4590 Cross Entropy Loss= 64.62772046107135\n",
      "iteration number =  4591 Cross Entropy Loss= 64.62538317316415\n",
      "iteration number =  4592 Cross Entropy Loss= 64.62304714564632\n",
      "iteration number =  4593 Cross Entropy Loss= 64.62071237785409\n",
      "iteration number =  4594 Cross Entropy Loss= 64.61837886912417\n",
      "iteration number =  4595 Cross Entropy Loss= 64.61604661879383\n",
      "iteration number =  4596 Cross Entropy Loss= 64.61371562620079\n",
      "iteration number =  4597 Cross Entropy Loss= 64.61138589068332\n",
      "iteration number =  4598 Cross Entropy Loss= 64.60905741158024\n",
      "iteration number =  4599 Cross Entropy Loss= 64.60673018823081\n",
      "iteration number =  4600 Cross Entropy Loss= 64.60440421997485\n",
      "iteration number =  4601 Cross Entropy Loss= 64.60207950615266\n",
      "iteration number =  4602 Cross Entropy Loss= 64.5997560461051\n",
      "iteration number =  4603 Cross Entropy Loss= 64.59743383917348\n",
      "iteration number =  4604 Cross Entropy Loss= 64.59511288469962\n",
      "iteration number =  4605 Cross Entropy Loss= 64.59279318202594\n",
      "iteration number =  4606 Cross Entropy Loss= 64.59047473049525\n",
      "iteration number =  4607 Cross Entropy Loss= 64.58815752945092\n",
      "iteration number =  4608 Cross Entropy Loss= 64.58584157823687\n",
      "iteration number =  4609 Cross Entropy Loss= 64.58352687619745\n",
      "iteration number =  4610 Cross Entropy Loss= 64.58121342267755\n",
      "iteration number =  4611 Cross Entropy Loss= 64.57890121702259\n",
      "iteration number =  4612 Cross Entropy Loss= 64.57659025857845\n",
      "iteration number =  4613 Cross Entropy Loss= 64.57428054669155\n",
      "iteration number =  4614 Cross Entropy Loss= 64.57197208070882\n",
      "iteration number =  4615 Cross Entropy Loss= 64.56966485997764\n",
      "iteration number =  4616 Cross Entropy Loss= 64.56735888384594\n",
      "iteration number =  4617 Cross Entropy Loss= 64.56505415166217\n",
      "iteration number =  4618 Cross Entropy Loss= 64.56275066277522\n",
      "iteration number =  4619 Cross Entropy Loss= 64.56044841653451\n",
      "iteration number =  4620 Cross Entropy Loss= 64.55814741229001\n",
      "iteration number =  4621 Cross Entropy Loss= 64.55584764939213\n",
      "iteration number =  4622 Cross Entropy Loss= 64.55354912719179\n",
      "iteration number =  4623 Cross Entropy Loss= 64.55125184504043\n",
      "iteration number =  4624 Cross Entropy Loss= 64.54895580228997\n",
      "iteration number =  4625 Cross Entropy Loss= 64.54666099829285\n",
      "iteration number =  4626 Cross Entropy Loss= 64.54436743240198\n",
      "iteration number =  4627 Cross Entropy Loss= 64.54207510397082\n",
      "iteration number =  4628 Cross Entropy Loss= 64.53978401235325\n",
      "iteration number =  4629 Cross Entropy Loss= 64.53749415690372\n",
      "iteration number =  4630 Cross Entropy Loss= 64.53520553697713\n",
      "iteration number =  4631 Cross Entropy Loss= 64.5329181519289\n",
      "iteration number =  4632 Cross Entropy Loss= 64.53063200111492\n",
      "iteration number =  4633 Cross Entropy Loss= 64.52834708389162\n",
      "iteration number =  4634 Cross Entropy Loss= 64.52606339961588\n",
      "iteration number =  4635 Cross Entropy Loss= 64.52378094764512\n",
      "iteration number =  4636 Cross Entropy Loss= 64.52149972733719\n",
      "iteration number =  4637 Cross Entropy Loss= 64.51921973805048\n",
      "iteration number =  4638 Cross Entropy Loss= 64.51694097914387\n",
      "iteration number =  4639 Cross Entropy Loss= 64.51466344997671\n",
      "iteration number =  4640 Cross Entropy Loss= 64.51238714990888\n",
      "iteration number =  4641 Cross Entropy Loss= 64.51011207830071\n",
      "iteration number =  4642 Cross Entropy Loss= 64.50783823451303\n",
      "iteration number =  4643 Cross Entropy Loss= 64.50556561790718\n",
      "iteration number =  4644 Cross Entropy Loss= 64.50329422784498\n",
      "iteration number =  4645 Cross Entropy Loss= 64.50102406368875\n",
      "iteration number =  4646 Cross Entropy Loss= 64.49875512480125\n",
      "iteration number =  4647 Cross Entropy Loss= 64.4964874105458\n",
      "iteration number =  4648 Cross Entropy Loss= 64.49422092028612\n",
      "iteration number =  4649 Cross Entropy Loss= 64.49195565338657\n",
      "iteration number =  4650 Cross Entropy Loss= 64.4896916092118\n",
      "iteration number =  4651 Cross Entropy Loss= 64.48742878712709\n",
      "iteration number =  4652 Cross Entropy Loss= 64.48516718649816\n",
      "iteration number =  4653 Cross Entropy Loss= 64.4829068066912\n",
      "iteration number =  4654 Cross Entropy Loss= 64.4806476470729\n",
      "iteration number =  4655 Cross Entropy Loss= 64.47838970701046\n",
      "iteration number =  4656 Cross Entropy Loss= 64.47613298587152\n",
      "iteration number =  4657 Cross Entropy Loss= 64.47387748302424\n",
      "iteration number =  4658 Cross Entropy Loss= 64.47162319783722\n",
      "iteration number =  4659 Cross Entropy Loss= 64.46937012967959\n",
      "iteration number =  4660 Cross Entropy Loss= 64.46711827792093\n",
      "iteration number =  4661 Cross Entropy Loss= 64.46486764193133\n",
      "iteration number =  4662 Cross Entropy Loss= 64.46261822108133\n",
      "iteration number =  4663 Cross Entropy Loss= 64.46037001474198\n",
      "iteration number =  4664 Cross Entropy Loss= 64.4581230222848\n",
      "iteration number =  4665 Cross Entropy Loss= 64.45587724308174\n",
      "iteration number =  4666 Cross Entropy Loss= 64.45363267650535\n",
      "iteration number =  4667 Cross Entropy Loss= 64.45138932192853\n",
      "iteration number =  4668 Cross Entropy Loss= 64.44914717872474\n",
      "iteration number =  4669 Cross Entropy Loss= 64.44690624626789\n",
      "iteration number =  4670 Cross Entropy Loss= 64.44466652393236\n",
      "iteration number =  4671 Cross Entropy Loss= 64.44242801109301\n",
      "iteration number =  4672 Cross Entropy Loss= 64.44019070712521\n",
      "iteration number =  4673 Cross Entropy Loss= 64.43795461140476\n",
      "iteration number =  4674 Cross Entropy Loss= 64.43571972330797\n",
      "iteration number =  4675 Cross Entropy Loss= 64.4334860422116\n",
      "iteration number =  4676 Cross Entropy Loss= 64.43125356749289\n",
      "iteration number =  4677 Cross Entropy Loss= 64.42902229852959\n",
      "iteration number =  4678 Cross Entropy Loss= 64.42679223469986\n",
      "iteration number =  4679 Cross Entropy Loss= 64.4245633753824\n",
      "iteration number =  4680 Cross Entropy Loss= 64.42233571995632\n",
      "iteration number =  4681 Cross Entropy Loss= 64.42010926780125\n",
      "iteration number =  4682 Cross Entropy Loss= 64.4178840182973\n",
      "iteration number =  4683 Cross Entropy Loss= 64.41565997082499\n",
      "iteration number =  4684 Cross Entropy Loss= 64.41343712476535\n",
      "iteration number =  4685 Cross Entropy Loss= 64.41121547949992\n",
      "iteration number =  4686 Cross Entropy Loss= 64.40899503441065\n",
      "iteration number =  4687 Cross Entropy Loss= 64.40677578887995\n",
      "iteration number =  4688 Cross Entropy Loss= 64.40455774229078\n",
      "iteration number =  4689 Cross Entropy Loss= 64.40234089402648\n",
      "iteration number =  4690 Cross Entropy Loss= 64.40012524347091\n",
      "iteration number =  4691 Cross Entropy Loss= 64.39791079000841\n",
      "iteration number =  4692 Cross Entropy Loss= 64.39569753302374\n",
      "iteration number =  4693 Cross Entropy Loss= 64.39348547190212\n",
      "iteration number =  4694 Cross Entropy Loss= 64.39127460602934\n",
      "iteration number =  4695 Cross Entropy Loss= 64.38906493479153\n",
      "iteration number =  4696 Cross Entropy Loss= 64.38685645757535\n",
      "iteration number =  4697 Cross Entropy Loss= 64.38464917376791\n",
      "iteration number =  4698 Cross Entropy Loss= 64.38244308275682\n",
      "iteration number =  4699 Cross Entropy Loss= 64.3802381839301\n",
      "iteration number =  4700 Cross Entropy Loss= 64.37803447667626\n",
      "iteration number =  4701 Cross Entropy Loss= 64.37583196038426\n",
      "iteration number =  4702 Cross Entropy Loss= 64.37363063444357\n",
      "iteration number =  4703 Cross Entropy Loss= 64.37143049824408\n",
      "iteration number =  4704 Cross Entropy Loss= 64.36923155117611\n",
      "iteration number =  4705 Cross Entropy Loss= 64.36703379263052\n",
      "iteration number =  4706 Cross Entropy Loss= 64.36483722199858\n",
      "iteration number =  4707 Cross Entropy Loss= 64.36264183867206\n",
      "iteration number =  4708 Cross Entropy Loss= 64.36044764204314\n",
      "iteration number =  4709 Cross Entropy Loss= 64.3582546315045\n",
      "iteration number =  4710 Cross Entropy Loss= 64.35606280644922\n",
      "iteration number =  4711 Cross Entropy Loss= 64.35387216627095\n",
      "iteration number =  4712 Cross Entropy Loss= 64.35168271036369\n",
      "iteration number =  4713 Cross Entropy Loss= 64.34949443812197\n",
      "iteration number =  4714 Cross Entropy Loss= 64.34730734894072\n",
      "iteration number =  4715 Cross Entropy Loss= 64.34512144221537\n",
      "iteration number =  4716 Cross Entropy Loss= 64.3429367173418\n",
      "iteration number =  4717 Cross Entropy Loss= 64.34075317371635\n",
      "iteration number =  4718 Cross Entropy Loss= 64.33857081073577\n",
      "iteration number =  4719 Cross Entropy Loss= 64.33638962779735\n",
      "iteration number =  4720 Cross Entropy Loss= 64.33420962429875\n",
      "iteration number =  4721 Cross Entropy Loss= 64.33203079963813\n",
      "iteration number =  4722 Cross Entropy Loss= 64.3298531532141\n",
      "iteration number =  4723 Cross Entropy Loss= 64.32767668442573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4724 Cross Entropy Loss= 64.32550139267252\n",
      "iteration number =  4725 Cross Entropy Loss= 64.32332727735445\n",
      "iteration number =  4726 Cross Entropy Loss= 64.32115433787192\n",
      "iteration number =  4727 Cross Entropy Loss= 64.31898257362583\n",
      "iteration number =  4728 Cross Entropy Loss= 64.31681198401749\n",
      "iteration number =  4729 Cross Entropy Loss= 64.31464256844868\n",
      "iteration number =  4730 Cross Entropy Loss= 64.3124743263216\n",
      "iteration number =  4731 Cross Entropy Loss= 64.31030725703897\n",
      "iteration number =  4732 Cross Entropy Loss= 64.30814136000389\n",
      "iteration number =  4733 Cross Entropy Loss= 64.30597663461995\n",
      "iteration number =  4734 Cross Entropy Loss= 64.30381308029116\n",
      "iteration number =  4735 Cross Entropy Loss= 64.30165069642202\n",
      "iteration number =  4736 Cross Entropy Loss= 64.29948948241741\n",
      "iteration number =  4737 Cross Entropy Loss= 64.29732943768275\n",
      "iteration number =  4738 Cross Entropy Loss= 64.29517056162383\n",
      "iteration number =  4739 Cross Entropy Loss= 64.29301285364693\n",
      "iteration number =  4740 Cross Entropy Loss= 64.29085631315873\n",
      "iteration number =  4741 Cross Entropy Loss= 64.28870093956645\n",
      "iteration number =  4742 Cross Entropy Loss= 64.28654673227763\n",
      "iteration number =  4743 Cross Entropy Loss= 64.28439369070036\n",
      "iteration number =  4744 Cross Entropy Loss= 64.28224181424312\n",
      "iteration number =  4745 Cross Entropy Loss= 64.28009110231484\n",
      "iteration number =  4746 Cross Entropy Loss= 64.27794155432494\n",
      "iteration number =  4747 Cross Entropy Loss= 64.27579316968321\n",
      "iteration number =  4748 Cross Entropy Loss= 64.27364594779993\n",
      "iteration number =  4749 Cross Entropy Loss= 64.27149988808583\n",
      "iteration number =  4750 Cross Entropy Loss= 64.26935498995203\n",
      "iteration number =  4751 Cross Entropy Loss= 64.26721125281017\n",
      "iteration number =  4752 Cross Entropy Loss= 64.26506867607226\n",
      "iteration number =  4753 Cross Entropy Loss= 64.2629272591508\n",
      "iteration number =  4754 Cross Entropy Loss= 64.2607870014587\n",
      "iteration number =  4755 Cross Entropy Loss= 64.25864790240931\n",
      "iteration number =  4756 Cross Entropy Loss= 64.25650996141647\n",
      "iteration number =  4757 Cross Entropy Loss= 64.25437317789437\n",
      "iteration number =  4758 Cross Entropy Loss= 64.25223755125774\n",
      "iteration number =  4759 Cross Entropy Loss= 64.25010308092166\n",
      "iteration number =  4760 Cross Entropy Loss= 64.2479697663017\n",
      "iteration number =  4761 Cross Entropy Loss= 64.24583760681385\n",
      "iteration number =  4762 Cross Entropy Loss= 64.24370660187456\n",
      "iteration number =  4763 Cross Entropy Loss= 64.2415767509007\n",
      "iteration number =  4764 Cross Entropy Loss= 64.23944805330956\n",
      "iteration number =  4765 Cross Entropy Loss= 64.23732050851888\n",
      "iteration number =  4766 Cross Entropy Loss= 64.23519411594685\n",
      "iteration number =  4767 Cross Entropy Loss= 64.2330688750121\n",
      "iteration number =  4768 Cross Entropy Loss= 64.23094478513362\n",
      "iteration number =  4769 Cross Entropy Loss= 64.22882184573095\n",
      "iteration number =  4770 Cross Entropy Loss= 64.22670005622399\n",
      "iteration number =  4771 Cross Entropy Loss= 64.2245794160331\n",
      "iteration number =  4772 Cross Entropy Loss= 64.22245992457903\n",
      "iteration number =  4773 Cross Entropy Loss= 64.22034158128304\n",
      "iteration number =  4774 Cross Entropy Loss= 64.21822438556674\n",
      "iteration number =  4775 Cross Entropy Loss= 64.21610833685223\n",
      "iteration number =  4776 Cross Entropy Loss= 64.21399343456204\n",
      "iteration number =  4777 Cross Entropy Loss= 64.21187967811909\n",
      "iteration number =  4778 Cross Entropy Loss= 64.20976706694677\n",
      "iteration number =  4779 Cross Entropy Loss= 64.20765560046888\n",
      "iteration number =  4780 Cross Entropy Loss= 64.20554527810965\n",
      "iteration number =  4781 Cross Entropy Loss= 64.20343609929375\n",
      "iteration number =  4782 Cross Entropy Loss= 64.20132806344628\n",
      "iteration number =  4783 Cross Entropy Loss= 64.19922116999275\n",
      "iteration number =  4784 Cross Entropy Loss= 64.19711541835913\n",
      "iteration number =  4785 Cross Entropy Loss= 64.1950108079718\n",
      "iteration number =  4786 Cross Entropy Loss= 64.19290733825754\n",
      "iteration number =  4787 Cross Entropy Loss= 64.19080500864362\n",
      "iteration number =  4788 Cross Entropy Loss= 64.18870381855768\n",
      "iteration number =  4789 Cross Entropy Loss= 64.18660376742781\n",
      "iteration number =  4790 Cross Entropy Loss= 64.18450485468254\n",
      "iteration number =  4791 Cross Entropy Loss= 64.18240707975079\n",
      "iteration number =  4792 Cross Entropy Loss= 64.18031044206194\n",
      "iteration number =  4793 Cross Entropy Loss= 64.17821494104575\n",
      "iteration number =  4794 Cross Entropy Loss= 64.17612057613248\n",
      "iteration number =  4795 Cross Entropy Loss= 64.17402734675272\n",
      "iteration number =  4796 Cross Entropy Loss= 64.17193525233758\n",
      "iteration number =  4797 Cross Entropy Loss= 64.16984429231852\n",
      "iteration number =  4798 Cross Entropy Loss= 64.16775446612743\n",
      "iteration number =  4799 Cross Entropy Loss= 64.16566577319668\n",
      "iteration number =  4800 Cross Entropy Loss= 64.16357821295898\n",
      "iteration number =  4801 Cross Entropy Loss= 64.16149178484756\n",
      "iteration number =  4802 Cross Entropy Loss= 64.15940648829594\n",
      "iteration number =  4803 Cross Entropy Loss= 64.1573223227382\n",
      "iteration number =  4804 Cross Entropy Loss= 64.15523928760875\n",
      "iteration number =  4805 Cross Entropy Loss= 64.15315738234246\n",
      "iteration number =  4806 Cross Entropy Loss= 64.15107660637457\n",
      "iteration number =  4807 Cross Entropy Loss= 64.14899695914082\n",
      "iteration number =  4808 Cross Entropy Loss= 64.14691844007729\n",
      "iteration number =  4809 Cross Entropy Loss= 64.14484104862055\n",
      "iteration number =  4810 Cross Entropy Loss= 64.1427647842075\n",
      "iteration number =  4811 Cross Entropy Loss= 64.14068964627556\n",
      "iteration number =  4812 Cross Entropy Loss= 64.13861563426248\n",
      "iteration number =  4813 Cross Entropy Loss= 64.13654274760648\n",
      "iteration number =  4814 Cross Entropy Loss= 64.13447098574616\n",
      "iteration number =  4815 Cross Entropy Loss= 64.13240034812058\n",
      "iteration number =  4816 Cross Entropy Loss= 64.13033083416919\n",
      "iteration number =  4817 Cross Entropy Loss= 64.12826244333183\n",
      "iteration number =  4818 Cross Entropy Loss= 64.1261951750488\n",
      "iteration number =  4819 Cross Entropy Loss= 64.12412902876079\n",
      "iteration number =  4820 Cross Entropy Loss= 64.12206400390893\n",
      "iteration number =  4821 Cross Entropy Loss= 64.12000009993471\n",
      "iteration number =  4822 Cross Entropy Loss= 64.11793731628009\n",
      "iteration number =  4823 Cross Entropy Loss= 64.11587565238742\n",
      "iteration number =  4824 Cross Entropy Loss= 64.11381510769945\n",
      "iteration number =  4825 Cross Entropy Loss= 64.11175568165936\n",
      "iteration number =  4826 Cross Entropy Loss= 64.10969737371074\n",
      "iteration number =  4827 Cross Entropy Loss= 64.10764018329759\n",
      "iteration number =  4828 Cross Entropy Loss= 64.10558410986432\n",
      "iteration number =  4829 Cross Entropy Loss= 64.10352915285574\n",
      "iteration number =  4830 Cross Entropy Loss= 64.1014753117171\n",
      "iteration number =  4831 Cross Entropy Loss= 64.099422585894\n",
      "iteration number =  4832 Cross Entropy Loss= 64.09737097483254\n",
      "iteration number =  4833 Cross Entropy Loss= 64.09532047797916\n",
      "iteration number =  4834 Cross Entropy Loss= 64.09327109478072\n",
      "iteration number =  4835 Cross Entropy Loss= 64.0912228246845\n",
      "iteration number =  4836 Cross Entropy Loss= 64.08917566713818\n",
      "iteration number =  4837 Cross Entropy Loss= 64.08712962158987\n",
      "iteration number =  4838 Cross Entropy Loss= 64.08508468748803\n",
      "iteration number =  4839 Cross Entropy Loss= 64.08304086428161\n",
      "iteration number =  4840 Cross Entropy Loss= 64.08099815141992\n",
      "iteration number =  4841 Cross Entropy Loss= 64.07895654835265\n",
      "iteration number =  4842 Cross Entropy Loss= 64.07691605452993\n",
      "iteration number =  4843 Cross Entropy Loss= 64.07487666940231\n",
      "iteration number =  4844 Cross Entropy Loss= 64.07283839242073\n",
      "iteration number =  4845 Cross Entropy Loss= 64.0708012230365\n",
      "iteration number =  4846 Cross Entropy Loss= 64.06876516070137\n",
      "iteration number =  4847 Cross Entropy Loss= 64.0667302048675\n",
      "iteration number =  4848 Cross Entropy Loss= 64.06469635498743\n",
      "iteration number =  4849 Cross Entropy Loss= 64.06266361051414\n",
      "iteration number =  4850 Cross Entropy Loss= 64.06063197090094\n",
      "iteration number =  4851 Cross Entropy Loss= 64.05860143560163\n",
      "iteration number =  4852 Cross Entropy Loss= 64.05657200407036\n",
      "iteration number =  4853 Cross Entropy Loss= 64.05454367576169\n",
      "iteration number =  4854 Cross Entropy Loss= 64.05251645013058\n",
      "iteration number =  4855 Cross Entropy Loss= 64.05049032663241\n",
      "iteration number =  4856 Cross Entropy Loss= 64.04846530472292\n",
      "iteration number =  4857 Cross Entropy Loss= 64.04644138385831\n",
      "iteration number =  4858 Cross Entropy Loss= 64.04441856349513\n",
      "iteration number =  4859 Cross Entropy Loss= 64.04239684309033\n",
      "iteration number =  4860 Cross Entropy Loss= 64.04037622210129\n",
      "iteration number =  4861 Cross Entropy Loss= 64.03835669998575\n",
      "iteration number =  4862 Cross Entropy Loss= 64.0363382762019\n",
      "iteration number =  4863 Cross Entropy Loss= 64.0343209502083\n",
      "iteration number =  4864 Cross Entropy Loss= 64.03230472146389\n",
      "iteration number =  4865 Cross Entropy Loss= 64.03028958942802\n",
      "iteration number =  4866 Cross Entropy Loss= 64.02827555356045\n",
      "iteration number =  4867 Cross Entropy Loss= 64.02626261332132\n",
      "iteration number =  4868 Cross Entropy Loss= 64.02425076817119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  4869 Cross Entropy Loss= 64.02224001757097\n",
      "iteration number =  4870 Cross Entropy Loss= 64.02023036098201\n",
      "iteration number =  4871 Cross Entropy Loss= 64.01822179786606\n",
      "iteration number =  4872 Cross Entropy Loss= 64.01621432768519\n",
      "iteration number =  4873 Cross Entropy Loss= 64.01420794990197\n",
      "iteration number =  4874 Cross Entropy Loss= 64.01220266397928\n",
      "iteration number =  4875 Cross Entropy Loss= 64.01019846938044\n",
      "iteration number =  4876 Cross Entropy Loss= 64.00819536556915\n",
      "iteration number =  4877 Cross Entropy Loss= 64.00619335200948\n",
      "iteration number =  4878 Cross Entropy Loss= 64.00419242816594\n",
      "iteration number =  4879 Cross Entropy Loss= 64.00219259350337\n",
      "iteration number =  4880 Cross Entropy Loss= 64.00019384748707\n",
      "iteration number =  4881 Cross Entropy Loss= 63.998196189582686\n",
      "iteration number =  4882 Cross Entropy Loss= 63.99619961925627\n",
      "iteration number =  4883 Cross Entropy Loss= 63.99420413597426\n",
      "iteration number =  4884 Cross Entropy Loss= 63.9922097392035\n",
      "iteration number =  4885 Cross Entropy Loss= 63.99021642841119\n",
      "iteration number =  4886 Cross Entropy Loss= 63.988224203064924\n",
      "iteration number =  4887 Cross Entropy Loss= 63.98623306263274\n",
      "iteration number =  4888 Cross Entropy Loss= 63.98424300658301\n",
      "iteration number =  4889 Cross Entropy Loss= 63.9822540343845\n",
      "iteration number =  4890 Cross Entropy Loss= 63.98026614550639\n",
      "iteration number =  4891 Cross Entropy Loss= 63.978279339418215\n",
      "iteration number =  4892 Cross Entropy Loss= 63.976293615589924\n",
      "iteration number =  4893 Cross Entropy Loss= 63.97430897349186\n",
      "iteration number =  4894 Cross Entropy Loss= 63.97232541259469\n",
      "iteration number =  4895 Cross Entropy Loss= 63.97034293236956\n",
      "iteration number =  4896 Cross Entropy Loss= 63.96836153228792\n",
      "iteration number =  4897 Cross Entropy Loss= 63.96638121182167\n",
      "iteration number =  4898 Cross Entropy Loss= 63.96440197044304\n",
      "iteration number =  4899 Cross Entropy Loss= 63.9624238076247\n",
      "iteration number =  4900 Cross Entropy Loss= 63.96044672283965\n",
      "iteration number =  4901 Cross Entropy Loss= 63.958470715561305\n",
      "iteration number =  4902 Cross Entropy Loss= 63.956495785263456\n",
      "iteration number =  4903 Cross Entropy Loss= 63.95452193142028\n",
      "iteration number =  4904 Cross Entropy Loss= 63.95254915350636\n",
      "iteration number =  4905 Cross Entropy Loss= 63.950577450996605\n",
      "iteration number =  4906 Cross Entropy Loss= 63.94860682336636\n",
      "iteration number =  4907 Cross Entropy Loss= 63.94663727009133\n",
      "iteration number =  4908 Cross Entropy Loss= 63.9446687906476\n",
      "iteration number =  4909 Cross Entropy Loss= 63.942701384511636\n",
      "iteration number =  4910 Cross Entropy Loss= 63.94073505116029\n",
      "iteration number =  4911 Cross Entropy Loss= 63.93876979007079\n",
      "iteration number =  4912 Cross Entropy Loss= 63.93680560072078\n",
      "iteration number =  4913 Cross Entropy Loss= 63.93484248258822\n",
      "iteration number =  4914 Cross Entropy Loss= 63.932880435151475\n",
      "iteration number =  4915 Cross Entropy Loss= 63.93091945788931\n",
      "iteration number =  4916 Cross Entropy Loss= 63.92895955028088\n",
      "iteration number =  4917 Cross Entropy Loss= 63.92700071180563\n",
      "iteration number =  4918 Cross Entropy Loss= 63.925042941943516\n",
      "iteration number =  4919 Cross Entropy Loss= 63.923086240174776\n",
      "iteration number =  4920 Cross Entropy Loss= 63.921130605980025\n",
      "iteration number =  4921 Cross Entropy Loss= 63.919176038840305\n",
      "iteration number =  4922 Cross Entropy Loss= 63.91722253823703\n",
      "iteration number =  4923 Cross Entropy Loss= 63.915270103651935\n",
      "iteration number =  4924 Cross Entropy Loss= 63.91331873456719\n",
      "iteration number =  4925 Cross Entropy Loss= 63.91136843046532\n",
      "iteration number =  4926 Cross Entropy Loss= 63.909419190829226\n",
      "iteration number =  4927 Cross Entropy Loss= 63.90747101514217\n",
      "iteration number =  4928 Cross Entropy Loss= 63.90552390288781\n",
      "iteration number =  4929 Cross Entropy Loss= 63.90357785355017\n",
      "iteration number =  4930 Cross Entropy Loss= 63.90163286661366\n",
      "iteration number =  4931 Cross Entropy Loss= 63.899688941563035\n",
      "iteration number =  4932 Cross Entropy Loss= 63.89774607788345\n",
      "iteration number =  4933 Cross Entropy Loss= 63.895804275060414\n",
      "iteration number =  4934 Cross Entropy Loss= 63.893863532579815\n",
      "iteration number =  4935 Cross Entropy Loss= 63.891923849927934\n",
      "iteration number =  4936 Cross Entropy Loss= 63.889985226591406\n",
      "iteration number =  4937 Cross Entropy Loss= 63.888047662057225\n",
      "iteration number =  4938 Cross Entropy Loss= 63.886111155812756\n",
      "iteration number =  4939 Cross Entropy Loss= 63.88417570734578\n",
      "iteration number =  4940 Cross Entropy Loss= 63.882241316144416\n",
      "iteration number =  4941 Cross Entropy Loss= 63.88030798169713\n",
      "iteration number =  4942 Cross Entropy Loss= 63.87837570349281\n",
      "iteration number =  4943 Cross Entropy Loss= 63.87644448102064\n",
      "iteration number =  4944 Cross Entropy Loss= 63.87451431377028\n",
      "iteration number =  4945 Cross Entropy Loss= 63.87258520123166\n",
      "iteration number =  4946 Cross Entropy Loss= 63.87065714289512\n",
      "iteration number =  4947 Cross Entropy Loss= 63.86873013825138\n",
      "iteration number =  4948 Cross Entropy Loss= 63.866804186791505\n",
      "iteration number =  4949 Cross Entropy Loss= 63.86487928800695\n",
      "iteration number =  4950 Cross Entropy Loss= 63.862955441389495\n",
      "iteration number =  4951 Cross Entropy Loss= 63.86103264643134\n",
      "iteration number =  4952 Cross Entropy Loss= 63.85911090262501\n",
      "iteration number =  4953 Cross Entropy Loss= 63.85719020946343\n",
      "iteration number =  4954 Cross Entropy Loss= 63.85527056643986\n",
      "iteration number =  4955 Cross Entropy Loss= 63.85335197304795\n",
      "iteration number =  4956 Cross Entropy Loss= 63.8514344287817\n",
      "iteration number =  4957 Cross Entropy Loss= 63.84951793313547\n",
      "iteration number =  4958 Cross Entropy Loss= 63.847602485604035\n",
      "iteration number =  4959 Cross Entropy Loss= 63.84568808568245\n",
      "iteration number =  4960 Cross Entropy Loss= 63.843774732866194\n",
      "iteration number =  4961 Cross Entropy Loss= 63.84186242665109\n",
      "iteration number =  4962 Cross Entropy Loss= 63.83995116653334\n",
      "iteration number =  4963 Cross Entropy Loss= 63.838040952009486\n",
      "iteration number =  4964 Cross Entropy Loss= 63.83613178257645\n",
      "iteration number =  4965 Cross Entropy Loss= 63.83422365773151\n",
      "iteration number =  4966 Cross Entropy Loss= 63.8323165769723\n",
      "iteration number =  4967 Cross Entropy Loss= 63.83041053979684\n",
      "iteration number =  4968 Cross Entropy Loss= 63.82850554570346\n",
      "iteration number =  4969 Cross Entropy Loss= 63.826601594190905\n",
      "iteration number =  4970 Cross Entropy Loss= 63.82469868475827\n",
      "iteration number =  4971 Cross Entropy Loss= 63.822796816904976\n",
      "iteration number =  4972 Cross Entropy Loss= 63.82089599013085\n",
      "iteration number =  4973 Cross Entropy Loss= 63.81899620393605\n",
      "iteration number =  4974 Cross Entropy Loss= 63.81709745782108\n",
      "iteration number =  4975 Cross Entropy Loss= 63.815199751286855\n",
      "iteration number =  4976 Cross Entropy Loss= 63.81330308383461\n",
      "iteration number =  4977 Cross Entropy Loss= 63.811407454965924\n",
      "iteration number =  4978 Cross Entropy Loss= 63.809512864182786\n",
      "iteration number =  4979 Cross Entropy Loss= 63.80761931098749\n",
      "iteration number =  4980 Cross Entropy Loss= 63.805726794882716\n",
      "iteration number =  4981 Cross Entropy Loss= 63.803835315371494\n",
      "iteration number =  4982 Cross Entropy Loss= 63.80194487195721\n",
      "iteration number =  4983 Cross Entropy Loss= 63.800055464143625\n",
      "iteration number =  4984 Cross Entropy Loss= 63.79816709143482\n",
      "iteration number =  4985 Cross Entropy Loss= 63.79627975333526\n",
      "iteration number =  4986 Cross Entropy Loss= 63.79439344934975\n",
      "iteration number =  4987 Cross Entropy Loss= 63.792508178983454\n",
      "iteration number =  4988 Cross Entropy Loss= 63.79062394174192\n",
      "iteration number =  4989 Cross Entropy Loss= 63.78874073713099\n",
      "iteration number =  4990 Cross Entropy Loss= 63.78685856465692\n",
      "iteration number =  4991 Cross Entropy Loss= 63.78497742382627\n",
      "iteration number =  4992 Cross Entropy Loss= 63.783097314146\n",
      "iteration number =  4993 Cross Entropy Loss= 63.78121823512339\n",
      "iteration number =  4994 Cross Entropy Loss= 63.77934018626609\n",
      "iteration number =  4995 Cross Entropy Loss= 63.77746316708209\n",
      "iteration number =  4996 Cross Entropy Loss= 63.77558717707974\n",
      "iteration number =  4997 Cross Entropy Loss= 63.77371221576775\n",
      "iteration number =  4998 Cross Entropy Loss= 63.77183828265516\n",
      "iteration number =  4999 Cross Entropy Loss= 63.76996537725138\n",
      "iteration number =  5000 Cross Entropy Loss= 63.76809349906617\n",
      "iteration number =  5001 Cross Entropy Loss= 63.766222647609624\n",
      "iteration number =  5002 Cross Entropy Loss= 63.76435282239221\n",
      "iteration number =  5003 Cross Entropy Loss= 63.762484022924724\n",
      "iteration number =  5004 Cross Entropy Loss= 63.76061624871832\n",
      "iteration number =  5005 Cross Entropy Loss= 63.75874949928453\n",
      "iteration number =  5006 Cross Entropy Loss= 63.756883774135176\n",
      "iteration number =  5007 Cross Entropy Loss= 63.755019072782474\n",
      "iteration number =  5008 Cross Entropy Loss= 63.75315539473897\n",
      "iteration number =  5009 Cross Entropy Loss= 63.751292739517574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5010 Cross Entropy Loss= 63.74943110663153\n",
      "iteration number =  5011 Cross Entropy Loss= 63.74757049559443\n",
      "iteration number =  5012 Cross Entropy Loss= 63.74571090592022\n",
      "iteration number =  5013 Cross Entropy Loss= 63.7438523371232\n",
      "iteration number =  5014 Cross Entropy Loss= 63.74199478871799\n",
      "iteration number =  5015 Cross Entropy Loss= 63.74013826021955\n",
      "iteration number =  5016 Cross Entropy Loss= 63.738282751143274\n",
      "iteration number =  5017 Cross Entropy Loss= 63.73642826100479\n",
      "iteration number =  5018 Cross Entropy Loss= 63.73457478932012\n",
      "iteration number =  5019 Cross Entropy Loss= 63.73272233560564\n",
      "iteration number =  5020 Cross Entropy Loss= 63.73087089937805\n",
      "iteration number =  5021 Cross Entropy Loss= 63.729020480154404\n",
      "iteration number =  5022 Cross Entropy Loss= 63.72717107745212\n",
      "iteration number =  5023 Cross Entropy Loss= 63.725322690788914\n",
      "iteration number =  5024 Cross Entropy Loss= 63.723475319682876\n",
      "iteration number =  5025 Cross Entropy Loss= 63.721628963652456\n",
      "iteration number =  5026 Cross Entropy Loss= 63.71978362221641\n",
      "iteration number =  5027 Cross Entropy Loss= 63.717939294893824\n",
      "iteration number =  5028 Cross Entropy Loss= 63.71609598120421\n",
      "iteration number =  5029 Cross Entropy Loss= 63.714253680667326\n",
      "iteration number =  5030 Cross Entropy Loss= 63.71241239280333\n",
      "iteration number =  5031 Cross Entropy Loss= 63.7105721171327\n",
      "iteration number =  5032 Cross Entropy Loss= 63.70873285317626\n",
      "iteration number =  5033 Cross Entropy Loss= 63.70689460045517\n",
      "iteration number =  5034 Cross Entropy Loss= 63.70505735849093\n",
      "iteration number =  5035 Cross Entropy Loss= 63.703221126805396\n",
      "iteration number =  5036 Cross Entropy Loss= 63.701385904920734\n",
      "iteration number =  5037 Cross Entropy Loss= 63.69955169235948\n",
      "iteration number =  5038 Cross Entropy Loss= 63.697718488644504\n",
      "iteration number =  5039 Cross Entropy Loss= 63.69588629329898\n",
      "iteration number =  5040 Cross Entropy Loss= 63.69405510584648\n",
      "iteration number =  5041 Cross Entropy Loss= 63.69222492581085\n",
      "iteration number =  5042 Cross Entropy Loss= 63.69039575271633\n",
      "iteration number =  5043 Cross Entropy Loss= 63.68856758608747\n",
      "iteration number =  5044 Cross Entropy Loss= 63.686740425449166\n",
      "iteration number =  5045 Cross Entropy Loss= 63.68491427032664\n",
      "iteration number =  5046 Cross Entropy Loss= 63.683089120245455\n",
      "iteration number =  5047 Cross Entropy Loss= 63.68126497473152\n",
      "iteration number =  5048 Cross Entropy Loss= 63.67944183331107\n",
      "iteration number =  5049 Cross Entropy Loss= 63.67761969551067\n",
      "iteration number =  5050 Cross Entropy Loss= 63.675798560857245\n",
      "iteration number =  5051 Cross Entropy Loss= 63.67397842887804\n",
      "iteration number =  5052 Cross Entropy Loss= 63.67215929910061\n",
      "iteration number =  5053 Cross Entropy Loss= 63.670341171052904\n",
      "iteration number =  5054 Cross Entropy Loss= 63.668524044263144\n",
      "iteration number =  5055 Cross Entropy Loss= 63.66670791825993\n",
      "iteration number =  5056 Cross Entropy Loss= 63.66489279257218\n",
      "iteration number =  5057 Cross Entropy Loss= 63.663078666729135\n",
      "iteration number =  5058 Cross Entropy Loss= 63.661265540260395\n",
      "iteration number =  5059 Cross Entropy Loss= 63.659453412695854\n",
      "iteration number =  5060 Cross Entropy Loss= 63.65764228356577\n",
      "iteration number =  5061 Cross Entropy Loss= 63.655832152400734\n",
      "iteration number =  5062 Cross Entropy Loss= 63.65402301873166\n",
      "iteration number =  5063 Cross Entropy Loss= 63.652214882089766\n",
      "iteration number =  5064 Cross Entropy Loss= 63.650407742006664\n",
      "iteration number =  5065 Cross Entropy Loss= 63.64860159801425\n",
      "iteration number =  5066 Cross Entropy Loss= 63.646796449644754\n",
      "iteration number =  5067 Cross Entropy Loss= 63.64499229643076\n",
      "iteration number =  5068 Cross Entropy Loss= 63.64318913790517\n",
      "iteration number =  5069 Cross Entropy Loss= 63.641386973601186\n",
      "iteration number =  5070 Cross Entropy Loss= 63.6395858030524\n",
      "iteration number =  5071 Cross Entropy Loss= 63.63778562579268\n",
      "iteration number =  5072 Cross Entropy Loss= 63.63598644135626\n",
      "iteration number =  5073 Cross Entropy Loss= 63.63418824927767\n",
      "iteration number =  5074 Cross Entropy Loss= 63.632391049091794\n",
      "iteration number =  5075 Cross Entropy Loss= 63.63059484033385\n",
      "iteration number =  5076 Cross Entropy Loss= 63.62879962253935\n",
      "iteration number =  5077 Cross Entropy Loss= 63.627005395244154\n",
      "iteration number =  5078 Cross Entropy Loss= 63.62521215798445\n",
      "iteration number =  5079 Cross Entropy Loss= 63.62341991029677\n",
      "iteration number =  5080 Cross Entropy Loss= 63.62162865171793\n",
      "iteration number =  5081 Cross Entropy Loss= 63.61983838178511\n",
      "iteration number =  5082 Cross Entropy Loss= 63.61804910003579\n",
      "iteration number =  5083 Cross Entropy Loss= 63.616260806007816\n",
      "iteration number =  5084 Cross Entropy Loss= 63.61447349923931\n",
      "iteration number =  5085 Cross Entropy Loss= 63.61268717926874\n",
      "iteration number =  5086 Cross Entropy Loss= 63.61090184563491\n",
      "iteration number =  5087 Cross Entropy Loss= 63.60911749787692\n",
      "iteration number =  5088 Cross Entropy Loss= 63.60733413553425\n",
      "iteration number =  5089 Cross Entropy Loss= 63.60555175814666\n",
      "iteration number =  5090 Cross Entropy Loss= 63.60377036525422\n",
      "iteration number =  5091 Cross Entropy Loss= 63.60198995639736\n",
      "iteration number =  5092 Cross Entropy Loss= 63.600210531116815\n",
      "iteration number =  5093 Cross Entropy Loss= 63.59843208895366\n",
      "iteration number =  5094 Cross Entropy Loss= 63.59665462944926\n",
      "iteration number =  5095 Cross Entropy Loss= 63.59487815214533\n",
      "iteration number =  5096 Cross Entropy Loss= 63.59310265658391\n",
      "iteration number =  5097 Cross Entropy Loss= 63.59132814230733\n",
      "iteration number =  5098 Cross Entropy Loss= 63.58955460885828\n",
      "iteration number =  5099 Cross Entropy Loss= 63.587782055779755\n",
      "iteration number =  5100 Cross Entropy Loss= 63.58601048261505\n",
      "iteration number =  5101 Cross Entropy Loss= 63.58423988890783\n",
      "iteration number =  5102 Cross Entropy Loss= 63.58247027420203\n",
      "iteration number =  5103 Cross Entropy Loss= 63.58070163804194\n",
      "iteration number =  5104 Cross Entropy Loss= 63.57893397997215\n",
      "iteration number =  5105 Cross Entropy Loss= 63.57716729953757\n",
      "iteration number =  5106 Cross Entropy Loss= 63.57540159628344\n",
      "iteration number =  5107 Cross Entropy Loss= 63.57363686975532\n",
      "iteration number =  5108 Cross Entropy Loss= 63.57187311949909\n",
      "iteration number =  5109 Cross Entropy Loss= 63.57011034506092\n",
      "iteration number =  5110 Cross Entropy Loss= 63.568348545987334\n",
      "iteration number =  5111 Cross Entropy Loss= 63.566587721825144\n",
      "iteration number =  5112 Cross Entropy Loss= 63.564827872121526\n",
      "iteration number =  5113 Cross Entropy Loss= 63.56306899642392\n",
      "iteration number =  5114 Cross Entropy Loss= 63.561311094280114\n",
      "iteration number =  5115 Cross Entropy Loss= 63.5595541652382\n",
      "iteration number =  5116 Cross Entropy Loss= 63.55779820884659\n",
      "iteration number =  5117 Cross Entropy Loss= 63.55604322465403\n",
      "iteration number =  5118 Cross Entropy Loss= 63.55428921220955\n",
      "iteration number =  5119 Cross Entropy Loss= 63.55253617106251\n",
      "iteration number =  5120 Cross Entropy Loss= 63.55078410076259\n",
      "iteration number =  5121 Cross Entropy Loss= 63.54903300085977\n",
      "iteration number =  5122 Cross Entropy Loss= 63.5472828709044\n",
      "iteration number =  5123 Cross Entropy Loss= 63.54553371044703\n",
      "iteration number =  5124 Cross Entropy Loss= 63.54378551903866\n",
      "iteration number =  5125 Cross Entropy Loss= 63.54203829623051\n",
      "iteration number =  5126 Cross Entropy Loss= 63.540292041574155\n",
      "iteration number =  5127 Cross Entropy Loss= 63.53854675462145\n",
      "iteration number =  5128 Cross Entropy Loss= 63.53680243492461\n",
      "iteration number =  5129 Cross Entropy Loss= 63.53505908203613\n",
      "iteration number =  5130 Cross Entropy Loss= 63.533316695508816\n",
      "iteration number =  5131 Cross Entropy Loss= 63.5315752748958\n",
      "iteration number =  5132 Cross Entropy Loss= 63.529834819750526\n",
      "iteration number =  5133 Cross Entropy Loss= 63.52809532962675\n",
      "iteration number =  5134 Cross Entropy Loss= 63.526356804078524\n",
      "iteration number =  5135 Cross Entropy Loss= 63.52461924266022\n",
      "iteration number =  5136 Cross Entropy Loss= 63.522882644926554\n",
      "iteration number =  5137 Cross Entropy Loss= 63.521147010432486\n",
      "iteration number =  5138 Cross Entropy Loss= 63.519412338733346\n",
      "iteration number =  5139 Cross Entropy Loss= 63.51767862938472\n",
      "iteration number =  5140 Cross Entropy Loss= 63.51594588194257\n",
      "iteration number =  5141 Cross Entropy Loss= 63.51421409596311\n",
      "iteration number =  5142 Cross Entropy Loss= 63.512483271002914\n",
      "iteration number =  5143 Cross Entropy Loss= 63.5107534066188\n",
      "iteration number =  5144 Cross Entropy Loss= 63.50902450236795\n",
      "iteration number =  5145 Cross Entropy Loss= 63.507296557807834\n",
      "iteration number =  5146 Cross Entropy Loss= 63.50556957249624\n",
      "iteration number =  5147 Cross Entropy Loss= 63.50384354599126\n",
      "iteration number =  5148 Cross Entropy Loss= 63.50211847785127\n",
      "iteration number =  5149 Cross Entropy Loss= 63.500394367634975\n",
      "iteration number =  5150 Cross Entropy Loss= 63.49867121490141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5151 Cross Entropy Loss= 63.496949019209865\n",
      "iteration number =  5152 Cross Entropy Loss= 63.49522778011999\n",
      "iteration number =  5153 Cross Entropy Loss= 63.4935074971917\n",
      "iteration number =  5154 Cross Entropy Loss= 63.49178816998523\n",
      "iteration number =  5155 Cross Entropy Loss= 63.490069798061135\n",
      "iteration number =  5156 Cross Entropy Loss= 63.488352380980245\n",
      "iteration number =  5157 Cross Entropy Loss= 63.48663591830374\n",
      "iteration number =  5158 Cross Entropy Loss= 63.48492040959307\n",
      "iteration number =  5159 Cross Entropy Loss= 63.48320585440999\n",
      "iteration number =  5160 Cross Entropy Loss= 63.48149225231655\n",
      "iteration number =  5161 Cross Entropy Loss= 63.47977960287517\n",
      "iteration number =  5162 Cross Entropy Loss= 63.478067905648494\n",
      "iteration number =  5163 Cross Entropy Loss= 63.476357160199505\n",
      "iteration number =  5164 Cross Entropy Loss= 63.4746473660915\n",
      "iteration number =  5165 Cross Entropy Loss= 63.47293852288804\n",
      "iteration number =  5166 Cross Entropy Loss= 63.47123063015306\n",
      "iteration number =  5167 Cross Entropy Loss= 63.46952368745072\n",
      "iteration number =  5168 Cross Entropy Loss= 63.4678176943455\n",
      "iteration number =  5169 Cross Entropy Loss= 63.46611265040221\n",
      "iteration number =  5170 Cross Entropy Loss= 63.46440855518597\n",
      "iteration number =  5171 Cross Entropy Loss= 63.46270540826217\n",
      "iteration number =  5172 Cross Entropy Loss= 63.461003209196505\n",
      "iteration number =  5173 Cross Entropy Loss= 63.45930195755497\n",
      "iteration number =  5174 Cross Entropy Loss= 63.45760165290388\n",
      "iteration number =  5175 Cross Entropy Loss= 63.45590229480986\n",
      "iteration number =  5176 Cross Entropy Loss= 63.45420388283978\n",
      "iteration number =  5177 Cross Entropy Loss= 63.45250641656085\n",
      "iteration number =  5178 Cross Entropy Loss= 63.450809895540594\n",
      "iteration number =  5179 Cross Entropy Loss= 63.449114319346805\n",
      "iteration number =  5180 Cross Entropy Loss= 63.44741968754757\n",
      "iteration number =  5181 Cross Entropy Loss= 63.44572599971133\n",
      "iteration number =  5182 Cross Entropy Loss= 63.444033255406744\n",
      "iteration number =  5183 Cross Entropy Loss= 63.44234145420284\n",
      "iteration number =  5184 Cross Entropy Loss= 63.440650595668906\n",
      "iteration number =  5185 Cross Entropy Loss= 63.438960679374546\n",
      "iteration number =  5186 Cross Entropy Loss= 63.43727170488964\n",
      "iteration number =  5187 Cross Entropy Loss= 63.43558367178438\n",
      "iteration number =  5188 Cross Entropy Loss= 63.43389657962925\n",
      "iteration number =  5189 Cross Entropy Loss= 63.43221042799506\n",
      "iteration number =  5190 Cross Entropy Loss= 63.430525216452864\n",
      "iteration number =  5191 Cross Entropy Loss= 63.42884094457405\n",
      "iteration number =  5192 Cross Entropy Loss= 63.4271576119303\n",
      "iteration number =  5193 Cross Entropy Loss= 63.42547521809358\n",
      "iteration number =  5194 Cross Entropy Loss= 63.42379376263615\n",
      "iteration number =  5195 Cross Entropy Loss= 63.42211324513056\n",
      "iteration number =  5196 Cross Entropy Loss= 63.420433665149694\n",
      "iteration number =  5197 Cross Entropy Loss= 63.41875502226669\n",
      "iteration number =  5198 Cross Entropy Loss= 63.41707731605498\n",
      "iteration number =  5199 Cross Entropy Loss= 63.41540054608832\n",
      "iteration number =  5200 Cross Entropy Loss= 63.41372471194075\n",
      "iteration number =  5201 Cross Entropy Loss= 63.41204981318658\n",
      "iteration number =  5202 Cross Entropy Loss= 63.41037584940044\n",
      "iteration number =  5203 Cross Entropy Loss= 63.40870282015726\n",
      "iteration number =  5204 Cross Entropy Loss= 63.40703072503221\n",
      "iteration number =  5205 Cross Entropy Loss= 63.405359563600825\n",
      "iteration number =  5206 Cross Entropy Loss= 63.40368933543889\n",
      "iteration number =  5207 Cross Entropy Loss= 63.40202004012247\n",
      "iteration number =  5208 Cross Entropy Loss= 63.40035167722798\n",
      "iteration number =  5209 Cross Entropy Loss= 63.39868424633206\n",
      "iteration number =  5210 Cross Entropy Loss= 63.39701774701168\n",
      "iteration number =  5211 Cross Entropy Loss= 63.39535217884411\n",
      "iteration number =  5212 Cross Entropy Loss= 63.39368754140686\n",
      "iteration number =  5213 Cross Entropy Loss= 63.39202383427778\n",
      "iteration number =  5214 Cross Entropy Loss= 63.390361057035\n",
      "iteration number =  5215 Cross Entropy Loss= 63.388699209256934\n",
      "iteration number =  5216 Cross Entropy Loss= 63.38703829052228\n",
      "iteration number =  5217 Cross Entropy Loss= 63.38537830041004\n",
      "iteration number =  5218 Cross Entropy Loss= 63.38371923849949\n",
      "iteration number =  5219 Cross Entropy Loss= 63.38206110437021\n",
      "iteration number =  5220 Cross Entropy Loss= 63.38040389760207\n",
      "iteration number =  5221 Cross Entropy Loss= 63.378747617775204\n",
      "iteration number =  5222 Cross Entropy Loss= 63.37709226447008\n",
      "iteration number =  5223 Cross Entropy Loss= 63.3754378372674\n",
      "iteration number =  5224 Cross Entropy Loss= 63.373784335748184\n",
      "iteration number =  5225 Cross Entropy Loss= 63.37213175949375\n",
      "iteration number =  5226 Cross Entropy Loss= 63.37048010808569\n",
      "iteration number =  5227 Cross Entropy Loss= 63.36882938110584\n",
      "iteration number =  5228 Cross Entropy Loss= 63.36717957813643\n",
      "iteration number =  5229 Cross Entropy Loss= 63.36553069875988\n",
      "iteration number =  5230 Cross Entropy Loss= 63.36388274255893\n",
      "iteration number =  5231 Cross Entropy Loss= 63.36223570911663\n",
      "iteration number =  5232 Cross Entropy Loss= 63.36058959801625\n",
      "iteration number =  5233 Cross Entropy Loss= 63.358944408841424\n",
      "iteration number =  5234 Cross Entropy Loss= 63.35730014117601\n",
      "iteration number =  5235 Cross Entropy Loss= 63.355656794604194\n",
      "iteration number =  5236 Cross Entropy Loss= 63.35401436871043\n",
      "iteration number =  5237 Cross Entropy Loss= 63.35237286307946\n",
      "iteration number =  5238 Cross Entropy Loss= 63.350732277296274\n",
      "iteration number =  5239 Cross Entropy Loss= 63.34909261094622\n",
      "iteration number =  5240 Cross Entropy Loss= 63.34745386361487\n",
      "iteration number =  5241 Cross Entropy Loss= 63.34581603488812\n",
      "iteration number =  5242 Cross Entropy Loss= 63.344179124352095\n",
      "iteration number =  5243 Cross Entropy Loss= 63.34254313159327\n",
      "iteration number =  5244 Cross Entropy Loss= 63.34090805619834\n",
      "iteration number =  5245 Cross Entropy Loss= 63.33927389775434\n",
      "iteration number =  5246 Cross Entropy Loss= 63.33764065584855\n",
      "iteration number =  5247 Cross Entropy Loss= 63.33600833006856\n",
      "iteration number =  5248 Cross Entropy Loss= 63.334376920002185\n",
      "iteration number =  5249 Cross Entropy Loss= 63.332746425237616\n",
      "iteration number =  5250 Cross Entropy Loss= 63.33111684536324\n",
      "iteration number =  5251 Cross Entropy Loss= 63.329488179967754\n",
      "iteration number =  5252 Cross Entropy Loss= 63.327860428640165\n",
      "iteration number =  5253 Cross Entropy Loss= 63.3262335909697\n",
      "iteration number =  5254 Cross Entropy Loss= 63.324607666545944\n",
      "iteration number =  5255 Cross Entropy Loss= 63.32298265495869\n",
      "iteration number =  5256 Cross Entropy Loss= 63.32135855579805\n",
      "iteration number =  5257 Cross Entropy Loss= 63.31973536865443\n",
      "iteration number =  5258 Cross Entropy Loss= 63.31811309311847\n",
      "iteration number =  5259 Cross Entropy Loss= 63.3164917287811\n",
      "iteration number =  5260 Cross Entropy Loss= 63.314871275233585\n",
      "iteration number =  5261 Cross Entropy Loss= 63.3132517320674\n",
      "iteration number =  5262 Cross Entropy Loss= 63.31163309887434\n",
      "iteration number =  5263 Cross Entropy Loss= 63.31001537524644\n",
      "iteration number =  5264 Cross Entropy Loss= 63.308398560776055\n",
      "iteration number =  5265 Cross Entropy Loss= 63.30678265505581\n",
      "iteration number =  5266 Cross Entropy Loss= 63.30516765767858\n",
      "iteration number =  5267 Cross Entropy Loss= 63.30355356823753\n",
      "iteration number =  5268 Cross Entropy Loss= 63.30194038632615\n",
      "iteration number =  5269 Cross Entropy Loss= 63.30032811153812\n",
      "iteration number =  5270 Cross Entropy Loss= 63.298716743467466\n",
      "iteration number =  5271 Cross Entropy Loss= 63.29710628170845\n",
      "iteration number =  5272 Cross Entropy Loss= 63.29549672585565\n",
      "iteration number =  5273 Cross Entropy Loss= 63.29388807550389\n",
      "iteration number =  5274 Cross Entropy Loss= 63.29228033024827\n",
      "iteration number =  5275 Cross Entropy Loss= 63.29067348968417\n",
      "iteration number =  5276 Cross Entropy Loss= 63.28906755340726\n",
      "iteration number =  5277 Cross Entropy Loss= 63.287462521013474\n",
      "iteration number =  5278 Cross Entropy Loss= 63.28585839209901\n",
      "iteration number =  5279 Cross Entropy Loss= 63.28425516626037\n",
      "iteration number =  5280 Cross Entropy Loss= 63.2826528430943\n",
      "iteration number =  5281 Cross Entropy Loss= 63.28105142219782\n",
      "iteration number =  5282 Cross Entropy Loss= 63.27945090316827\n",
      "iteration number =  5283 Cross Entropy Loss= 63.27785128560319\n",
      "iteration number =  5284 Cross Entropy Loss= 63.27625256910048\n",
      "iteration number =  5285 Cross Entropy Loss= 63.27465475325823\n",
      "iteration number =  5286 Cross Entropy Loss= 63.273057837674855\n",
      "iteration number =  5287 Cross Entropy Loss= 63.27146182194903\n",
      "iteration number =  5288 Cross Entropy Loss= 63.269866705679696\n",
      "iteration number =  5289 Cross Entropy Loss= 63.26827248846608\n",
      "iteration number =  5290 Cross Entropy Loss= 63.26667916990766\n",
      "iteration number =  5291 Cross Entropy Loss= 63.2650867496042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5292 Cross Entropy Loss= 63.26349522715576\n",
      "iteration number =  5293 Cross Entropy Loss= 63.26190460216261\n",
      "iteration number =  5294 Cross Entropy Loss= 63.260314874225344\n",
      "iteration number =  5295 Cross Entropy Loss= 63.258726042944815\n",
      "iteration number =  5296 Cross Entropy Loss= 63.257138107922124\n",
      "iteration number =  5297 Cross Entropy Loss= 63.25555106875868\n",
      "iteration number =  5298 Cross Entropy Loss= 63.25396492505614\n",
      "iteration number =  5299 Cross Entropy Loss= 63.252379676416425\n",
      "iteration number =  5300 Cross Entropy Loss= 63.25079532244173\n",
      "iteration number =  5301 Cross Entropy Loss= 63.24921186273457\n",
      "iteration number =  5302 Cross Entropy Loss= 63.24762929689761\n",
      "iteration number =  5303 Cross Entropy Loss= 63.24604762453392\n",
      "iteration number =  5304 Cross Entropy Loss= 63.24446684524674\n",
      "iteration number =  5305 Cross Entropy Loss= 63.24288695863963\n",
      "iteration number =  5306 Cross Entropy Loss= 63.24130796431643\n",
      "iteration number =  5307 Cross Entropy Loss= 63.23972986188116\n",
      "iteration number =  5308 Cross Entropy Loss= 63.23815265093823\n",
      "iteration number =  5309 Cross Entropy Loss= 63.23657633109224\n",
      "iteration number =  5310 Cross Entropy Loss= 63.235000901948055\n",
      "iteration number =  5311 Cross Entropy Loss= 63.23342636311086\n",
      "iteration number =  5312 Cross Entropy Loss= 63.23185271418606\n",
      "iteration number =  5313 Cross Entropy Loss= 63.230279954779334\n",
      "iteration number =  5314 Cross Entropy Loss= 63.22870808449666\n",
      "iteration number =  5315 Cross Entropy Loss= 63.22713710294424\n",
      "iteration number =  5316 Cross Entropy Loss= 63.225567009728564\n",
      "iteration number =  5317 Cross Entropy Loss= 63.223997804456395\n",
      "iteration number =  5318 Cross Entropy Loss= 63.222429486734725\n",
      "iteration number =  5319 Cross Entropy Loss= 63.22086205617089\n",
      "iteration number =  5320 Cross Entropy Loss= 63.21929551237238\n",
      "iteration number =  5321 Cross Entropy Loss= 63.21772985494705\n",
      "iteration number =  5322 Cross Entropy Loss= 63.216165083502965\n",
      "iteration number =  5323 Cross Entropy Loss= 63.21460119764848\n",
      "iteration number =  5324 Cross Entropy Loss= 63.21303819699217\n",
      "iteration number =  5325 Cross Entropy Loss= 63.211476081142955\n",
      "iteration number =  5326 Cross Entropy Loss= 63.20991484970995\n",
      "iteration number =  5327 Cross Entropy Loss= 63.208354502302555\n",
      "iteration number =  5328 Cross Entropy Loss= 63.20679503853042\n",
      "iteration number =  5329 Cross Entropy Loss= 63.2052364580035\n",
      "iteration number =  5330 Cross Entropy Loss= 63.203678760331975\n",
      "iteration number =  5331 Cross Entropy Loss= 63.202121945126294\n",
      "iteration number =  5332 Cross Entropy Loss= 63.20056601199717\n",
      "iteration number =  5333 Cross Entropy Loss= 63.19901096055561\n",
      "iteration number =  5334 Cross Entropy Loss= 63.19745679041281\n",
      "iteration number =  5335 Cross Entropy Loss= 63.195903501180304\n",
      "iteration number =  5336 Cross Entropy Loss= 63.19435109246985\n",
      "iteration number =  5337 Cross Entropy Loss= 63.192799563893466\n",
      "iteration number =  5338 Cross Entropy Loss= 63.191248915063454\n",
      "iteration number =  5339 Cross Entropy Loss= 63.18969914559236\n",
      "iteration number =  5340 Cross Entropy Loss= 63.188150255092985\n",
      "iteration number =  5341 Cross Entropy Loss= 63.1866022431784\n",
      "iteration number =  5342 Cross Entropy Loss= 63.185055109461935\n",
      "iteration number =  5343 Cross Entropy Loss= 63.183508853557186\n",
      "iteration number =  5344 Cross Entropy Loss= 63.18196347507801\n",
      "iteration number =  5345 Cross Entropy Loss= 63.180418973638496\n",
      "iteration number =  5346 Cross Entropy Loss= 63.178875348853026\n",
      "iteration number =  5347 Cross Entropy Loss= 63.17733260033626\n",
      "iteration number =  5348 Cross Entropy Loss= 63.175790727703024\n",
      "iteration number =  5349 Cross Entropy Loss= 63.17424973056851\n",
      "iteration number =  5350 Cross Entropy Loss= 63.17270960854812\n",
      "iteration number =  5351 Cross Entropy Loss= 63.17117036125751\n",
      "iteration number =  5352 Cross Entropy Loss= 63.1696319883126\n",
      "iteration number =  5353 Cross Entropy Loss= 63.16809448932958\n",
      "iteration number =  5354 Cross Entropy Loss= 63.166557863924886\n",
      "iteration number =  5355 Cross Entropy Loss= 63.16502211171522\n",
      "iteration number =  5356 Cross Entropy Loss= 63.163487232317514\n",
      "iteration number =  5357 Cross Entropy Loss= 63.161953225348995\n",
      "iteration number =  5358 Cross Entropy Loss= 63.16042009042714\n",
      "iteration number =  5359 Cross Entropy Loss= 63.158887827169664\n",
      "iteration number =  5360 Cross Entropy Loss= 63.15735643519456\n",
      "iteration number =  5361 Cross Entropy Loss= 63.15582591412002\n",
      "iteration number =  5362 Cross Entropy Loss= 63.15429626356459\n",
      "iteration number =  5363 Cross Entropy Loss= 63.152767483147\n",
      "iteration number =  5364 Cross Entropy Loss= 63.151239572486254\n",
      "iteration number =  5365 Cross Entropy Loss= 63.149712531201615\n",
      "iteration number =  5366 Cross Entropy Loss= 63.1481863589126\n",
      "iteration number =  5367 Cross Entropy Loss= 63.146661055238965\n",
      "iteration number =  5368 Cross Entropy Loss= 63.145136619800766\n",
      "iteration number =  5369 Cross Entropy Loss= 63.143613052218264\n",
      "iteration number =  5370 Cross Entropy Loss= 63.14209035211198\n",
      "iteration number =  5371 Cross Entropy Loss= 63.14056851910273\n",
      "iteration number =  5372 Cross Entropy Loss= 63.13904755281153\n",
      "iteration number =  5373 Cross Entropy Loss= 63.13752745285971\n",
      "iteration number =  5374 Cross Entropy Loss= 63.13600821886878\n",
      "iteration number =  5375 Cross Entropy Loss= 63.134489850460575\n",
      "iteration number =  5376 Cross Entropy Loss= 63.13297234725713\n",
      "iteration number =  5377 Cross Entropy Loss= 63.131455708880786\n",
      "iteration number =  5378 Cross Entropy Loss= 63.12993993495406\n",
      "iteration number =  5379 Cross Entropy Loss= 63.1284250250998\n",
      "iteration number =  5380 Cross Entropy Loss= 63.126910978941055\n",
      "iteration number =  5381 Cross Entropy Loss= 63.125397796101154\n",
      "iteration number =  5382 Cross Entropy Loss= 63.123885476203654\n",
      "iteration number =  5383 Cross Entropy Loss= 63.1223740188724\n",
      "iteration number =  5384 Cross Entropy Loss= 63.12086342373145\n",
      "iteration number =  5385 Cross Entropy Loss= 63.11935369040513\n",
      "iteration number =  5386 Cross Entropy Loss= 63.11784481851802\n",
      "iteration number =  5387 Cross Entropy Loss= 63.11633680769493\n",
      "iteration number =  5388 Cross Entropy Loss= 63.11482965756095\n",
      "iteration number =  5389 Cross Entropy Loss= 63.113323367741415\n",
      "iteration number =  5390 Cross Entropy Loss= 63.111817937861886\n",
      "iteration number =  5391 Cross Entropy Loss= 63.11031336754819\n",
      "iteration number =  5392 Cross Entropy Loss= 63.10880965642644\n",
      "iteration number =  5393 Cross Entropy Loss= 63.107306804122906\n",
      "iteration number =  5394 Cross Entropy Loss= 63.1058048102642\n",
      "iteration number =  5395 Cross Entropy Loss= 63.104303674477144\n",
      "iteration number =  5396 Cross Entropy Loss= 63.10280339638881\n",
      "iteration number =  5397 Cross Entropy Loss= 63.10130397562651\n",
      "iteration number =  5398 Cross Entropy Loss= 63.099805411817826\n",
      "iteration number =  5399 Cross Entropy Loss= 63.09830770459057\n",
      "iteration number =  5400 Cross Entropy Loss= 63.09681085357281\n",
      "iteration number =  5401 Cross Entropy Loss= 63.09531485839287\n",
      "iteration number =  5402 Cross Entropy Loss= 63.0938197186793\n",
      "iteration number =  5403 Cross Entropy Loss= 63.092325434060925\n",
      "iteration number =  5404 Cross Entropy Loss= 63.09083200416679\n",
      "iteration number =  5405 Cross Entropy Loss= 63.089339428626204\n",
      "iteration number =  5406 Cross Entropy Loss= 63.08784770706872\n",
      "iteration number =  5407 Cross Entropy Loss= 63.086356839124136\n",
      "iteration number =  5408 Cross Entropy Loss= 63.0848668244225\n",
      "iteration number =  5409 Cross Entropy Loss= 63.083377662594074\n",
      "iteration number =  5410 Cross Entropy Loss= 63.081889353269425\n",
      "iteration number =  5411 Cross Entropy Loss= 63.080401896079344\n",
      "iteration number =  5412 Cross Entropy Loss= 63.078915290654834\n",
      "iteration number =  5413 Cross Entropy Loss= 63.07742953662717\n",
      "iteration number =  5414 Cross Entropy Loss= 63.07594463362788\n",
      "iteration number =  5415 Cross Entropy Loss= 63.07446058128873\n",
      "iteration number =  5416 Cross Entropy Loss= 63.07297737924171\n",
      "iteration number =  5417 Cross Entropy Loss= 63.0714950271191\n",
      "iteration number =  5418 Cross Entropy Loss= 63.070013524553374\n",
      "iteration number =  5419 Cross Entropy Loss= 63.068532871177275\n",
      "iteration number =  5420 Cross Entropy Loss= 63.067053066623814\n",
      "iteration number =  5421 Cross Entropy Loss= 63.06557411052619\n",
      "iteration number =  5422 Cross Entropy Loss= 63.064096002517886\n",
      "iteration number =  5423 Cross Entropy Loss= 63.062618742232615\n",
      "iteration number =  5424 Cross Entropy Loss= 63.06114232930433\n",
      "iteration number =  5425 Cross Entropy Loss= 63.05966676336727\n",
      "iteration number =  5426 Cross Entropy Loss= 63.05819204405582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5427 Cross Entropy Loss= 63.05671817100472\n",
      "iteration number =  5428 Cross Entropy Loss= 63.055245143848865\n",
      "iteration number =  5429 Cross Entropy Loss= 63.05377296222345\n",
      "iteration number =  5430 Cross Entropy Loss= 63.052301625763874\n",
      "iteration number =  5431 Cross Entropy Loss= 63.050831134105806\n",
      "iteration number =  5432 Cross Entropy Loss= 63.04936148688512\n",
      "iteration number =  5433 Cross Entropy Loss= 63.04789268373797\n",
      "iteration number =  5434 Cross Entropy Loss= 63.046424724300735\n",
      "iteration number =  5435 Cross Entropy Loss= 63.04495760821004\n",
      "iteration number =  5436 Cross Entropy Loss= 63.043491335102736\n",
      "iteration number =  5437 Cross Entropy Loss= 63.042025904615926\n",
      "iteration number =  5438 Cross Entropy Loss= 63.04056131638694\n",
      "iteration number =  5439 Cross Entropy Loss= 63.039097570053386\n",
      "iteration number =  5440 Cross Entropy Loss= 63.03763466525308\n",
      "iteration number =  5441 Cross Entropy Loss= 63.03617260162407\n",
      "iteration number =  5442 Cross Entropy Loss= 63.03471137880467\n",
      "iteration number =  5443 Cross Entropy Loss= 63.03325099643341\n",
      "iteration number =  5444 Cross Entropy Loss= 63.03179145414907\n",
      "iteration number =  5445 Cross Entropy Loss= 63.03033275159068\n",
      "iteration number =  5446 Cross Entropy Loss= 63.02887488839748\n",
      "iteration number =  5447 Cross Entropy Loss= 63.027417864208985\n",
      "iteration number =  5448 Cross Entropy Loss= 63.02596167866492\n",
      "iteration number =  5449 Cross Entropy Loss= 63.02450633140526\n",
      "iteration number =  5450 Cross Entropy Loss= 63.0230518220702\n",
      "iteration number =  5451 Cross Entropy Loss= 63.02159815030022\n",
      "iteration number =  5452 Cross Entropy Loss= 63.02014531573598\n",
      "iteration number =  5453 Cross Entropy Loss= 63.01869331801841\n",
      "iteration number =  5454 Cross Entropy Loss= 63.01724215678867\n",
      "iteration number =  5455 Cross Entropy Loss= 63.015791831688155\n",
      "iteration number =  5456 Cross Entropy Loss= 63.01434234235849\n",
      "iteration number =  5457 Cross Entropy Loss= 63.01289368844158\n",
      "iteration number =  5458 Cross Entropy Loss= 63.01144586957949\n",
      "iteration number =  5459 Cross Entropy Loss= 63.00999888541459\n",
      "iteration number =  5460 Cross Entropy Loss= 63.00855273558945\n",
      "iteration number =  5461 Cross Entropy Loss= 63.00710741974687\n",
      "iteration number =  5462 Cross Entropy Loss= 63.005662937529934\n",
      "iteration number =  5463 Cross Entropy Loss= 63.00421928858189\n",
      "iteration number =  5464 Cross Entropy Loss= 63.0027764725463\n",
      "iteration number =  5465 Cross Entropy Loss= 63.00133448906689\n",
      "iteration number =  5466 Cross Entropy Loss= 62.99989333778766\n",
      "iteration number =  5467 Cross Entropy Loss= 62.99845301835284\n",
      "iteration number =  5468 Cross Entropy Loss= 62.99701353040687\n",
      "iteration number =  5469 Cross Entropy Loss= 62.99557487359448\n",
      "iteration number =  5470 Cross Entropy Loss= 62.99413704756056\n",
      "iteration number =  5471 Cross Entropy Loss= 62.9927000519503\n",
      "iteration number =  5472 Cross Entropy Loss= 62.99126388640908\n",
      "iteration number =  5473 Cross Entropy Loss= 62.98982855058253\n",
      "iteration number =  5474 Cross Entropy Loss= 62.98839404411652\n",
      "iteration number =  5475 Cross Entropy Loss= 62.98696036665716\n",
      "iteration number =  5476 Cross Entropy Loss= 62.98552751785074\n",
      "iteration number =  5477 Cross Entropy Loss= 62.98409549734386\n",
      "iteration number =  5478 Cross Entropy Loss= 62.98266430478328\n",
      "iteration number =  5479 Cross Entropy Loss= 62.98123393981605\n",
      "iteration number =  5480 Cross Entropy Loss= 62.97980440208942\n",
      "iteration number =  5481 Cross Entropy Loss= 62.97837569125089\n",
      "iteration number =  5482 Cross Entropy Loss= 62.97694780694816\n",
      "iteration number =  5483 Cross Entropy Loss= 62.975520748829204\n",
      "iteration number =  5484 Cross Entropy Loss= 62.9740945165422\n",
      "iteration number =  5485 Cross Entropy Loss= 62.97266910973556\n",
      "iteration number =  5486 Cross Entropy Loss= 62.97124452805794\n",
      "iteration number =  5487 Cross Entropy Loss= 62.9698207711582\n",
      "iteration number =  5488 Cross Entropy Loss= 62.968397838685476\n",
      "iteration number =  5489 Cross Entropy Loss= 62.96697573028908\n",
      "iteration number =  5490 Cross Entropy Loss= 62.96555444561859\n",
      "iteration number =  5491 Cross Entropy Loss= 62.964133984323816\n",
      "iteration number =  5492 Cross Entropy Loss= 62.96271434605477\n",
      "iteration number =  5493 Cross Entropy Loss= 62.96129553046172\n",
      "iteration number =  5494 Cross Entropy Loss= 62.95987753719517\n",
      "iteration number =  5495 Cross Entropy Loss= 62.9584603659058\n",
      "iteration number =  5496 Cross Entropy Loss= 62.95704401624459\n",
      "iteration number =  5497 Cross Entropy Loss= 62.95562848786268\n",
      "iteration number =  5498 Cross Entropy Loss= 62.95421378041152\n",
      "iteration number =  5499 Cross Entropy Loss= 62.95279989354272\n",
      "iteration number =  5500 Cross Entropy Loss= 62.95138682690812\n",
      "iteration number =  5501 Cross Entropy Loss= 62.94997458015985\n",
      "iteration number =  5502 Cross Entropy Loss= 62.9485631529502\n",
      "iteration number =  5503 Cross Entropy Loss= 62.947152544931726\n",
      "iteration number =  5504 Cross Entropy Loss= 62.94574275575718\n",
      "iteration number =  5505 Cross Entropy Loss= 62.944333785079586\n",
      "iteration number =  5506 Cross Entropy Loss= 62.94292563255216\n",
      "iteration number =  5507 Cross Entropy Loss= 62.94151829782837\n",
      "iteration number =  5508 Cross Entropy Loss= 62.940111780561864\n",
      "iteration number =  5509 Cross Entropy Loss= 62.93870608040659\n",
      "iteration number =  5510 Cross Entropy Loss= 62.93730119701665\n",
      "iteration number =  5511 Cross Entropy Loss= 62.93589713004644\n",
      "iteration number =  5512 Cross Entropy Loss= 62.934493879150516\n",
      "iteration number =  5513 Cross Entropy Loss= 62.933091443983685\n",
      "iteration number =  5514 Cross Entropy Loss= 62.931689824201015\n",
      "iteration number =  5515 Cross Entropy Loss= 62.93028901945774\n",
      "iteration number =  5516 Cross Entropy Loss= 62.928889029409376\n",
      "iteration number =  5517 Cross Entropy Loss= 62.927489853711634\n",
      "iteration number =  5518 Cross Entropy Loss= 62.92609149202042\n",
      "iteration number =  5519 Cross Entropy Loss= 62.924693943991926\n",
      "iteration number =  5520 Cross Entropy Loss= 62.92329720928256\n",
      "iteration number =  5521 Cross Entropy Loss= 62.92190128754889\n",
      "iteration number =  5522 Cross Entropy Loss= 62.92050617844778\n",
      "iteration number =  5523 Cross Entropy Loss= 62.9191118816363\n",
      "iteration number =  5524 Cross Entropy Loss= 62.91771839677171\n",
      "iteration number =  5525 Cross Entropy Loss= 62.91632572351154\n",
      "iteration number =  5526 Cross Entropy Loss= 62.914933861513504\n",
      "iteration number =  5527 Cross Entropy Loss= 62.91354281043558\n",
      "iteration number =  5528 Cross Entropy Loss= 62.91215256993594\n",
      "iteration number =  5529 Cross Entropy Loss= 62.910763139672966\n",
      "iteration number =  5530 Cross Entropy Loss= 62.909374519305324\n",
      "iteration number =  5531 Cross Entropy Loss= 62.907986708491826\n",
      "iteration number =  5532 Cross Entropy Loss= 62.906599706891555\n",
      "iteration number =  5533 Cross Entropy Loss= 62.90521351416379\n",
      "iteration number =  5534 Cross Entropy Loss= 62.90382812996808\n",
      "iteration number =  5535 Cross Entropy Loss= 62.90244355396413\n",
      "iteration number =  5536 Cross Entropy Loss= 62.90105978581191\n",
      "iteration number =  5537 Cross Entropy Loss= 62.899676825171596\n",
      "iteration number =  5538 Cross Entropy Loss= 62.898294671703596\n",
      "iteration number =  5539 Cross Entropy Loss= 62.896913325068525\n",
      "iteration number =  5540 Cross Entropy Loss= 62.89553278492723\n",
      "iteration number =  5541 Cross Entropy Loss= 62.89415305094076\n",
      "iteration number =  5542 Cross Entropy Loss= 62.892774122770426\n",
      "iteration number =  5543 Cross Entropy Loss= 62.89139600007772\n",
      "iteration number =  5544 Cross Entropy Loss= 62.89001868252436\n",
      "iteration number =  5545 Cross Entropy Loss= 62.88864216977231\n",
      "iteration number =  5546 Cross Entropy Loss= 62.887266461483705\n",
      "iteration number =  5547 Cross Entropy Loss= 62.88589155732098\n",
      "iteration number =  5548 Cross Entropy Loss= 62.88451745694669\n",
      "iteration number =  5549 Cross Entropy Loss= 62.88314416002369\n",
      "iteration number =  5550 Cross Entropy Loss= 62.881771666215\n",
      "iteration number =  5551 Cross Entropy Loss= 62.880399975183906\n",
      "iteration number =  5552 Cross Entropy Loss= 62.879029086593874\n",
      "iteration number =  5553 Cross Entropy Loss= 62.87765900010861\n",
      "iteration number =  5554 Cross Entropy Loss= 62.87628971539202\n",
      "iteration number =  5555 Cross Entropy Loss= 62.87492123210826\n",
      "iteration number =  5556 Cross Entropy Loss= 62.873553549921674\n",
      "iteration number =  5557 Cross Entropy Loss= 62.87218666849684\n",
      "iteration number =  5558 Cross Entropy Loss= 62.87082058749854\n",
      "iteration number =  5559 Cross Entropy Loss= 62.869455306591796\n",
      "iteration number =  5560 Cross Entropy Loss= 62.86809082544183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5561 Cross Entropy Loss= 62.86672714371408\n",
      "iteration number =  5562 Cross Entropy Loss= 62.8653642610742\n",
      "iteration number =  5563 Cross Entropy Loss= 62.86400217718807\n",
      "iteration number =  5564 Cross Entropy Loss= 62.862640891721796\n",
      "iteration number =  5565 Cross Entropy Loss= 62.86128040434167\n",
      "iteration number =  5566 Cross Entropy Loss= 62.85992071471424\n",
      "iteration number =  5567 Cross Entropy Loss= 62.85856182250623\n",
      "iteration number =  5568 Cross Entropy Loss= 62.857203727384615\n",
      "iteration number =  5569 Cross Entropy Loss= 62.85584642901655\n",
      "iteration number =  5570 Cross Entropy Loss= 62.85448992706945\n",
      "iteration number =  5571 Cross Entropy Loss= 62.8531342212109\n",
      "iteration number =  5572 Cross Entropy Loss= 62.85177931110873\n",
      "iteration number =  5573 Cross Entropy Loss= 62.850425196430976\n",
      "iteration number =  5574 Cross Entropy Loss= 62.84907187684588\n",
      "iteration number =  5575 Cross Entropy Loss= 62.847719352021926\n",
      "iteration number =  5576 Cross Entropy Loss= 62.84636762162779\n",
      "iteration number =  5577 Cross Entropy Loss= 62.84501668533234\n",
      "iteration number =  5578 Cross Entropy Loss= 62.843666542804726\n",
      "iteration number =  5579 Cross Entropy Loss= 62.84231719371426\n",
      "iteration number =  5580 Cross Entropy Loss= 62.84096863773047\n",
      "iteration number =  5581 Cross Entropy Loss= 62.839620874523106\n",
      "iteration number =  5582 Cross Entropy Loss= 62.838273903762136\n",
      "iteration number =  5583 Cross Entropy Loss= 62.83692772511775\n",
      "iteration number =  5584 Cross Entropy Loss= 62.835582338260316\n",
      "iteration number =  5585 Cross Entropy Loss= 62.834237742860466\n",
      "iteration number =  5586 Cross Entropy Loss= 62.832893938589\n",
      "iteration number =  5587 Cross Entropy Loss= 62.83155092511695\n",
      "iteration number =  5588 Cross Entropy Loss= 62.83020870211556\n",
      "iteration number =  5589 Cross Entropy Loss= 62.82886726925629\n",
      "iteration number =  5590 Cross Entropy Loss= 62.82752662621082\n",
      "iteration number =  5591 Cross Entropy Loss= 62.82618677265099\n",
      "iteration number =  5592 Cross Entropy Loss= 62.82484770824893\n",
      "iteration number =  5593 Cross Entropy Loss= 62.82350943267693\n",
      "iteration number =  5594 Cross Entropy Loss= 62.822171945607494\n",
      "iteration number =  5595 Cross Entropy Loss= 62.82083524671337\n",
      "iteration number =  5596 Cross Entropy Loss= 62.81949933566747\n",
      "iteration number =  5597 Cross Entropy Loss= 62.81816421214297\n",
      "iteration number =  5598 Cross Entropy Loss= 62.8168298758132\n",
      "iteration number =  5599 Cross Entropy Loss= 62.81549632635176\n",
      "iteration number =  5600 Cross Entropy Loss= 62.8141635634324\n",
      "iteration number =  5601 Cross Entropy Loss= 62.81283158672914\n",
      "iteration number =  5602 Cross Entropy Loss= 62.811500395916156\n",
      "iteration number =  5603 Cross Entropy Loss= 62.81016999066787\n",
      "iteration number =  5604 Cross Entropy Loss= 62.808840370658906\n",
      "iteration number =  5605 Cross Entropy Loss= 62.80751153556409\n",
      "iteration number =  5606 Cross Entropy Loss= 62.80618348505846\n",
      "iteration number =  5607 Cross Entropy Loss= 62.804856218817264\n",
      "iteration number =  5608 Cross Entropy Loss= 62.80352973651597\n",
      "iteration number =  5609 Cross Entropy Loss= 62.802204037830236\n",
      "iteration number =  5610 Cross Entropy Loss= 62.800879122435944\n",
      "iteration number =  5611 Cross Entropy Loss= 62.799554990009185\n",
      "iteration number =  5612 Cross Entropy Loss= 62.798231640226234\n",
      "iteration number =  5613 Cross Entropy Loss= 62.7969090727636\n",
      "iteration number =  5614 Cross Entropy Loss= 62.795587287297984\n",
      "iteration number =  5615 Cross Entropy Loss= 62.79426628350633\n",
      "iteration number =  5616 Cross Entropy Loss= 62.792946061065756\n",
      "iteration number =  5617 Cross Entropy Loss= 62.79162661965357\n",
      "iteration number =  5618 Cross Entropy Loss= 62.79030795894735\n",
      "iteration number =  5619 Cross Entropy Loss= 62.78899007862481\n",
      "iteration number =  5620 Cross Entropy Loss= 62.78767297836393\n",
      "iteration number =  5621 Cross Entropy Loss= 62.78635665784286\n",
      "iteration number =  5622 Cross Entropy Loss= 62.785041116739954\n",
      "iteration number =  5623 Cross Entropy Loss= 62.78372635473383\n",
      "iteration number =  5624 Cross Entropy Loss= 62.78241237150322\n",
      "iteration number =  5625 Cross Entropy Loss= 62.78109916672715\n",
      "iteration number =  5626 Cross Entropy Loss= 62.7797867400848\n",
      "iteration number =  5627 Cross Entropy Loss= 62.77847509125559\n",
      "iteration number =  5628 Cross Entropy Loss= 62.7771642199191\n",
      "iteration number =  5629 Cross Entropy Loss= 62.775854125755146\n",
      "iteration number =  5630 Cross Entropy Loss= 62.77454480844375\n",
      "iteration number =  5631 Cross Entropy Loss= 62.77323626766515\n",
      "iteration number =  5632 Cross Entropy Loss= 62.771928503099744\n",
      "iteration number =  5633 Cross Entropy Loss= 62.77062151442819\n",
      "iteration number =  5634 Cross Entropy Loss= 62.76931530133131\n",
      "iteration number =  5635 Cross Entropy Loss= 62.76800986349017\n",
      "iteration number =  5636 Cross Entropy Loss= 62.766705200585996\n",
      "iteration number =  5637 Cross Entropy Loss= 62.765401312300234\n",
      "iteration number =  5638 Cross Entropy Loss= 62.764098198314564\n",
      "iteration number =  5639 Cross Entropy Loss= 62.76279585831083\n",
      "iteration number =  5640 Cross Entropy Loss= 62.761494291971104\n",
      "iteration number =  5641 Cross Entropy Loss= 62.76019349897763\n",
      "iteration number =  5642 Cross Entropy Loss= 62.758893479012904\n",
      "iteration number =  5643 Cross Entropy Loss= 62.75759423175961\n",
      "iteration number =  5644 Cross Entropy Loss= 62.756295756900585\n",
      "iteration number =  5645 Cross Entropy Loss= 62.75499805411895\n",
      "iteration number =  5646 Cross Entropy Loss= 62.75370112309797\n",
      "iteration number =  5647 Cross Entropy Loss= 62.752404963521116\n",
      "iteration number =  5648 Cross Entropy Loss= 62.7511095750721\n",
      "iteration number =  5649 Cross Entropy Loss= 62.74981495743481\n",
      "iteration number =  5650 Cross Entropy Loss= 62.74852111029333\n",
      "iteration number =  5651 Cross Entropy Loss= 62.747228033331965\n",
      "iteration number =  5652 Cross Entropy Loss= 62.74593572623522\n",
      "iteration number =  5653 Cross Entropy Loss= 62.744644188687765\n",
      "iteration number =  5654 Cross Entropy Loss= 62.74335342037452\n",
      "iteration number =  5655 Cross Entropy Loss= 62.74206342098058\n",
      "iteration number =  5656 Cross Entropy Loss= 62.74077419019126\n",
      "iteration number =  5657 Cross Entropy Loss= 62.73948572769206\n",
      "iteration number =  5658 Cross Entropy Loss= 62.73819803316868\n",
      "iteration number =  5659 Cross Entropy Loss= 62.736911106307026\n",
      "iteration number =  5660 Cross Entropy Loss= 62.73562494679321\n",
      "iteration number =  5661 Cross Entropy Loss= 62.734339554313536\n",
      "iteration number =  5662 Cross Entropy Loss= 62.733054928554516\n",
      "iteration number =  5663 Cross Entropy Loss= 62.73177106920285\n",
      "iteration number =  5664 Cross Entropy Loss= 62.730487975945444\n",
      "iteration number =  5665 Cross Entropy Loss= 62.72920564846942\n",
      "iteration number =  5666 Cross Entropy Loss= 62.727924086462075\n",
      "iteration number =  5667 Cross Entropy Loss= 62.72664328961092\n",
      "iteration number =  5668 Cross Entropy Loss= 62.725363257603654\n",
      "iteration number =  5669 Cross Entropy Loss= 62.72408399012819\n",
      "iteration number =  5670 Cross Entropy Loss= 62.722805486872616\n",
      "iteration number =  5671 Cross Entropy Loss= 62.72152774752527\n",
      "iteration number =  5672 Cross Entropy Loss= 62.720250771774616\n",
      "iteration number =  5673 Cross Entropy Loss= 62.71897455930937\n",
      "iteration number =  5674 Cross Entropy Loss= 62.71769910981843\n",
      "iteration number =  5675 Cross Entropy Loss= 62.716424422990904\n",
      "iteration number =  5676 Cross Entropy Loss= 62.71515049851608\n",
      "iteration number =  5677 Cross Entropy Loss= 62.71387733608344\n",
      "iteration number =  5678 Cross Entropy Loss= 62.712604935382686\n",
      "iteration number =  5679 Cross Entropy Loss= 62.7113332961037\n",
      "iteration number =  5680 Cross Entropy Loss= 62.710062417936584\n",
      "iteration number =  5681 Cross Entropy Loss= 62.7087923005716\n",
      "iteration number =  5682 Cross Entropy Loss= 62.70752294369923\n",
      "iteration number =  5683 Cross Entropy Loss= 62.70625434701017\n",
      "iteration number =  5684 Cross Entropy Loss= 62.70498651019528\n",
      "iteration number =  5685 Cross Entropy Loss= 62.703719432945626\n",
      "iteration number =  5686 Cross Entropy Loss= 62.702453114952505\n",
      "iteration number =  5687 Cross Entropy Loss= 62.701187555907346\n",
      "iteration number =  5688 Cross Entropy Loss= 62.69992275550183\n",
      "iteration number =  5689 Cross Entropy Loss= 62.69865871342779\n",
      "iteration number =  5690 Cross Entropy Loss= 62.6973954293773\n",
      "iteration number =  5691 Cross Entropy Loss= 62.69613290304262\n",
      "iteration number =  5692 Cross Entropy Loss= 62.694871134116156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5693 Cross Entropy Loss= 62.693610122290565\n",
      "iteration number =  5694 Cross Entropy Loss= 62.692349867258685\n",
      "iteration number =  5695 Cross Entropy Loss= 62.69109036871353\n",
      "iteration number =  5696 Cross Entropy Loss= 62.68983162634836\n",
      "iteration number =  5697 Cross Entropy Loss= 62.68857363985656\n",
      "iteration number =  5698 Cross Entropy Loss= 62.68731640893175\n",
      "iteration number =  5699 Cross Entropy Loss= 62.68605993326773\n",
      "iteration number =  5700 Cross Entropy Loss= 62.684804212558525\n",
      "iteration number =  5701 Cross Entropy Loss= 62.683549246498316\n",
      "iteration number =  5702 Cross Entropy Loss= 62.68229503478151\n",
      "iteration number =  5703 Cross Entropy Loss= 62.68104157710266\n",
      "iteration number =  5704 Cross Entropy Loss= 62.67978887315658\n",
      "iteration number =  5705 Cross Entropy Loss= 62.678536922638216\n",
      "iteration number =  5706 Cross Entropy Loss= 62.677285725242754\n",
      "iteration number =  5707 Cross Entropy Loss= 62.67603528066553\n",
      "iteration number =  5708 Cross Entropy Loss= 62.67478558860211\n",
      "iteration number =  5709 Cross Entropy Loss= 62.67353664874823\n",
      "iteration number =  5710 Cross Entropy Loss= 62.67228846079984\n",
      "iteration number =  5711 Cross Entropy Loss= 62.67104102445307\n",
      "iteration number =  5712 Cross Entropy Loss= 62.66979433940422\n",
      "iteration number =  5713 Cross Entropy Loss= 62.66854840534983\n",
      "iteration number =  5714 Cross Entropy Loss= 62.6673032219866\n",
      "iteration number =  5715 Cross Entropy Loss= 62.66605878901142\n",
      "iteration number =  5716 Cross Entropy Loss= 62.6648151061214\n",
      "iteration number =  5717 Cross Entropy Loss= 62.66357217301381\n",
      "iteration number =  5718 Cross Entropy Loss= 62.66232998938611\n",
      "iteration number =  5719 Cross Entropy Loss= 62.661088554936\n",
      "iteration number =  5720 Cross Entropy Loss= 62.65984786936132\n",
      "iteration number =  5721 Cross Entropy Loss= 62.65860793236011\n",
      "iteration number =  5722 Cross Entropy Loss= 62.65736874363063\n",
      "iteration number =  5723 Cross Entropy Loss= 62.65613030287128\n",
      "iteration number =  5724 Cross Entropy Loss= 62.6548926097807\n",
      "iteration number =  5725 Cross Entropy Loss= 62.653655664057716\n",
      "iteration number =  5726 Cross Entropy Loss= 62.65241946540131\n",
      "iteration number =  5727 Cross Entropy Loss= 62.65118401351067\n",
      "iteration number =  5728 Cross Entropy Loss= 62.64994930808518\n",
      "iteration number =  5729 Cross Entropy Loss= 62.648715348824446\n",
      "iteration number =  5730 Cross Entropy Loss= 62.64748213542819\n",
      "iteration number =  5731 Cross Entropy Loss= 62.646249667596386\n",
      "iteration number =  5732 Cross Entropy Loss= 62.64501794502916\n",
      "iteration number =  5733 Cross Entropy Loss= 62.64378696742684\n",
      "iteration number =  5734 Cross Entropy Loss= 62.64255673448997\n",
      "iteration number =  5735 Cross Entropy Loss= 62.64132724591923\n",
      "iteration number =  5736 Cross Entropy Loss= 62.64009850141554\n",
      "iteration number =  5737 Cross Entropy Loss= 62.638870500679985\n",
      "iteration number =  5738 Cross Entropy Loss= 62.63764324341382\n",
      "iteration number =  5739 Cross Entropy Loss= 62.636416729318526\n",
      "iteration number =  5740 Cross Entropy Loss= 62.63519095809574\n",
      "iteration number =  5741 Cross Entropy Loss= 62.63396592944733\n",
      "iteration number =  5742 Cross Entropy Loss= 62.63274164307531\n",
      "iteration number =  5743 Cross Entropy Loss= 62.63151809868188\n",
      "iteration number =  5744 Cross Entropy Loss= 62.63029529596946\n",
      "iteration number =  5745 Cross Entropy Loss= 62.629073234640636\n",
      "iteration number =  5746 Cross Entropy Loss= 62.627851914398185\n",
      "iteration number =  5747 Cross Entropy Loss= 62.62663133494509\n",
      "iteration number =  5748 Cross Entropy Loss= 62.62541149598448\n",
      "iteration number =  5749 Cross Entropy Loss= 62.6241923972197\n",
      "iteration number =  5750 Cross Entropy Loss= 62.62297403835429\n",
      "iteration number =  5751 Cross Entropy Loss= 62.62175641909195\n",
      "iteration number =  5752 Cross Entropy Loss= 62.6205395391366\n",
      "iteration number =  5753 Cross Entropy Loss= 62.61932339819231\n",
      "iteration number =  5754 Cross Entropy Loss= 62.618107995963356\n",
      "iteration number =  5755 Cross Entropy Loss= 62.61689333215417\n",
      "iteration number =  5756 Cross Entropy Loss= 62.615679406469454\n",
      "iteration number =  5757 Cross Entropy Loss= 62.614466218613984\n",
      "iteration number =  5758 Cross Entropy Loss= 62.61325376829281\n",
      "iteration number =  5759 Cross Entropy Loss= 62.612042055211134\n",
      "iteration number =  5760 Cross Entropy Loss= 62.61083107907431\n",
      "iteration number =  5761 Cross Entropy Loss= 62.60962083958795\n",
      "iteration number =  5762 Cross Entropy Loss= 62.608411336457785\n",
      "iteration number =  5763 Cross Entropy Loss= 62.60720256938978\n",
      "iteration number =  5764 Cross Entropy Loss= 62.60599453809004\n",
      "iteration number =  5765 Cross Entropy Loss= 62.60478724226489\n",
      "iteration number =  5766 Cross Entropy Loss= 62.60358068162083\n",
      "iteration number =  5767 Cross Entropy Loss= 62.60237485586452\n",
      "iteration number =  5768 Cross Entropy Loss= 62.601169764702846\n",
      "iteration number =  5769 Cross Entropy Loss= 62.59996540784285\n",
      "iteration number =  5770 Cross Entropy Loss= 62.59876178499177\n",
      "iteration number =  5771 Cross Entropy Loss= 62.59755889585701\n",
      "iteration number =  5772 Cross Entropy Loss= 62.59635674014618\n",
      "iteration number =  5773 Cross Entropy Loss= 62.59515531756706\n",
      "iteration number =  5774 Cross Entropy Loss= 62.59395462782763\n",
      "iteration number =  5775 Cross Entropy Loss= 62.59275467063601\n",
      "iteration number =  5776 Cross Entropy Loss= 62.59155544570058\n",
      "iteration number =  5777 Cross Entropy Loss= 62.59035695272982\n",
      "iteration number =  5778 Cross Entropy Loss= 62.58915919143243\n",
      "iteration number =  5779 Cross Entropy Loss= 62.587962161517304\n",
      "iteration number =  5780 Cross Entropy Loss= 62.58676586269352\n",
      "iteration number =  5781 Cross Entropy Loss= 62.5855702946703\n",
      "iteration number =  5782 Cross Entropy Loss= 62.584375457157066\n",
      "iteration number =  5783 Cross Entropy Loss= 62.583181349863466\n",
      "iteration number =  5784 Cross Entropy Loss= 62.58198797249927\n",
      "iteration number =  5785 Cross Entropy Loss= 62.58079532477445\n",
      "iteration number =  5786 Cross Entropy Loss= 62.57960340639916\n",
      "iteration number =  5787 Cross Entropy Loss= 62.57841221708376\n",
      "iteration number =  5788 Cross Entropy Loss= 62.57722175653875\n",
      "iteration number =  5789 Cross Entropy Loss= 62.576032024474834\n",
      "iteration number =  5790 Cross Entropy Loss= 62.574843020602884\n",
      "iteration number =  5791 Cross Entropy Loss= 62.573654744634\n",
      "iteration number =  5792 Cross Entropy Loss= 62.572467196279376\n",
      "iteration number =  5793 Cross Entropy Loss= 62.57128037525046\n",
      "iteration number =  5794 Cross Entropy Loss= 62.570094281258875\n",
      "iteration number =  5795 Cross Entropy Loss= 62.56890891401636\n",
      "iteration number =  5796 Cross Entropy Loss= 62.56772427323492\n",
      "iteration number =  5797 Cross Entropy Loss= 62.566540358626696\n",
      "iteration number =  5798 Cross Entropy Loss= 62.565357169904\n",
      "iteration number =  5799 Cross Entropy Loss= 62.56417470677933\n",
      "iteration number =  5800 Cross Entropy Loss= 62.562992968965375\n",
      "iteration number =  5801 Cross Entropy Loss= 62.56181195617502\n",
      "iteration number =  5802 Cross Entropy Loss= 62.5606316681213\n",
      "iteration number =  5803 Cross Entropy Loss= 62.55945210451743\n",
      "iteration number =  5804 Cross Entropy Loss= 62.55827326507682\n",
      "iteration number =  5805 Cross Entropy Loss= 62.55709514951303\n",
      "iteration number =  5806 Cross Entropy Loss= 62.55591775753984\n",
      "iteration number =  5807 Cross Entropy Loss= 62.55474108887119\n",
      "iteration number =  5808 Cross Entropy Loss= 62.55356514322119\n",
      "iteration number =  5809 Cross Entropy Loss= 62.55238992030414\n",
      "iteration number =  5810 Cross Entropy Loss= 62.5512154198345\n",
      "iteration number =  5811 Cross Entropy Loss= 62.550041641526946\n",
      "iteration number =  5812 Cross Entropy Loss= 62.54886858509629\n",
      "iteration number =  5813 Cross Entropy Loss= 62.54769625025753\n",
      "iteration number =  5814 Cross Entropy Loss= 62.546524636725884\n",
      "iteration number =  5815 Cross Entropy Loss= 62.54535374421668\n",
      "iteration number =  5816 Cross Entropy Loss= 62.54418357244549\n",
      "iteration number =  5817 Cross Entropy Loss= 62.54301412112801\n",
      "iteration number =  5818 Cross Entropy Loss= 62.54184538998015\n",
      "iteration number =  5819 Cross Entropy Loss= 62.54067737871798\n",
      "iteration number =  5820 Cross Entropy Loss= 62.539510087057735\n",
      "iteration number =  5821 Cross Entropy Loss= 62.53834351471586\n",
      "iteration number =  5822 Cross Entropy Loss= 62.53717766140893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5823 Cross Entropy Loss= 62.53601252685375\n",
      "iteration number =  5824 Cross Entropy Loss= 62.534848110767264\n",
      "iteration number =  5825 Cross Entropy Loss= 62.533684412866606\n",
      "iteration number =  5826 Cross Entropy Loss= 62.53252143286909\n",
      "iteration number =  5827 Cross Entropy Loss= 62.53135917049219\n",
      "iteration number =  5828 Cross Entropy Loss= 62.53019762545357\n",
      "iteration number =  5829 Cross Entropy Loss= 62.52903679747105\n",
      "iteration number =  5830 Cross Entropy Loss= 62.52787668626267\n",
      "iteration number =  5831 Cross Entropy Loss= 62.52671729154659\n",
      "iteration number =  5832 Cross Entropy Loss= 62.525558613041184\n",
      "iteration number =  5833 Cross Entropy Loss= 62.52440065046498\n",
      "iteration number =  5834 Cross Entropy Loss= 62.52324340353671\n",
      "iteration number =  5835 Cross Entropy Loss= 62.52208687197524\n",
      "iteration number =  5836 Cross Entropy Loss= 62.52093105549963\n",
      "iteration number =  5837 Cross Entropy Loss= 62.51977595382912\n",
      "iteration number =  5838 Cross Entropy Loss= 62.51862156668314\n",
      "iteration number =  5839 Cross Entropy Loss= 62.517467893781244\n",
      "iteration number =  5840 Cross Entropy Loss= 62.516314934843194\n",
      "iteration number =  5841 Cross Entropy Loss= 62.515162689588934\n",
      "iteration number =  5842 Cross Entropy Loss= 62.51401115773858\n",
      "iteration number =  5843 Cross Entropy Loss= 62.51286033901239\n",
      "iteration number =  5844 Cross Entropy Loss= 62.511710233130835\n",
      "iteration number =  5845 Cross Entropy Loss= 62.51056083981452\n",
      "iteration number =  5846 Cross Entropy Loss= 62.50941215878427\n",
      "iteration number =  5847 Cross Entropy Loss= 62.50826418976105\n",
      "iteration number =  5848 Cross Entropy Loss= 62.50711693246602\n",
      "iteration number =  5849 Cross Entropy Loss= 62.50597038662046\n",
      "iteration number =  5850 Cross Entropy Loss= 62.504824551945916\n",
      "iteration number =  5851 Cross Entropy Loss= 62.503679428164006\n",
      "iteration number =  5852 Cross Entropy Loss= 62.5025350149966\n",
      "iteration number =  5853 Cross Entropy Loss= 62.50139131216571\n",
      "iteration number =  5854 Cross Entropy Loss= 62.5002483193935\n",
      "iteration number =  5855 Cross Entropy Loss= 62.49910603640234\n",
      "iteration number =  5856 Cross Entropy Loss= 62.49796446291472\n",
      "iteration number =  5857 Cross Entropy Loss= 62.4968235986534\n",
      "iteration number =  5858 Cross Entropy Loss= 62.49568344334121\n",
      "iteration number =  5859 Cross Entropy Loss= 62.4945439967012\n",
      "iteration number =  5860 Cross Entropy Loss= 62.49340525845659\n",
      "iteration number =  5861 Cross Entropy Loss= 62.49226722833076\n",
      "iteration number =  5862 Cross Entropy Loss= 62.49112990604729\n",
      "iteration number =  5863 Cross Entropy Loss= 62.48999329132987\n",
      "iteration number =  5864 Cross Entropy Loss= 62.48885738390241\n",
      "iteration number =  5865 Cross Entropy Loss= 62.487722183489\n",
      "iteration number =  5866 Cross Entropy Loss= 62.48658768981385\n",
      "iteration number =  5867 Cross Entropy Loss= 62.4854539026014\n",
      "iteration number =  5868 Cross Entropy Loss= 62.48432082157622\n",
      "iteration number =  5869 Cross Entropy Loss= 62.48318844646306\n",
      "iteration number =  5870 Cross Entropy Loss= 62.48205677698682\n",
      "iteration number =  5871 Cross Entropy Loss= 62.48092581287265\n",
      "iteration number =  5872 Cross Entropy Loss= 62.479795553845754\n",
      "iteration number =  5873 Cross Entropy Loss= 62.47866599963159\n",
      "iteration number =  5874 Cross Entropy Loss= 62.47753714995576\n",
      "iteration number =  5875 Cross Entropy Loss= 62.47640900454403\n",
      "iteration number =  5876 Cross Entropy Loss= 62.47528156312235\n",
      "iteration number =  5877 Cross Entropy Loss= 62.47415482541681\n",
      "iteration number =  5878 Cross Entropy Loss= 62.4730287911537\n",
      "iteration number =  5879 Cross Entropy Loss= 62.47190346005948\n",
      "iteration number =  5880 Cross Entropy Loss= 62.47077883186075\n",
      "iteration number =  5881 Cross Entropy Loss= 62.469654906284305\n",
      "iteration number =  5882 Cross Entropy Loss= 62.46853168305708\n",
      "iteration number =  5883 Cross Entropy Loss= 62.467409161906225\n",
      "iteration number =  5884 Cross Entropy Loss= 62.46628734255902\n",
      "iteration number =  5885 Cross Entropy Loss= 62.46516622474293\n",
      "iteration number =  5886 Cross Entropy Loss= 62.46404580818556\n",
      "iteration number =  5887 Cross Entropy Loss= 62.46292609261472\n",
      "iteration number =  5888 Cross Entropy Loss= 62.46180707775839\n",
      "iteration number =  5889 Cross Entropy Loss= 62.46068876334468\n",
      "iteration number =  5890 Cross Entropy Loss= 62.45957114910189\n",
      "iteration number =  5891 Cross Entropy Loss= 62.458454234758506\n",
      "iteration number =  5892 Cross Entropy Loss= 62.45733802004314\n",
      "iteration number =  5893 Cross Entropy Loss= 62.4562225046846\n",
      "iteration number =  5894 Cross Entropy Loss= 62.45510768841184\n",
      "iteration number =  5895 Cross Entropy Loss= 62.453993570954026\n",
      "iteration number =  5896 Cross Entropy Loss= 62.452880152040436\n",
      "iteration number =  5897 Cross Entropy Loss= 62.45176743140053\n",
      "iteration number =  5898 Cross Entropy Loss= 62.450655408763986\n",
      "iteration number =  5899 Cross Entropy Loss= 62.44954408386056\n",
      "iteration number =  5900 Cross Entropy Loss= 62.44843345642023\n",
      "iteration number =  5901 Cross Entropy Loss= 62.44732352617315\n",
      "iteration number =  5902 Cross Entropy Loss= 62.446214292849604\n",
      "iteration number =  5903 Cross Entropy Loss= 62.445105756180055\n",
      "iteration number =  5904 Cross Entropy Loss= 62.443997915895146\n",
      "iteration number =  5905 Cross Entropy Loss= 62.44289077172567\n",
      "iteration number =  5906 Cross Entropy Loss= 62.441784323402594\n",
      "iteration number =  5907 Cross Entropy Loss= 62.44067857065704\n",
      "iteration number =  5908 Cross Entropy Loss= 62.439573513220296\n",
      "iteration number =  5909 Cross Entropy Loss= 62.43846915082382\n",
      "iteration number =  5910 Cross Entropy Loss= 62.43736548319926\n",
      "iteration number =  5911 Cross Entropy Loss= 62.43626251007839\n",
      "iteration number =  5912 Cross Entropy Loss= 62.435160231193166\n",
      "iteration number =  5913 Cross Entropy Loss= 62.43405864627569\n",
      "iteration number =  5914 Cross Entropy Loss= 62.43295775505827\n",
      "iteration number =  5915 Cross Entropy Loss= 62.43185755727333\n",
      "iteration number =  5916 Cross Entropy Loss= 62.43075805265351\n",
      "iteration number =  5917 Cross Entropy Loss= 62.42965924093157\n",
      "iteration number =  5918 Cross Entropy Loss= 62.42856112184043\n",
      "iteration number =  5919 Cross Entropy Loss= 62.42746369511324\n",
      "iteration number =  5920 Cross Entropy Loss= 62.42636696048322\n",
      "iteration number =  5921 Cross Entropy Loss= 62.42527091768383\n",
      "iteration number =  5922 Cross Entropy Loss= 62.42417556644867\n",
      "iteration number =  5923 Cross Entropy Loss= 62.42308090651147\n",
      "iteration number =  5924 Cross Entropy Loss= 62.42198693760618\n",
      "iteration number =  5925 Cross Entropy Loss= 62.42089365946687\n",
      "iteration number =  5926 Cross Entropy Loss= 62.41980107182779\n",
      "iteration number =  5927 Cross Entropy Loss= 62.41870917442335\n",
      "iteration number =  5928 Cross Entropy Loss= 62.41761796698813\n",
      "iteration number =  5929 Cross Entropy Loss= 62.41652744925686\n",
      "iteration number =  5930 Cross Entropy Loss= 62.41543762096445\n",
      "iteration number =  5931 Cross Entropy Loss= 62.41434848184595\n",
      "iteration number =  5932 Cross Entropy Loss= 62.41326003163658\n",
      "iteration number =  5933 Cross Entropy Loss= 62.412172270071736\n",
      "iteration number =  5934 Cross Entropy Loss= 62.411085196886965\n",
      "iteration number =  5935 Cross Entropy Loss= 62.40999881181797\n",
      "iteration number =  5936 Cross Entropy Loss= 62.40891311460064\n",
      "iteration number =  5937 Cross Entropy Loss= 62.40782810497098\n",
      "iteration number =  5938 Cross Entropy Loss= 62.4067437826652\n",
      "iteration number =  5939 Cross Entropy Loss= 62.40566014741967\n",
      "iteration number =  5940 Cross Entropy Loss= 62.4045771989709\n",
      "iteration number =  5941 Cross Entropy Loss= 62.40349493705554\n",
      "iteration number =  5942 Cross Entropy Loss= 62.40241336141047\n",
      "iteration number =  5943 Cross Entropy Loss= 62.401332471772676\n",
      "iteration number =  5944 Cross Entropy Loss= 62.40025226787932\n",
      "iteration number =  5945 Cross Entropy Loss= 62.39917274946771\n",
      "iteration number =  5946 Cross Entropy Loss= 62.39809391627535\n",
      "iteration number =  5947 Cross Entropy Loss= 62.39701576803988\n",
      "iteration number =  5948 Cross Entropy Loss= 62.3959383044991\n",
      "iteration number =  5949 Cross Entropy Loss= 62.394861525390965\n",
      "iteration number =  5950 Cross Entropy Loss= 62.39378543045362\n",
      "iteration number =  5951 Cross Entropy Loss= 62.39271001942534\n",
      "iteration number =  5952 Cross Entropy Loss= 62.391635292044555\n",
      "iteration number =  5953 Cross Entropy Loss= 62.3905612480499\n",
      "iteration number =  5954 Cross Entropy Loss= 62.3894878871801\n",
      "iteration number =  5955 Cross Entropy Loss= 62.388415209174106\n",
      "iteration number =  5956 Cross Entropy Loss= 62.38734321377099\n",
      "iteration number =  5957 Cross Entropy Loss= 62.386271900710014\n",
      "iteration number =  5958 Cross Entropy Loss= 62.38520126973055\n",
      "iteration number =  5959 Cross Entropy Loss= 62.384131320572166\n",
      "iteration number =  5960 Cross Entropy Loss= 62.38306205297458\n",
      "iteration number =  5961 Cross Entropy Loss= 62.381993466677685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number =  5962 Cross Entropy Loss= 62.3809255614215\n",
      "iteration number =  5963 Cross Entropy Loss= 62.37985833694622\n",
      "iteration number =  5964 Cross Entropy Loss= 62.37879179299221\n",
      "iteration number =  5965 Cross Entropy Loss= 62.37772592929998\n",
      "iteration number =  5966 Cross Entropy Loss= 62.376660745610195\n",
      "iteration number =  5967 Cross Entropy Loss= 62.375596241663665\n",
      "iteration number =  5968 Cross Entropy Loss= 62.37453241720141\n",
      "iteration number =  5969 Cross Entropy Loss= 62.37346927196455\n",
      "iteration number =  5970 Cross Entropy Loss= 62.372406805694396\n",
      "iteration number =  5971 Cross Entropy Loss= 62.3713450181324\n",
      "iteration number =  5972 Cross Entropy Loss= 62.37028390902019\n",
      "iteration number =  5973 Cross Entropy Loss= 62.369223478099514\n",
      "iteration number =  5974 Cross Entropy Loss= 62.36816372511234\n",
      "iteration number =  5975 Cross Entropy Loss= 62.36710464980072\n",
      "iteration number =  5976 Cross Entropy Loss= 62.36604625190692\n",
      "iteration number =  5977 Cross Entropy Loss= 62.36498853117334\n",
      "iteration number =  5978 Cross Entropy Loss= 62.36393148734253\n",
      "iteration number =  5979 Cross Entropy Loss= 62.36287512015722\n",
      "iteration number =  5980 Cross Entropy Loss= 62.36181942936027\n",
      "iteration number =  5981 Cross Entropy Loss= 62.36076441469469\n",
      "iteration number =  5982 Cross Entropy Loss= 62.359710075903706\n",
      "iteration number =  5983 Cross Entropy Loss= 62.358656412730625\n",
      "iteration number =  5984 Cross Entropy Loss= 62.35760342491895\n",
      "iteration number =  5985 Cross Entropy Loss= 62.356551112212344\n",
      "iteration number =  5986 Cross Entropy Loss= 62.355499474354616\n",
      "iteration number =  5987 Cross Entropy Loss= 62.35444851108973\n",
      "iteration number =  5988 Cross Entropy Loss= 62.35339822216177\n",
      "iteration number =  5989 Cross Entropy Loss= 62.352348607315065\n",
      "iteration number =  5990 Cross Entropy Loss= 62.35129966629401\n",
      "iteration number =  5991 Cross Entropy Loss= 62.35025139884321\n",
      "iteration number =  5992 Cross Entropy Loss= 62.349203804707386\n",
      "iteration number =  5993 Cross Entropy Loss= 62.34815688363145\n",
      "iteration number =  5994 Cross Entropy Loss= 62.34711063536044\n",
      "iteration number =  5995 Cross Entropy Loss= 62.34606505963956\n",
      "iteration number =  5996 Cross Entropy Loss= 62.34502015621419\n",
      "iteration number =  5997 Cross Entropy Loss= 62.34397592482983\n",
      "iteration number =  5998 Cross Entropy Loss= 62.34293236523215\n",
      "iteration number =  5999 Cross Entropy Loss= 62.34188947716697\n",
      "iteration number =  6000 Cross Entropy Loss= 62.340847260380265\n",
      "iteration number =  6001 Cross Entropy Loss= 62.33980571461818\n",
      "iteration number =  6002 Cross Entropy Loss= 62.338764839627\n",
      "iteration number =  6003 Cross Entropy Loss= 62.33772463515314\n",
      "iteration number =  6004 Cross Entropy Loss= 62.3366851009432\n",
      "iteration number =  6005 Cross Entropy Loss= 62.33564623674394\n",
      "iteration number =  6006 Cross Entropy Loss= 62.334608042302264\n",
      "iteration number =  6007 Cross Entropy Loss= 62.33357051736522\n",
      "iteration number =  6008 Cross Entropy Loss= 62.33253366168\n",
      "iteration number =  6009 Cross Entropy Loss= 62.33149747499397\n",
      "iteration number =  6010 Cross Entropy Loss= 62.33046195705466\n",
      "iteration number =  6011 Cross Entropy Loss= 62.32942710760972\n",
      "iteration number =  6012 Cross Entropy Loss= 62.32839292640695\n",
      "iteration number =  6013 Cross Entropy Loss= 62.327359413194344\n",
      "iteration number =  6014 Cross Entropy Loss= 62.32632656772003\n",
      "iteration number =  6015 Cross Entropy Loss= 62.32529438973228\n",
      "iteration number =  6016 Cross Entropy Loss= 62.324262878979496\n",
      "iteration number =  6017 Cross Entropy Loss= 62.32323203521031\n",
      "iteration number =  6018 Cross Entropy Loss= 62.32220185817341\n",
      "iteration number =  6019 Cross Entropy Loss= 62.32117234761769\n",
      "iteration number =  6020 Cross Entropy Loss= 62.32014350329219\n",
      "iteration number =  6021 Cross Entropy Loss= 62.31911532494611\n",
      "iteration number =  6022 Cross Entropy Loss= 62.31808781232877\n",
      "iteration number =  6023 Cross Entropy Loss= 62.317060965189675\n",
      "iteration number =  6024 Cross Entropy Loss= 62.31603478327846\n",
      "iteration number =  6025 Cross Entropy Loss= 62.31500926634494\n",
      "iteration number =  6026 Cross Entropy Loss= 62.313984414139\n",
      "iteration number =  6027 Cross Entropy Loss= 62.312960226410816\n",
      "iteration number =  6028 Cross Entropy Loss= 62.31193670291058\n",
      "iteration number =  6029 Cross Entropy Loss= 62.310913843388704\n",
      "iteration number =  6030 Cross Entropy Loss= 62.30989164759576\n",
      "iteration number =  6031 Cross Entropy Loss= 62.3088701152824\n",
      "iteration number =  6032 Cross Entropy Loss= 62.30784924619954\n",
      "iteration number =  6033 Cross Entropy Loss= 62.3068290400981\n",
      "iteration number =  6034 Cross Entropy Loss= 62.30580949672931\n",
      "iteration number =  6035 Cross Entropy Loss= 62.3047906158444\n",
      "iteration number =  6036 Cross Entropy Loss= 62.30377239719487\n",
      "iteration number =  6037 Cross Entropy Loss= 62.3027548405323\n",
      "iteration number =  6038 Cross Entropy Loss= 62.30173794560845\n",
      "iteration number =  6039 Cross Entropy Loss= 62.30072171217521\n",
      "iteration number =  6040 Cross Entropy Loss= 62.299706139984636\n",
      "iteration number =  6041 Cross Entropy Loss= 62.29869122878893\n",
      "iteration number =  6042 Cross Entropy Loss= 62.297676978340434\n",
      "iteration number =  6043 Cross Entropy Loss= 62.29666338839166\n",
      "iteration number =  6044 Cross Entropy Loss= 62.295650458695235\n",
      "iteration number =  6045 Cross Entropy Loss= 62.29463818900396\n",
      "iteration number =  6046 Cross Entropy Loss= 62.293626579070796\n",
      "iteration number =  6047 Cross Entropy Loss= 62.29261562864882\n",
      "iteration number =  6048 Cross Entropy Loss= 62.29160533749129\n",
      "iteration number =  6049 Cross Entropy Loss= 62.29059570535158\n",
      "iteration number =  6050 Cross Entropy Loss= 62.28958673198325\n",
      "iteration number =  6051 Cross Entropy Loss= 62.28857841713995\n",
      "iteration number =  6052 Cross Entropy Loss= 62.28757076057556\n",
      "iteration number =  6053 Cross Entropy Loss= 62.28656376204406\n",
      "iteration number =  6054 Cross Entropy Loss= 62.28555742129956\n",
      "iteration number =  6055 Cross Entropy Loss= 62.28455173809635\n",
      "iteration number =  6056 Cross Entropy Loss= 62.28354671218885\n",
      "iteration number =  6057 Cross Entropy Loss= 62.282542343331656\n",
      "iteration number =  6058 Cross Entropy Loss= 62.28153863127949\n",
      "iteration number =  6059 Cross Entropy Loss= 62.2805355757872\n",
      "iteration number =  6060 Cross Entropy Loss= 62.279533176609846\n",
      "iteration number =  6061 Cross Entropy Loss= 62.278531433502536\n",
      "iteration number =  6062 Cross Entropy Loss= 62.277530346220615\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    # 1: decision boundaries at initial\n",
    "    decision_boundaries_initial_vector = N_train_decision_boundaries(X_transpose,theta_hat_aka_weights_initial,theta_0_hat_aka_intercept_initial)\n",
    "    # 2: P Vector at intial Caclulate \n",
    "    P_vector_initial = calculating_P_vector(decision_boundaries_initial_vector,N_train)\n",
    "    # 3: neg log loss at intial value\n",
    "    neg_log_loss_func_at_initial_value = loss_function(C_vector,P_vector_initial)\n",
    "    \n",
    "    #calculating values of theta\n",
    "    \n",
    "    # 4: theta 0 hat final\n",
    "    theta_0_hat_aka_intercept_final = theta_0_hat_aka_intercept_initial - (epsilon_learning_rate * np.mean(C_vector-P_vector_initial))\n",
    "    # 5: theta hat final\n",
    "    theta_hat_aka_weights_final = theta_hat_aka_weights_initial - (epsilon_learning_rate * gradient_vector(X_transpose, C_vector, P_vector_initial))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 6: decision boundaries at final\n",
    "    decision_boundaries_final_vector = N_train_decision_boundaries(X_transpose,theta_hat_aka_weights_final,theta_0_hat_aka_intercept_final)\n",
    "    # 7: P Vector at final Caclulate \n",
    "    P_vector_final = calculating_P_vector(decision_boundaries_final_vector,N_train)\n",
    "    # 8: neg log loss at final value\n",
    "    neg_log_loss_func_at_final_value = loss_function(C_vector,P_vector_final)\n",
    "    \n",
    "    \n",
    "    # difference of neg log loss function value at initial value and final value\n",
    "    if abs(neg_log_loss_func_at_initial_value - neg_log_loss_func_at_final_value) < tolerance:\n",
    "        \n",
    "        break\n",
    "        \n",
    "    theta_0_hat_aka_intercept_initial = theta_0_hat_aka_intercept_final\n",
    "    \n",
    "    theta_hat_aka_weights_initial = theta_hat_aka_weights_final\n",
    "    \n",
    "    iterations.append(iteration_num)\n",
    "    \n",
    "    neg_log_loss_history.append(neg_log_loss_func_at_initial_value[0][0])\n",
    "    \n",
    "    print(\"iteration number = \",iteration_num,\"Cross Entropy Loss=\", neg_log_loss_func_at_initial_value[0][0])\n",
    "    \n",
    "    iteration_num = iteration_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cross Entropy Loss')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc5X3v8c9P0oz2xZIlI8v7BhgDNjgGCgkmhLUpAdqkJLmF29BLSUkCNGkvpOnCq5eGm4Xk0hQaEpKQhkBpSAJNCITdCSEYG2yDbYwNtrGwsWTZsjZrm/ndP+ZoPNiyPF5GZ0bzfb9e5zVnnnNm9HvA9lfnPOc8x9wdERERgIKwCxARkeyhUBARkSSFgoiIJCkUREQkSaEgIiJJRWEXcCTGjx/v06ZNC7sMEZGcsnz58h3uXj/ctpwOhWnTprFs2bKwyxARySlmtvlA23T6SEREkhQKIiKSpFAQEZEkhYKIiCQpFEREJEmhICIiSQoFERFJystQ2Nq+h9t/vY6NO7rDLkVEJKvkZSjs7O7njqc38Mb2zrBLERHJKnkZCtWlEQB27xkIuRIRkeySl6FQUxaEQo9CQUQkVV6GQkVxEYUFRvue/rBLERHJKnkZCmZGTWmEdh0piIi8R16GAkB1WYR2jSmIiLxH3oZCTWlEYwoiIvvI31Aoi2pMQURkH/kbChpTEBHZT96GQnWZTh+JiOwrb0OhpjRKZ98gA7F42KWIiGSN/A2F4Aa2Dl2BJCKSlPehoMtSRUT2yttQqArmP9Jgs4jIXnkbCjWlOn0kIrKv/A2FsiiA7lUQEUmRv6Gg00ciIvvJ21DQmIKIyP7yNhQKC4yqkiI9aEdEJEXehgIkxhV29WhMQURkSF6HwrjyKDu7FQoiIkMyFgpmNtnMnjGztWa22syuD9r/yczeMbMVwXJxymduNrMNZrbOzC7IVG1DxpdHaetSKIiIDCnK4HcPAp9395fNrBJYbmZPBNu+4e5fS93ZzOYCVwAnABOBJ81sjrvHMlVgbXmU1Vs7MvX1IiI5J2NHCu6+zd1fDtY7gbVA0wgf+QjwgLv3uftGYAOwKFP1AdRWJE4fuXsmf4yISM4YlTEFM5sGLABeDJo+Y2arzOx7ZjYuaGsCtqR8rJlhQsTMrjGzZWa2rLW19YjqqiuP0h+L09U3eETfIyIyVmQ8FMysAngIuMHdO4C7gJnAfGAb8PWhXYf5+H6/wrv73e6+0N0X1tfXH1FtdeXFABpXEBEJZDQUzCxCIhDuc/efArj7dnePuXsc+A57TxE1A5NTPj4J2JrJ+morElNdtOkKJBERILNXHxlwD7DW3W9PaW9M2e0y4LVg/RHgCjMrNrPpwGxgaabqg8TpI0CXpYqIBDJ59dGZwJ8Br5rZiqDti8DHzWw+iVNDm4C/BHD31Wb2ILCGxJVL12XyyiNIXH0EsLO7L5M/RkQkZ2QsFNz9tww/TvDoCJ+5Fbg1UzXta2hMYYfGFEREgDy/o7k0WkhZtFCnj0REAnkdCpA4haRQEBFJyPtQqKso1tVHIiIBhUJ5lLYuDTSLiIBCQaePRERS5H0o1JVHadP8RyIigEKB+spi+gfjdPRq/iMREYVCZeJehdbO3pArEREJX96HQkNlCQAtHRpsFhFRKFQljhS260hBREShMKFKRwoiIkPyPhQqiosoixbS0qlQEBHJ+1AAaKgsViiIiKBQABKDzds7NKYgIqJQAOqrimnVkYKIiEIBYEJlCS06UhARUShA4rLU7v4YXX26q1lE8ptCgcRAM6CjBRHJewoFUu5q1riCiOQ5hQIwIbirWaEgIvlOoQA0BHc1v7t7T8iViIiES6EAVJUUUVFcxNZ2jSmISH5TKABmxsSaEra260hBRPKbQiEwsaaUrTp9JCJ57qChYGblZlYQrM8xs0vMLJL50kbXxJpSnT4SkbyXzpHCEqDEzJqAp4A/B36QyaLCMLG6hJ3d/fQOxMIuRUQkNOmEgrl7D3A58K/ufhkwN7Nljb6JNaUAGlcQkbyWViiY2RnAJ4FfBm1FmSspHHtDQaeQRCR/pRMKNwA3Az9z99VmNgN4JrNljb4mHSmIiBz8N353fw54DiAYcN7h7p/LdGGjbUJVCWbwjkJBRPJYOlcf/djMqsysHFgDrDOzv8l8aaMrWlRAQ2Ux23RZqojksXROH8119w7gUuBRYArwZxmtKiSN1bosVUTyWzqhEAnuS7gUeNjdBwDPbFnhaKop1ekjEclr6YTCt4FNQDmwxMymAh2ZLCosk2vLaN7VQyw+JjNPROSgDhoK7n6Huze5+8WesBk4ZxRqG3VT68oYiLnGFUQkb6Uz0FxtZreb2bJg+TqJo4YxZ2ptGQBvt/WEXImISDjSOX30PaAT+FiwdADfz2RRYZlSlwiFzTsVCiKSn9IJhZnu/o/u/law3ALMONiHzGyymT1jZmvNbLWZXR+015rZE2a2Pngdl/KZm81sg5mtM7MLDr9bh6exupRIobGprXu0f7SISFZIJxT2mNlZQ2/M7EwgnZPug8Dn3f144HTgOjObC9wEPOXus0lMsHdT8L1zgSuAE4ALgTvNrPBQOnOkCguMyePKdPpIRPJWOnMYXQv80Myqg/e7gKsO9iF33wZsC9Y7zWwt0AR8BFgc7HYv8Czwv4P2B9y9D9hoZhuARcAL6XbmaJhSV8ZmhYKI5Kl0rj5a6e4nAycBJ7n7AuCDh/JDzGwasAB4EZgQBMZQcDQEuzUBW1I+1hy07ftd1wwNere2th5KGWmZWlvG2zt7cNdlqSKSf9J+8pq7dwR3NgP8dbqfM7MK4CHghpTPD7vrcD92mDrudveF7r6wvr4+3TLSNqWunK6+QXZ29x/17xYRyXaH+zjO4f4B33+nxJ3QDwH3uftPg+btZtYYbG8EWoL2ZmByyscnAVsPs77DNnRZqq5AEpF8dLihcNBzK2ZmwD3AWne/PWXTI+wdk7gKeDil/QozKzaz6cBsYOlh1nfYpo0PQkFXIIlIHjrgQLOZdTL8P/4GlKbx3WeSmDjvVTNbEbR9EbgNeNDMrgbeBj4KEDyr4UESM7EOAte5+6g/G3NKbTmFBcabLQoFEck/BwwFd688ki92999y4NNM5x7gM7cCtx7Jzz1S0aICptaWsaGlK8wyRERCcbinj8a0WQ0VbGhVKIhI/lEoDGNWQwWbdnQzEIuHXYqIyKhSKAxjVkMFg3HXYLOI5J10Zkn9TOr8RPlgVkMFgMYVRCTvpHOkcAzwkpk9aGYXBpeajmkz6xUKIpKf0pnm4ksk7hm4B/ifwHoz+xczm5nh2kJTXlxEU02pQkFE8k5aYwqemAjo3WAZBMYBPzGzr2SwtlDN1BVIIpKH0hlT+JyZLQe+AjwPnOjunwZOBf44w/WFZnZDBRtauvS8ZhHJK+lMnT0euDx4NnOSu8fN7MOZKSt8xzdW0TsQZ1Nbd3KMQURkrEtnTOEfgLrgiOGzZnZKyra1Ga0uRHMbqwBYs3WkiV1FRMaWdE4f/T2Jh+HUkThq+L6ZfSnThYVtVkMFkUJjzTaFgojkj3ROH30CWODuvQBmdhvwMvB/MllY2KJFBcxqqNSRgojklXSuPtoElKS8LwbezEg1WWZuY5WOFEQkr6QTCn3AajP7gZl9H3gN6DKzO8zsjsyWF665E6to7eyjtbMv7FJEREZFOqePfhYsQ57NTCnZZ2iwee22Duorj/6jP0VEss1BQ8Hd7zWzKDAnaFrn7gOZLSs7DIXC6q0dfGCOQkFExr6DhoKZLSZx9dEmEg/NmWxmV7n7ksyWFr7qsgiTa0t59Z32sEsRERkV6Zw++jpwvruvAzCzOcD9JO5oHvPmTx7Hsk07wy5DRGRUpDPQHBkKBAB3fwOIZK6k7LJgcg3bdveyvaM37FJERDIunVBYbmb3mNniYPkOsDzThWWL+VNqAHjlbZ1CEpGxL51QuBZYDXwOuB5YE7TlhbmNVUQKjVe27Aq7FBGRjBtxTMHMCoDl7j4PuH10SsouJZFC5k6sZoWOFEQkD4x4pODucWClmU0ZpXqy0oLJNbz6zm4GY/GwSxERyah0Th81krij+Skze2RoyXRh2WTBlBp6+mO8/m5n2KWIiGRUOpek3pLxKrLcwmm1ACzduJN5TdUhVyMikjnpHClc7O7PpS7AxZkuLJs01ZQyubaUFze2hV2KiEhGpRMK5w3TdtHRLiTbnT69jhc37iSux3OKyBh2wFAws0+b2avAsWa2KmXZCLw6eiVmh9Nn1NHeM8C67RpXEJGxa6QxhR8DvwK+DNyU0t7p7nk378NpMxLjCr9/q43jg4nyRETGmgMeKbj7bnff5O4fB5qBAcCBiny8RHXSuDImjSvlxbfyLg9FJI+kM0vqZ4B/ArYDQxfqO3BS5srKTqfPqOPJtduJxZ3CAgu7HBGRoy6dgeYbgGPd/QR3PzFY8i4QAD4wp572ngFWNuvuZhEZm9IJhS3A7kwXkgveP2s8BQbPrmsNuxQRkYxI5+a1t4BnzeyXJJ7XDIC7591cSOPKo5w8uYbn1rXw1+fNOfgHRERyTDpHCm8DTwBRoDJlyUuL5zSw6p3dtHX1HXxnEZEck84zmveb5sLM0jnCGJMWH1vPN558g9+s38GlC5rCLkdE5Kga6ea136as/8c+m5dmrKIsd2JTNXXlUZ5+vSXsUkREjrqRTh+Vp6zP22db3l6PWVBgnHt8A8+83kLfYCzsckREjqqRQsEPsD7c+/2Y2ffMrMXMXktp+ycze8fMVgTLxSnbbjazDWa2zswuSLsHIbhoXiOdfYM8v2FH2KWIiBxVI40N1JjZZSSCo8bMLg/aDUhn/ugfAN8CfrhP+zfc/WupDWY2F7gCOAGYCDxpZnPcPSt/Ff+DWXVUFhfxq1ff5YPHTQi7HBGRo2akUHgOuCRl/Y9Sti052Be7+xIzm5ZmHR8BHnD3PmCjmW0AFgEvpPn5UVVcVMi5xzfwxNrtDMTiRArTuYhLRCT7HTAU3P3PM/QzP2NmVwLLgM+7+y6gCfh9yj7NQdt+zOwa4BqAKVPCm4LpwnmN/HzFVl58aydnzR4fWh0iIkfTaP+KexcwE5gPbAO+HrQPN3A97LiFu9/t7gvdfWF9fX1mqkzD4mPrKYsW8t8rt4ZWg4jI0TaqoeDu29095u5x4DskThFB4shgcsquk4Cs/te2JFLIRfMaefTVbfQOZOXQh4jIIRvVUDCzxpS3lwFDVyY9AlxhZsVmNh2YTQ7cC/HHpzTR2TfIr9dsD7sUEZGj4qChYGYfNbPKYP1LZvZTMzsljc/dT2Kg+Fgzazazq4GvmNmrZrYKOAe4EcDdVwMPAmuAx4DrsvXKo1Snz6hjYnUJP325OexSRESOinSmq/h7d/8vMzsLuAD4GomxgdNG+lDwcJ593TPC/rcCt6ZRT9YoKDAuO6WJu559k5aOXhqqSsIuSUTkiKRz+mjoN/Y/BO5y94dJTI4nwOWnTCLu8NDL74RdiojIEUsnFN4xs28DHwMeNbPiND+XF2bWV3Da9Frue3EzsfhBb/QWEclq6fzj/jHgceBCd28HaoG/yWhVOebKM6bRvGsPz67TJHkiktvSCYVG4Jfuvt7MFgMfJQeuDBpN558wgQlVxdz7wuawSxEROSLphMJDQMzMZpEYKJ4O/DijVeWYSGEBH180hSVvtLJxR3fY5YiIHLZ0QiHu7oPA5cA33f1GEkcPkuITi6YQKTS+//zGsEsRETls6YTCgJl9HLgS+EXQFslcSbmpoaqEyxY08Z8vbWGHHtUpIjkqnVD4c+AM4FZ33xjccfyjzJaVm/7y7Jn0x+I6WhCRnHXQUHD3NcAXgFfNbB7Q7O63ZbyyHDSzvoKL5h3DD1/YTGfvQNjliIgcsnSmuVgMrAf+DbgTeMPMPpDhunLWp8+eRWfvID/UlUgikoPSOX30deB8dz/b3T9AYqqLb2S2rNx14qRqzj2ugW8/9ya7e3S0ICK5JZ1QiLj7uqE37v4GGmge0RcuOJbOvkHueu7NsEsRETkk6YTCcjO7x8wWB8t3gOWZLiyXHd9YxaXzm/j+8xt5d3dv2OWIiKQtnVC4FlgNfA64nsT01tdmsqix4MYPzSHuzjeffCPsUkRE0jbi1NlmVgAsd/d5wO2jU9LYMKWujKvOmMY9z2/kE6dN4aRJNWGXJCJyUCMeKQSPzVxpZlNGqZ4x5foPzWZ8RTF///Bq4ppBVURyQLoT4q02s6fM7JGhJdOFjQWVJRG+ePFxrNzSzoPLtoRdjojIQaXz5LVbMl7FGHbp/Cbuf3ELtz32OuceP4H6yuKwSxIROaADHimY2SwzO9Pdn0tdAAf0UOI0mRn/cvk8evpjfOnnr+Ku00gikr1GOn30TaBzmPaeYJukaVZDJZ8/bw6Pr97OIyu3hl2OiMgBjRQK09x91b6N7r4MmJaxisaov3j/DBZMqeEfH1lNS4fuXRCR7DRSKJSMsK30aBcy1hUWGF/76Mn0DcS5/oEVep6ziGSlkULhJTP7X/s2mtnV6I7mwzKzvoJ/vnQeL7zVxh1PrQ+7HBGR/Yx09dENwM/M7JPsDYGFQBS4LNOFjVV/cuokXnizjTueXs+i6bWcOWt82CWJiCQd8EjB3be7+x+QuCR1U7Dc4u5nuPu7o1Pe2PTPl57ArPoKPnv/K2zZ2RN2OSIiSek8ZOcZd//XYHl6NIoa68qiRdx95UJicefqe1/SA3lEJGukc0ezZMD08eXc9clTeKu1m8/e/4oGnkUkKygUQvQHs8Zzy0dO4Nl1rbqxTUSyQjrTXEgGffK0qWxt38O/PfMmVSURbrroOMws7LJEJE8pFLLAF84/lo49g3x7yVtUlUa47pxZYZckInlKoZAFzIxbLjmBzt4Bvvr4OszgrxYrGERk9CkUskRBcMezA195bB29/TFuPG+OTiWJyKhSKGSRosICbv/YfEqKCrnj6Q309Mf44sXHU1CgYBCR0aFQyDKFBcaXLz+R0mgh3/3tRt7t6OVrHz2Zkkhh2KWJSB5QKGShggLjH/9oLsdUl3Dbr15na/sevnPlQuoq9IAeEcks3aeQpcyMa8+eyZ2fPIXVWzu47M7fsXZbR9hlicgYp1DIchef2MgD15xO70CMy+58np8s10PvRCRzMhYKZvY9M2sxs9dS2mrN7AkzWx+8jkvZdrOZbTCzdWZ2QabqykULpozjF587i/mTa/jCf63k5p+uoncgFnZZIjIGZfJI4QfAhfu03QQ85e6zgaeC95jZXOAK4ITgM3eamUZWUzRUlvCjq0/jrxbP5P6lW7jkW7/ltXd2h12WiIwxGQsFd18C7Nyn+SPAvcH6vcClKe0PuHufu28ENgCLMlVbrioqLOBvLzyOez+1iPaeAS6783n+7ZkNDMbiYZcmImPEaI8pTHD3bQDBa0PQ3gRsSdmvOWjbj5ldY2bLzGxZa2trRovNVmfPqefXN36AC044hq8+vo4/+fcXNAgtIkdFtgw0D3d31rBThrr73e6+0N0X1tfXZ7is7FVTFuVbnziFOz6+gLd39vDhf/0t//LoWrr7BsMuTURy2GiHwnYzawQIXluC9mZgcsp+k4Cto1xbTrrk5Ik8/fmz+djCSdy95C3Ou/05Hnttm6bhFpHDMtqh8AhwVbB+FfBwSvsVZlZsZtOB2cDSUa4tZ9WURfny5Sfx0KfPoKo0wrU/epmP/vsLvPz2rrBLE5Eck8lLUu8HXgCONbNmM7sauA04z8zWA+cF73H31cCDwBrgMeA6d9c1l4fo1Km1/OKzZ/Hly09kU1sPl9/5O66772U2t3WHXZqI5AjL5dMMCxcu9GXLloVdRlbq7hvk7iVvcfeSt+iPxblsQRPXnTOL6ePLwy5NREJmZsvdfeGw2xQKY1tLRy///txb3PfiZgZicS6d38R1H5zFzPqKsEsTkZAoFISWzl6++5uN/McLm+kdjPGh4yfwqTOnc/qMWj2zQSTPKBQkqa2rjx/8bhP3vfg2O7v7mdtYxafOms4fndxIcZFuIhfJBwoF2U/vQIyfv/IO33t+I29s76KuPMqfnDqJj71vsk4tiYxxCgU5IHfnN+t3cN+Lm3lqbQuDcWfRtFquWDSZi+Y1UhrV0YPIWKNQkLS0dPby0PJ3+M+X3mZTWw+VxUWcf8IxXDJ/ImfOrKOoMFtugBeRI6FQkEPi7ry4cSc/Wd7M46+9S2ffIHXlUS4+sZFL5k/k1Cnj9NxokRymUJDD1jsQ49l1rfz3yq08uXY7fYNxJlQV86HjJ3De3AmcMbNOA9QiOUahIEdFV98gT6x5l8df286S9a309MeoKC7i7Dn1nDd3AmfPqWdceTTsMkXkIBQKctT1DsT43Zs7eGLNdp5c20JrZx9mcFJTNe+fXc/7Z49nwZRxRIs0DiGSbRQKklHxuLOiuZ0lb7Tym/U7WLGlnVjcKY8WcsbMOs6aNZ7TZtRx7IRKjUWIZAGFgoyqjt4BXnizjd+sT4TE5rYeAKpLIyycOo5F02tZNL2WeU3VRHRFk8ioGykUika7GBn7qkoiXHDCMVxwwjEAbNnZw9KNO3lp006WbtzJU68nHqNRGinklKk1nDJlHCdPquHkyTXUVxaHWbpI3lMoSMZNri1jcm0Zf3zqJCBxP8SyTbtYujEREnc++yaxeOKItammlJMnVydD4sSmasqL9cdUZLTob5uMuobKEi4+sZGLT2wEoKd/kNVbO1i5pZ0VW9pZ2dzOo6++C0CBwbTx5RzfWMXcYDm+sYoJVcWayE8kAxQKErqyaBHvm1bL+6bVJtvauvpY1bybFVvaWbMtERi/XLUtub22PMrxjZUcf0wiJGZPqGBmfYWOKkSOkP4GSVaqqyjmnOMaOOe4hmTb7j0DvL6tg7XbOli7rZM12zr44e830z8YT+4zsbqEmQ0VzG6oZFZDRXKp1f0TImlRKEjOqC6NcNqMOk6bUZdsG4zF2dTWzYaWrr1Laxc/XrqZ3oG9YVFXHmVGfTlT68qZWlvG1PHlTKsrY2ptOdVlkTC6I5KVFAqS04oKC5jVUMmshsr3tMfjzjvte9jQ2sWbLV2s397FxrZufrO+lZ909L1n35qySCIo6hJBMaWunEnjSmmqKeWY6hJdNit5RaEgY1JBgSWvejrn2Ib3bNvTH+PtnT1sauvm7bbE6+a2Hl7ZsotfrNpKPOXWnQKDCVUlNNWU0jSulIk1pcn1pmBd4xgyluhPs+Sd0mghxx5TybHHVO63rX8wTvOuHt5p38PW9j28s2sPzcHry2/v4pertjEYf+8Nn9WlEY6pKqGhqpgJVSUcU1XChKpiGqpKku/HV0Q19bjkBIWCSIpoUQEz6iuYcYCnz8XiTktnL1vb99C8a08yPLZ39NHS0cv67V20dvUl77sYYgbjK4qZUFXMhMoSGqpKqK+IUldRzPiKYuoqooyviFJXXkx1aUTTgUhoFAoih6CwwGisLqWxupRTpw6/TyzutHX1sb2jj+0dvWzv7GX77t7E+85etu7uZcWWdnb29DPcLDNFBUZt+VBgRBOhkfK+tjxKTVmUmrII48qiVJdGKFSIyFGiUBA5ygoLjIaqxNHAiVQfcL9Y3NnV08+Orj7auva+tnX3saMzeO3qZ1NbN21d/fT0xw74XVUlRYwrj1JTGqGmLMq4ssh7gqNm6H1phJqyCJUlESpLijSILvtRKIiEpLDAGB+cPkpHT/8gbV397OzuZ1dPP7v3DLCru59dPQOJ9Z7EentPPxt3dLOrp5/O3sERv7M0UkhVaVEyJKqGXkv3vq8qSWzfd7+KkiLKo0U6ShljFAoiOaIsWkRZbRGTa8vS/sxgLB4ExgC79/Szq3uA9j0DdPYO0Nk7SGfvAB17BunsS7y29/Tz9s6eZHt/LH7Qn1ESKaCiuIjy4iLKokVUFBcGr0WURQspLy6ivDh4jRYFr0PtwbagvTRSSHFRgcZUQqRQEBnDigoLqKsopi7No5F99Q7E6OwdpCMIkY49e8Oks3eQ7v5BuvsG6eqL0ROsd/fF2NXTT/OuHrr7Ysl94ocwS39JpIDSSCGlkUJKooXJ9dJoISWRA7yPJj5TMtRetHd7SaSA4qJE4CSWQqLBugLovRQKInJAJcE/skc6pbm70zcYp6tvkJ6+WOK1fzB4Tbzv7hukdyDOnoEYvQMx9vTH2DOQWHqD9a6+QXZ09e+3PXWqk0MVKTSihQUUB0cp0WGCY2974XvXIwXBZxOv0aICIoUFFBXYe9YjRYntQ+uRggIiRUak8MDrRQUWyqSPCgURyTgzSwYMw1/te0RicU8ERRAWqes9AzH6BuL0x+L0DcToG4zTPxinbzBO32Bs+PWh/QcTn+3qG6RvYN/9E9+Tzim2wxUptPeETFFKaJx7XAN/94dzj/rPVCiISM4rLLDkGMVoi8c9CJw4A/E4gzFnIJYIi6H1xDLy+mAsTn/MGQzaU9eH2/+Y6tKM9EehICJyBAoKjJKC4ChoDNBFyiIikqRQEBGRJIWCiIgkKRRERCRJoSAiIkkKBRERSVIoiIhIkkJBRESSzId7ykeOMLNWYPMRfMV4YMdRKidM6kd2UT+yz1jpy9Hqx1R3rx9uQ06HwpEys2XuvjDsOo6U+pFd1I/sM1b6Mhr90OkjERFJUiiIiEhSvofC3WEXcJSoH9lF/cg+Y6UvGe9HXo8piIjIe+X7kYKIiKRQKIiISFJehoKZXWhm68xsg5ndFHY9+zKz75lZi5m9ltJWa2ZPmNn64HVcyrabg76sM7MLUtpPNbNXg2132Cg/8NXMJpvZM2a21sxWm9n1udgXMysxs6VmtjLoxy252I+UGgrN7BUz+0WO92NTUMMKM1uWq30xsxoz+4mZvR78XTkj1H64e14tQCHwJjADiAIrgblh17VPjR8ATgFeS2n7CnBTsH4T8H+D9blBH4qB6UHfCoNtS4EzAAN+BVw0yv1oBE4J1iuBN4J6c6ovwc+sCNYjwIvA6bnWj5T+/DXwY+AXufpnK6hhEzB+n7ac6wtwL/AXwXoUqAmzH6P6PzEbluA/2uMp728Gbg67rmHqnMZ7Q2Ed0BisNwLrhqsfeDzoYyPwekr7x4Fvh9ynh2CCA0IAAAS4SURBVIHzcrkvQBnwMnBaLvYDmAQ8BXyQvaGQc/0Ifu4m9g+FnOoLUAVsJLjoJxv6kY+nj5qALSnvm4O2bDfB3bcBBK8NQfuB+tMUrO/bHgozmwYsIPFbds71JTjlsgJoAZ5w95zsB/BN4G+BeEpbLvYDwIFfm9lyM7smaMu1vswAWoHvB6f0vmtm5YTYj3wMheHOs+XydbkH6k/W9NPMKoCHgBvcvWOkXYdpy4q+uHvM3eeT+E17kZnNG2H3rOyHmX0YaHH35el+ZJi20PuR4kx3PwW4CLjOzD4wwr7Z2pciEqeK73L3BUA3idNFB5LxfuRjKDQDk1PeTwK2hlTLodhuZo0AwWtL0H6g/jQH6/u2jyozi5AIhPvc/adBc072BcDd24FngQvJvX6cCVxiZpuAB4APmtmPyL1+AODuW4PXFuBnwCJyry/NQHNw5AnwExIhEVo/8jEUXgJmm9l0M4sCVwCPhFxTOh4BrgrWryJxfn6o/QozKzaz6cBsYGlwyNlpZqcHVyFcmfKZURH83HuAte5+e8qmnOqLmdWbWU2wXgp8CHg91/rh7je7+yR3n0biz/3T7v4/cq0fAGZWbmaVQ+vA+cBr5Fhf3P1dYIuZHRs0nQusCbUfoz04lA0LcDGJK2HeBP4u7HqGqe9+YBswQOI3gKuBOhIDhOuD19qU/f8u6Ms6Uq44ABaS+IvyJvAt9hnMGoV+nEXiEHYVsCJYLs61vgAnAa8E/XgN+IegPaf6sU+fFrN3oDnn+kHiXPzKYFk99Pc4R/syH1gW/Pn6OTAuzH5omgsREUnKx9NHIiJyAAoFERFJUiiIiEiSQkFERJIUCiIikqRQkLxmZr8LXqeZ2SeO8nd/cbifJZLNdEmqCGBmi4EvuPuHD+Ezhe4eG2F7l7tXHI36REaLjhQkr5lZV7B6G/D+YG7+G4MJ8L5qZi+Z2Soz+8tg/8WWeEbEj4FXg7afB5OyrR6amM3MbgNKg++7L/VnWcJXzey1YP77P0357mdT5ta/b2hOfDO7zczWBLV8bTT/G0l+KQq7AJEscRMpRwrBP+673f19ZlYMPG9mvw72XQTMc/eNwftPufvOYAqMl8zsIXe/ycw+44lJ9PZ1OYm7WE8GxgefWRJsWwCcQGLemueBM81sDXAZcJy7+9CUGyKZoCMFkeGdD1wZTJf9IolpB2YH25amBALA58xsJfB7EpOVzWZkZwH3e2Lm1e3Ac8D7Ur672d3jJKYFmQZ0AL3Ad83scqDniHsncgAKBZHhGfBZd58fLNPdfehIoTu5U2Is4kPAGe5+Mok5kkrS+O4D6UtZjwFF7j5I4ujkIeBS4LFD6onIIVAoiCR0knhk6JDHgU8HU39jZnOC2Tj3VQ3scvceMzuOxGM6hwwMfX4fS4A/DcYt6kk8fnXpgQoLnkdR7e6PAjeQOPUkkhEaUxBJWAUMBqeBfgD8PxKnbl4OBntbSfyWvq/HgGvNbBWJWSt/n7LtbmCVmb3s7p9Maf8ZiUcoriQxi+zfuvu7QagMpxJ42MxKSBxl3Hh4XRQ5OF2SKiIiSTp9JCIiSQoFERFJUiiIiEiSQkFERJIUCiIikqRQEBGRJIWCiIgk/X8BD39vyYS3IQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iterations,neg_log_loss_history)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]], dtype=uint32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction = np.uint(P_vector_final > 0.5)\n",
    "training_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26206995646323467"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_0_hat_aka_intercept_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31627804],\n",
       "       [ 0.30606876],\n",
       "       [ 0.31182924],\n",
       "       [ 0.27204453],\n",
       "       [ 0.14239034],\n",
       "       [ 0.15353413],\n",
       "       [ 0.23033429],\n",
       "       [ 0.30739899],\n",
       "       [ 0.11962604],\n",
       "       [-0.13600005],\n",
       "       [ 0.21102423],\n",
       "       [-0.02722994],\n",
       "       [ 0.16828931],\n",
       "       [ 0.1634676 ],\n",
       "       [-0.06549935],\n",
       "       [-0.0412921 ],\n",
       "       [-0.01693214],\n",
       "       [ 0.14051939],\n",
       "       [-0.09212674],\n",
       "       [-0.12535215],\n",
       "       [ 0.34787276],\n",
       "       [ 0.35538611],\n",
       "       [ 0.33276803],\n",
       "       [ 0.2818846 ],\n",
       "       [ 0.24634712],\n",
       "       [ 0.18557787],\n",
       "       [ 0.24903798],\n",
       "       [ 0.37611039],\n",
       "       [ 0.23178645],\n",
       "       [ 0.0809595 ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat_aka_weights_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 30)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transpose_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train_test 171\n",
      "M_testM 30\n"
     ]
    }
   ],
   "source": [
    "N_train_test, M_test = X_transpose_test.shape # N is N_train (number of rows) & M is Number of columns\n",
    "print(\"N_train_test\",N_train_test)\n",
    "print(\"M_testM\",M_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: decision boundaries for testing data\n",
    "decision_boundaries_test = N_train_decision_boundaries(X_transpose_test,theta_hat_aka_weights_final,theta_0_hat_aka_intercept_final)\n",
    "# 2: P Vector for testing data\n",
    "P_vector_test = calculating_P_vector(decision_boundaries_test,N_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_vector_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=uint32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_prediction = np.uint(P_vector_test > 0.5) # greater than zero means malignant tumor\n",
    "testing_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction_correct = np.count_nonzero(training_prediction == y_train)\n",
    "training_prediction_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy=(training_prediction_correct/X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9396984924623115"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_prediction_correct = np.count_nonzero(testing_prediction == y_test)\n",
    "testing_prediction_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accuracy=(testing_prediction_correct/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis\n",
       "232          0\n",
       "406          0\n",
       "227          0\n",
       "298          0\n",
       "294          0\n",
       "..         ...\n",
       "32           1\n",
       "24           1\n",
       "441          1\n",
       "372          1\n",
       "370          1\n",
       "\n",
       "[171 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_class = np.array(y_test)\n",
    "actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_class = actual_class.reshape(actual_class.shape[0])\n",
    "actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=uint32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_prediction=testing_prediction.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_matrix_df(model_name,train_acc,test_acc,y_pred_testing_data_prediction,y_test_testing_data_actual_class):\n",
    "    \n",
    "    confusion_matrix_df = pd.crosstab(y_test_testing_data_actual_class,y_pred_testing_data_prediction, colnames = ['predicted'], rownames = ['actual'], margins = True)\n",
    "    \n",
    "    # negative is 0\n",
    "    # positive is 1\n",
    "    \n",
    "    tp = confusion_matrix_df[1][1] #True Positive\n",
    "    fn = confusion_matrix_df[0][1] #False Negative\n",
    "    fp = confusion_matrix_df[1][0] #False Positive\n",
    "    tn = confusion_matrix_df[0][0] #True Negative\n",
    "    total_size = y_test.shape[0]\n",
    "    \n",
    "    # Accuracy is the number of correct predictions over all kinds of prediction\n",
    "    Accuracy  = ((tp+tn)/(tp+tn+fp+fn)) \n",
    "    \n",
    "    #Precision is a measure that tells out of positive predicted (𝑻𝑷+𝑭𝑷), what % is truly positive (𝑻𝑷).\n",
    "    Precision = (tp/(tp+fp)) \n",
    "    \n",
    "    # Recall is a measure that tells out of positive(𝑻𝑷+𝑭𝑵),what % are predicted positive (𝑻𝑷).\n",
    "    Recall_true_positive_rate    = (tp/(tp+fn))  \n",
    "    \n",
    "    #F1-Score is the Harmonic Mean of precision and recall. It takes both 𝑭𝑷 & 𝑭𝑵 into account.\n",
    "    #Alone, neither precision or recall tells the whole story.\n",
    "    #We can have really good precision with terrible recall or vice versa.\n",
    "    F1_score  = (2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))) \n",
    "    \n",
    "    #Specificity is a measure that tells out of negative(𝑻𝑵+𝑭𝑷),what % are predicted negative (𝑻𝑵).\n",
    "    Specificity = (tn/(tn+fp))\n",
    "    \n",
    "    #proportion of true negatives that are misclassified as positives. \n",
    "    #out of predicted positive(𝑻𝑵+𝑭𝑷),what % are false positive (𝑭𝑷).\n",
    "    False_positive_rate = (fp/(tn+fp)) # 1-Specificity\n",
    "\n",
    "    algo_performance_df = pd.DataFrame(columns=[\"Model\", \"Train Accuracy\", \"Test Accuracy\",\"TP\",\"TN\",\"FP\",\"FN\",\"Total\",\"Accuracy\",\"Precision\",\"Recall or TPR\",\"F1-Score\",\"Specificity\",\"FPR\"])\n",
    "    \n",
    "    algo_performance_df = algo_performance_df.append(\n",
    "                                     { \"Model\" : model_name,\n",
    "                                          \"Train Accuracy\": train_acc,\n",
    "                                          \"Test Accuracy\": test_acc,\n",
    "                                          \"TP\" : tp,\n",
    "                                          \"TN\": tn,\n",
    "                                          \"FP\" : fp,\n",
    "                                          \"FN\" : fn,\n",
    "                                          \"Total\" : total_size,\n",
    "                                          \"Accuracy\": Accuracy,\n",
    "                                          \"Precision\" : Precision,\n",
    "                                          \"Recall or TPR\": Recall_true_positive_rate,\n",
    "                                          \"F1-Score\": F1_score,\n",
    "                                          \"Specificity\": Specificity,\n",
    "                                          \"FPR\":False_positive_rate\n",
    "                                         }, ignore_index=True)\n",
    "\n",
    "    return confusion_matrix_df, algo_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, algo_performance_dataframe = performance_matrix_df('logisitc regression', training_accuracy, testing_accuracy, testing_prediction,actual_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>162</td>\n",
       "      <td>9</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted    0  1  All\n",
       "actual                \n",
       "0          158  0  158\n",
       "1            4  9   13\n",
       "All        162  9  171"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Total</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall or TPR</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logisitc regression</td>\n",
       "      <td>0.939698</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>9</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>171</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Accuracy  Test Accuracy TP   TN FP FN Total  \\\n",
       "0  logisitc regression        0.939698       0.976608  9  158  0  4   171   \n",
       "\n",
       "   Accuracy  Precision  Recall or TPR  F1-Score  Specificity  FPR  \n",
       "0  0.976608        1.0       0.692308  0.818182          1.0  0.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_performance_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
